{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2Cybg_6vE2qM",
        "outputId": "19391cee-880e-430c-c52b-b3f6677d067b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/nougat\n",
            "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-9a6ivfsd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-9a6ivfsd\n",
            "  Resolved https://github.com/facebookresearch/nougat to commit 47c77d70727558b4a2025005491ecb26ee97f523\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.38.2\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m595.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting requests==2.31.0\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (2024.5.15)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (0.4.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.2) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4) (0.19.0+cu121)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2024.8.30)\n",
            "Collecting orjson (from nougat-ocr==0.1.17)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m719.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from nougat-ocr==0.1.17) (4.10.0.84)\n",
            "Collecting datasets[vision] (from nougat-ocr==0.1.17)\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting lightning<2022,>=2.0.0 (from nougat-ocr==0.1.17)\n",
            "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from nougat-ocr==0.1.17) (3.8.1)\n",
            "Collecting python-Levenshtein (from nougat-ocr==0.1.17)\n",
            "  Downloading python_Levenshtein-0.25.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from nougat-ocr==0.1.17) (0.1.99)\n",
            "Collecting sconf>=0.2.3 (from nougat-ocr==0.1.17)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: albumentations>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nougat-ocr==0.1.17) (1.4.14)\n",
            "Collecting pypdf>=3.1.0 (from nougat-ocr==0.1.17)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting pypdfium2 (from nougat-ocr==0.1.17)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m951.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (0.23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (4.12.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (2.8.2)\n",
            "Requirement already satisfied: albucore>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (0.0.14)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (0.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.6.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
            "  Downloading torchmetrics-1.4.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr==0.1.17)\n",
            "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting munch (from sconf>=0.2.3->nougat-ocr==0.1.17)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4) (3.1.4)\n",
            "INFO: pip is looking at multiple versions of datasets[vision] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting datasets[vision] (from nougat-ocr==0.1.17)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets[vision]->nougat-ocr==0.1.17)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (2.1.4)\n",
            "Collecting datasets[vision] (from nougat-ocr==0.1.17)\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting xxhash (from datasets[vision]->nougat-ocr==0.1.17)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets[vision]->nougat-ocr==0.1.17)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2)\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (3.10.5)\n",
            "Requirement already satisfied: Pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (9.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->nougat-ocr==0.1.17) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->nougat-ocr==0.1.17) (1.4.2)\n",
            "Collecting Levenshtein==0.25.1 (from python-Levenshtein->nougat-ocr==0.1.17)\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein==0.25.1->python-Levenshtein->nougat-ocr==0.1.17)\n",
            "  Downloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (4.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (71.0.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations>=1.0.0->nougat-ocr==0.1.17) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations>=1.0.0->nougat-ocr==0.1.17) (2.20.1)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations>=1.0.0->nougat-ocr==0.1.17) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations>=1.0.0->nougat-ocr==0.1.17) (2024.8.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations>=1.0.0->nougat-ocr==0.1.17) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.5.4) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2024.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.17)\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.5.4) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.17) (1.16.0)\n",
            "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_Levenshtein-0.25.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.4.1-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nougat-ocr\n",
            "  Building wheel for nougat-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nougat-ocr: filename=nougat_ocr-0.1.17-py3-none-any.whl size=81856 sha256=cce746ab6caae34e8cd59ce709924214797f7ff727c12737ae7835aa532ae4ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6v6hjex8/wheels/ff/5c/73/60d3f5a7019f3f03c3b487b9b3d56f84eed49ad10959bd321a\n",
            "Successfully built nougat-ocr\n",
            "Installing collected packages: xxhash, ruamel.yaml.clib, requests, rapidfuzz, pypdfium2, pypdf, pyarrow, orjson, munch, lightning-utilities, fsspec, dill, ruamel.yaml, multiprocess, Levenshtein, torchmetrics, tokenizers, sconf, python-Levenshtein, transformers, timm, pytorch-lightning, datasets, lightning, nougat-ocr\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.3.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.25.1 datasets-2.19.1 dill-0.3.8 fsspec-2024.3.1 lightning-2.4.0 lightning-utilities-0.11.7 multiprocess-0.70.16 munch-4.0.0 nougat-ocr-0.1.17 orjson-3.10.7 pyarrow-14.0.1 pypdf-4.3.1 pypdfium2-4.30.0 python-Levenshtein-0.25.1 pytorch-lightning-2.4.0 rapidfuzz-3.9.7 requests-2.31.0 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 sconf-0.2.5 timm-0.5.4 tokenizers-0.15.2 torchmetrics-1.4.1 transformers-4.38.2 xxhash-3.5.0\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=91daedeb660c410b33615bebaada137415bcc867e1bf65f6fb2635833db89d4d\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a9190ef79638>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "#@title Setup and Authentication\n",
        "#!apt-get install -y poppler-utils tesseract-ocr libmagic-dev\n",
        "\n",
        "#nougat\n",
        "!pip install transformers==4.38.2 pyarrow==14.0.1 timm==0.5.4 requests==2.31.0 git+https://github.com/facebookresearch/nougat\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install   textblob langdetect\n",
        "#!pip install nltk>3.8.1\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "def find_pdfs(directory):\n",
        "    pdf_files = []\n",
        "    for root, dirs, files in os.walk(directory, followlinks=True):\n",
        "        for file in files:\n",
        "            if file.endswith('.pdf'):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "\n",
        "pdf_directory = '/content/drive/MyDrive/'  # Replace with your directory path\n",
        "pdf_files = find_pdfs(pdf_directory)\n",
        "for p in pdf_files:\n",
        "    print(p)\n",
        "\n",
        "import os\n",
        "import logging\n",
        "#import PyPDF2\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "from google.colab import drive\n",
        "import re\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.info, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyLuNnwd4nWm",
        "outputId": "33658eac-cd1c-486f-d04d-07b703187adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/deloitte/us-state-of-gen-ai-q3.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_XNyT3EEnom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a4425e-78fc-496b-b89f-91816a2feb7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "import fcntl  # Import the fcntl module for file locking\n",
        "def sanitize_filename(filename):\n",
        "    # Remove spaces and other potentially problematic characters\n",
        "    sanitized = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "\n",
        "def upload_to_drive(csv_path, drive_directory):\n",
        "    try:\n",
        "        #drive.mount('/content/drive', force_remount=True)\n",
        "        destination_path = os.path.join(drive_directory, os.path.basename(csv_path))\n",
        "        shutil.copy(csv_path, destination_path)\n",
        "        print(f\"Uploaded {csv_path} to {destination_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error uploading to Google Drive: {e}\")\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import words, brown\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('brown')\n",
        "GARBAGE_THRESHOLD = 0.8\n",
        "\n",
        "# Load a pre-trained language model for fluency scoring\n",
        "language_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "from textblob import TextBlob\n",
        "\n",
        "def text_quality_score(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Check if tokens are common English words\n",
        "    english_words = set(words.words())\n",
        "    common_word_ratio = sum(1 for word in tokens if word.lower() in english_words) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Part-of-Speech tagging\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_score = sum(1 for word, tag in pos_tags if tag in ['NN', 'VB', 'JJ', 'RB']) / len(pos_tags) if pos_tags else 0\n",
        "\n",
        "    # Language Model fluency score (simplified)\n",
        "    lm_score = len(text.split()) / 10  # Simplified scoring\n",
        "\n",
        "    # Spell check\n",
        "    blob = TextBlob(text)\n",
        "    misspelled_words = [word for word, pos in blob.tags if pos != 'NNP' and pos != 'NNPS' and word.lower() not in english_words and not word.isalpha()]\n",
        "    misspelled_ratio = len(misspelled_words) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Check for too many numbers\n",
        "    number_ratio = sum(1 for token in tokens if token.isdigit()) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Check for words with spaces between letters\n",
        "    spaced_word_ratio = sum(1 for token in tokens if re.search(r'(\\w) (\\w)', token)) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Combine scores\n",
        "    combined_score = (common_word_ratio + pos_score + lm_score) / 3\n",
        "\n",
        "    # Include misspelled ratio, number ratio, and spaced word ratio in the score\n",
        "    combined_score *= (1 - misspelled_ratio)\n",
        "    combined_score *= (1 - number_ratio)\n",
        "    combined_score *= (1 - spaced_word_ratio)\n",
        "\n",
        "    return combined_score\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, LENWORD=50):\n",
        "    # Calculate the quality score\n",
        "    score = text_quality_score(text) #this check is now ooutsourced to the nougat OCR\n",
        "\n",
        "    # Determine if the text is garbage based on the threshold\n",
        "    if score < threshold:\n",
        "        return True\n",
        "\n",
        "    # Check for jammed words\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    for word in words:\n",
        "        if len(word) > LENWORD and '_' not in word:  # heuristic to detect jammed words\n",
        "            return True\n",
        "\n",
        "    # Check for a high proportion of single-letter words\n",
        "    single_letter_words = [word for word in words if len(word) == 1]\n",
        "    if len(single_letter_words) > 0.1 * len(words):  # threshold for single-letter words\n",
        "        return True\n",
        "\n",
        "    #TODO check for repetition\n",
        "      # Check for repetitive patterns\n",
        "     # Check for repetitive loops\n",
        "    loops = re.findall(r'(.+?)\\1{2,}', text) #finds almost all loops but some sneak in\n",
        "    #loops = re.findall(r'(.+?)\\1', text) #finds almost no loops\n",
        "    if loops:\n",
        "        return True\n",
        "\n",
        "    # Check if the text is predominantly in English\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            return True\n",
        "    except LangDetectException:\n",
        "        # If language detection fails, consider the text as garbage\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "import re\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra newlines and spaces\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    # Remove leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove any text between square brackets\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "\n",
        "    # Remove academic references, bracketed references, and references to tables and figures\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE)  # Remove (82)\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE)  # Remove [RD47] and similar\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)  # Remove (Krizhevsky et al., 2014)\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE)  # Remove (Menendez and Woodworth 2010)\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE)  # Remove (see equations (5) and (10))\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)  # Remove (Yao et al., 2007; Lin et al., 2016)\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)  # Remove Table 7\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE)  # Remove [FIGURE:S4.F1]\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE)  # Remove [23, 32, 36, 37, 38, 39]\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE)  # Remove any text containing \"arxiv\" between brackets\n",
        "\n",
        "    # Remove any remaining non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove any sequences of punctuation or symbols that are likely to be garbled text\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "def download_pdf(file_id, output_name):\n",
        "    try:\n",
        "        # Assuming file_id is the source path and output_name is the destination path\n",
        "        if not os.path.exists(output_name):\n",
        "            shutil.copy(file_id, output_name)\n",
        "            print(f\"Copied {file_id} to {output_name}\")\n",
        "        else:\n",
        "            print(f\"{output_name} already exists. Skipping download.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error copying {file_id} to {output_name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gapbfXGUhvnY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import os\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "#import PyPDF2\n",
        "\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "import fcntl\n",
        "\n",
        "\n",
        "def chunk_text(content, max_size=8192):\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith((\"#\", \"##\", \"###\")):\n",
        "            if current_segment:\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            current_segment = \"\"\n",
        "        else:\n",
        "            current_segment += line + \" \"\n",
        "\n",
        "    if current_segment:\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    return segments\n",
        "\n",
        "import nltk\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    sentences = nltk.sent_tokenize(segment)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 1 <= max_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def clean_text_from_mmd(file_path, garbage_threshold=0.8):\n",
        "    raw_text_path = file_path  # Assuming the file is the raw text file\n",
        "    garbage_text_path = f\"{os.path.splitext(file_path)[0]}_garbage_text.jsonl\"\n",
        "    cleaned_text_path = f\"{os.path.splitext(file_path)[0]}_cleaned_text.jsonl\"\n",
        "\n",
        "    if not os.path.exists(raw_text_path):\n",
        "        print(f\"Raw text for {file_path} does not exist. Skipping cleaning.\")\n",
        "        return\n",
        "\n",
        "    # Delete existing JSONL files\n",
        "    if os.path.exists(garbage_text_path):\n",
        "        os.remove(garbage_text_path)\n",
        "    if os.path.exists(cleaned_text_path):\n",
        "        os.remove(cleaned_text_path)\n",
        "\n",
        "    print(f\"Starting text cleaning from {file_path}\")\n",
        "    garbage_count = 0\n",
        "    total_segments = 0\n",
        "\n",
        "    with open(raw_text_path, \"r\", encoding='utf-8') as raw_f, \\\n",
        "         open(cleaned_text_path, \"a\", encoding='utf-8') as cleaned_f, \\\n",
        "         open(garbage_text_path, \"a\", encoding='utf-8') as garbage_f:\n",
        "        progress_bar = tqdm(desc=f\"Cleaning {file_path}\", unit=\"segment\", leave=True)\n",
        "\n",
        "        # Read the entire file content\n",
        "        content = raw_f.read()\n",
        "\n",
        "        # Call a separate function for chunking the text\n",
        "        segments = chunk_text(content)\n",
        "\n",
        "        for segment in segments:\n",
        "            #print(segment)\n",
        "            try:\n",
        "                cleaned, is_garb = process_segment(segment, garbage_threshold)\n",
        "                if is_garb:\n",
        "                    fcntl.flock(garbage_f, fcntl.LOCK_EX)\n",
        "                    garbage_f.write(json.dumps({\"text\": cleaned}) + '\\n')\n",
        "                    fcntl.flock(garbage_f, fcntl.LOCK_UN)\n",
        "                    garbage_count += 1\n",
        "                else:\n",
        "                    fcntl.flock(cleaned_f, fcntl.LOCK_EX)\n",
        "                    cleaned_f.write(json.dumps({\"text\": cleaned}) + '\\n')\n",
        "                    fcntl.flock(cleaned_f, fcntl.LOCK_UN)\n",
        "                total_segments += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing segment: {e}\")\n",
        "            progress_bar.update(1)\n",
        "        progress_bar.close()\n",
        "\n",
        "    garbage_percentage = (garbage_count / total_segments) * 100 if total_segments else 0\n",
        "    print(f\"Completed text cleaning from {file_path}. Garbage segments: {garbage_percentage:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I829qzLMhpY1"
      },
      "outputs": [],
      "source": [
        "def extract_page_text(page):\n",
        "    return page.extract_text()\n",
        "\n",
        "def process_segment(line, garbage_threshold):\n",
        "    segment = line#json.loads(line)[\"text\"]\n",
        "    cleaned = clean_text(segment)\n",
        "    is_garb = is_garbage(cleaned, threshold=garbage_threshold)\n",
        "    return cleaned, is_garb\n",
        "\n",
        "\n",
        "def check_file_in_drive(file_path, drive_directory):\n",
        "    \"\"\"Check if a file exists in the specified Google Drive directory.\"\"\"\n",
        "    # Construct the full path in the mounted drive\n",
        "    full_path = os.path.join(drive_directory, os.path.basename(file_path))\n",
        "\n",
        "    # Check if the file exists\n",
        "    return os.path.exists(full_path)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def process_pdf(file_id, drive_directory):\n",
        "    try:\n",
        "        logger.info(f\"Starting processing for {file_id}\")\n",
        "\n",
        "        base_filename = os.path.basename(file_id)\n",
        "        sanitized_base_filename = sanitize_filename(base_filename)\n",
        "        sanitized_output_name = f\"{sanitized_base_filename}.pdf\"\n",
        "        logger.info(f\"Sanitized output filename: {sanitized_output_name}\")\n",
        "\n",
        "        if not os.path.exists(sanitized_output_name):\n",
        "            download_pdf(file_id, sanitized_output_name)\n",
        "            logger.info(f\"Downloaded {sanitized_output_name}\")\n",
        "        else:\n",
        "            logger.info(f\"{sanitized_output_name} already exists. Skipping download.\")\n",
        "\n",
        "        raw_text_path = f\"{os.path.splitext(sanitized_output_name)[0]}.mmd\"\n",
        "\n",
        "        # Check if the files already exist in the drive directory\n",
        "        if check_file_in_drive(raw_text_path, drive_directory):\n",
        "            logger.info(f\"{raw_text_path} already exists in {drive_directory}. Skipping processing.Going to cleaning\")\n",
        "            clean_text_from_mmd(drive_directory+\"/\"+raw_text_path, GARBAGE_THRESHOLD)\n",
        "            return\n",
        "\n",
        "        # Check for the presence of raw_text_path (MML file)\n",
        "        if not os.path.exists(raw_text_path):\n",
        "            # Use nougat for text extraction if MML file is not present\n",
        "            nougat_output_path = f\"{os.path.splitext(sanitized_output_name)[0]}.mmd\"\n",
        "            nougat_command = f\"nougat {sanitized_output_name} -o {nougat_output_path} --no-skipping -m 0.1.0-base --recompute\"\n",
        "            !nougat {sanitized_output_name} -o ./ --no-skipping --recompute\n",
        "            #result = subprocess.run(nougat_command, shell=True, check=True, capture_output=True, text=True)\n",
        "            #logger.info(f\"Nougat output: {result.stdout}\")\n",
        "\n",
        "            # Check for the presence of MML output from nougat\n",
        "            if not os.path.exists(raw_text_path):\n",
        "                logger.error(f\"MML output not found for {sanitized_output_name}\")\n",
        "                return\n",
        "\n",
        "        # Upload results to drive\n",
        "        if os.path.exists(raw_text_path):\n",
        "            upload_to_drive(raw_text_path, drive_directory)\n",
        "            logger.info(f\"Uploaded {raw_text_path} to {drive_directory}\")\n",
        "\n",
        "        os.remove(sanitized_output_name)  # Clean up the downloaded PDF file\n",
        "        logger.info(f\"Removed {sanitized_output_name}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error processing {file_id}: {e}\")\n",
        "        logger.error(f\"Nougat stderr: {e.stderr}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {file_id}: {e}\")\n",
        "\n",
        "\n",
        "def process_pdfs(pdf_files, drive_directory):\n",
        "    total_pdfs = len(pdf_files)\n",
        "    progress_bar = tqdm(total=total_pdfs, desc=\"Processing PDFs\", unit=\"PDF\", leave=True)\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"Copying {pdf_file} to working directory\")\n",
        "        # Assuming you have a function to copy the file to the working directory\n",
        "        # copy_to_working_directory(pdf_file)\n",
        "        process_pdf(pdf_file, drive_directory)\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "    print(\"Completed processing all PDFs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN58P5wtll1H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96789071-32c9-444c-dada-e594558495db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 5, in <module>\n",
            "    from predict import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/predict.py\", line 18, in <module>\n",
            "    from nougat import NougatModel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nougat/__init__.py\", line 7, in <module>\n",
            "    from .model import NougatConfig, NougatModel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nougat/model.py\", line 34, in <module>\n",
            "    from nougat.transforms import train_transform, test_transform\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nougat/transforms.py\", line 146, in <module>\n",
            "    alb.ElasticTransform(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/validation.py\", line 35, in custom_init\n",
            "    config = dct[\"InitSchema\"](**full_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/main.py\", line 193, in __init__\n",
            "    self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
            "pydantic_core._pydantic_core.ValidationError: 1 validation error for InitSchema\n",
            "alpha_affine\n",
            "  Input should be None [type=none_required, input_value=1.2, input_type=float]\n",
            "    For further information visit https://errors.pydantic.dev/2.8/v/none_required\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!nougat {sanitized_output_name} -o ./ --no-skipping --recompute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxQL7-rNEzNX",
        "outputId": "9f160d38-34f4-451f-c855-9142fcbbcc5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDFs:   0%|          | 0/1 [00:00<?, ?PDF/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying /content/drive/MyDrive/deloitte/us-state-of-gen-ai-q3.pdf to working directory\n",
            "Copied /content/drive/MyDrive/deloitte/us-state-of-gen-ai-q3.pdf to us-state-of-gen-ai-q3.pdf.pdf\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 5, in <module>\n",
            "    from predict import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/predict.py\", line 18, in <module>\n",
            "    from nougat import NougatModel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nougat/__init__.py\", line 7, in <module>\n",
            "    from .model import NougatConfig, NougatModel\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nougat/model.py\", line 34, in <module>\n",
            "    from nougat.transforms import train_transform, test_transform\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nougat/transforms.py\", line 146, in <module>\n",
            "    alb.ElasticTransform(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/validation.py\", line 35, in custom_init\n",
            "    config = dct[\"InitSchema\"](**full_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pydantic/main.py\", line 193, in __init__\n",
            "    self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
            "pydantic_core._pydantic_core.ValidationError: 1 validation error for InitSchema\n",
            "alpha_affine\n",
            "  Input should be None [type=none_required, input_value=1.2, input_type=float]\n",
            "    For further information visit https://errors.pydantic.dev/2.8/v/none_required\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:MML output not found for us-state-of-gen-ai-q3.pdf.pdf\n",
            "Processing PDFs: 100%|██████████| 1/1 [00:13<00:00, 13.51s/PDF]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing all PDFs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive_directory = '/content/drive/MyDrive/deloitte/'\n",
        "\n",
        "process_pdfs(pdf_files, drive_directory)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}