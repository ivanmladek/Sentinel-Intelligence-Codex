{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanmladek/Sentinel-Intelligence-Codex/blob/main/process_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Processing Pipeline"
      ],
      "metadata": {
        "id": "markdown-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Dependencies"
      ],
      "metadata": {
        "id": "markdown-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Cybg_6vE2qM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed3a8f9-c97a-4126-b02b-8a348c858785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "libmagic-dev is already the newest version (1:5.41-3ubuntu0.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Install System Dependencies\n",
        "!apt-get install -y poppler-utils tesseract-ocr libmagic-dev unrar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Python Libraries (Part 1)\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "MO_yilG-WRis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a588353-2029-4ae1-f31a-2fbce2a0298c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Python Libraries (Part 2)\n",
        "!pip install transformers==4.38.2 pyarrow==14.0.1 timm==0.5.4 requests==2.31.0 albumentations==1.0.0 git+https://github.com/facebookresearch/nougat\n",
        "!pip install textblob langdetect beautifulsoup4 huggingface_hub tqdm pandas"
      ],
      "metadata": {
        "id": "1sjFat5tLLfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6272e258-b592-4af6-c83e-d465d3c0107d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/nougat\n",
            "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-zdsuwx5n\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-zdsuwx5n\n",
            "  Resolved https://github.com/facebookresearch/nougat to commit 5a92920d342fb6acf05fc9b594ccb4053dbe8e7a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.38.2\n",
            "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "Collecting pyarrow==14.0.1\n",
            "  Using cached pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting timm==0.5.4\n",
            "  Using cached timm-0.5.4-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting requests==2.31.0\n",
            "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting albumentations==1.0.0\n",
            "  Using cached albumentations-1.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2024.11.6)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
            "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (0.21.0+cu124)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2025.6.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (1.15.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (0.25.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (4.11.0.86)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.10.18)\n",
            "Requirement already satisfied: datasets[vision] in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (2.14.4)\n",
            "Collecting lightning<2022,>=2.0.0 (from nougat-ocr==0.1.18)\n",
            "  Using cached lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.9.1)\n",
            "Collecting rapidfuzz (from nougat-ocr==0.1.18)\n",
            "  Using cached rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (0.2.0)\n",
            "Collecting sconf>=0.2.3 (from nougat-ocr==0.1.18)\n",
            "  Using cached sconf-0.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pypdf>=3.1.0 (from nougat-ocr==0.1.18)\n",
            "  Using cached pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pypdfium2 (from nougat-ocr==0.1.18)\n",
            "  Using cached pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.1.5)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Using cached lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Using cached torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Using cached pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (0.4)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Using cached ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting munch (from sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Using cached munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.5.4) (1.3.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (1.5.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.20.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.5.4) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Using cached ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.18) (1.17.0)\n",
            "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "Downloading pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-1.0.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nougat-ocr\n",
            "  Building wheel for nougat-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nougat-ocr: filename=nougat_ocr-0.1.18-py3-none-any.whl size=81724 sha256=3ad14b3bf60620a0b6940485f4e77244d1afc1c4ac419d661c0903d9f581b0ee\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j_h63k4m/wheels/a8/b1/ca/3509cca72deee3fab5cf02371a58b5d57b82b1c99656402e8c\n",
            "Successfully built nougat-ocr\n",
            "Installing collected packages: ruamel.yaml.clib, requests, rapidfuzz, pypdfium2, pypdf, pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightning-utilities, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, sconf, nvidia-cusolver-cu12, albumentations, transformers, torchmetrics, timm, pytorch-lightning, lightning, nougat-ocr\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.8\n",
            "    Uninstalling albumentations-2.0.8:\n",
            "      Successfully uninstalled albumentations-2.0.8\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.15\n",
            "    Uninstalling timm-1.0.15:\n",
            "      Successfully uninstalled timm-1.0.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "bigframes 2.8.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed albumentations-1.0.0 lightning-2.5.2 lightning-utilities-0.14.3 munch-4.0.0 nougat-ocr-0.1.18 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-14.0.1 pypdf-5.7.0 pypdfium2-4.30.1 pytorch-lightning-2.5.2 rapidfuzz-3.13.0 requests-2.31.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 sconf-0.2.5 timm-0.5.4 tokenizers-0.15.2 torchmetrics-1.7.3 transformers-4.38.2\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Collecting langdetect\n",
            "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=0d8beb65efef9059ccaee9184e55448bc89d05e775d0185177c1c7b80f815cb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Imports and Configuration"
      ],
      "metadata": {
        "id": "markdown-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import shutil\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import HfApi\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = \"https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\"\n",
        "HUGGING_FACE_REPO = \"ivanmladek/Sentinel-Intelligence-Codex\"  # Replace with your Hugging Face repo\n",
        "GARBAGE_THRESHOLD = 0.8\n",
        "LENWORD = 50\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.propagate = True # Ensure messages are propagated to the root logger\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# --- Download NLTK Data ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615e9d8b-ffcc-4e65-da75-f2b4c431b138"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Helper Functions"
      ],
      "metadata": {
        "id": "markdown-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. File and Web Operations"
      ],
      "metadata": {
        "id": "markdown-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_file_list(url):\n",
        "    \"\"\"Recursively get a list of files from a URL and its subdirectories.\"\"\"\n",
        "    rar_files = []\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                # Handle relative and absolute links\n",
        "                full_url = requests.compat.urljoin(url, href)\n",
        "                if full_url.endswith('.rar'):\n",
        "                    rar_files.append(full_url)\n",
        "                elif full_url.endswith('/'):\n",
        "                    # Recursively call for subdirectories, avoiding infinite loops\n",
        "                    if url != full_url: # Avoid processing the same directory again\n",
        "                         rar_files.extend(get_file_list(full_url))\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error accessing URL {url}: {e}\")\n",
        "    return rar_files\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    \"\"\"Download a file from a URL.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        logger.info(f\"{output_path} already exists. Skipping download.\")\n",
        "        return\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        logger.info(f\"Downloaded {url} to {output_path}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error downloading file from {url}: {e}\")\n",
        "\n",
        "\n",
        "def extract_rar(file_path, output_path):\n",
        "    \"\"\"Extract a RAR file.\"\"\"\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    try:\n",
        "        subprocess.run(['unrar', 'x', '-o+', file_path, output_path], check=True, capture_output=True, text=True) # Added -o+ to overwrite without prompting\n",
        "        logger.info(f\"Extracted {file_path} to {output_path}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error extracting {file_path}: {e.stderr}\")\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Sanitize a filename.\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)"
      ],
      "metadata": {
        "id": "file-ops-cell"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. PDF Processing (Nougat)"
      ],
      "metadata": {
        "id": "markdown-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf(pdf_path, output_dir):\n",
        "    \"\"\"Process a single PDF file with Nougat.\"\"\"\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(pdf_path))\n",
        "    mmd_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}.mmd\")\n",
        "    if os.path.exists(mmd_path):\n",
        "        logger.info(f\"{mmd_path} already exists. Skipping Nougat processing.\")\n",
        "        return mmd_path\n",
        "\n",
        "    try:\n",
        "        subprocess.run(['nougat', pdf_path, '-o', output_dir, '--no-skipping', '--recompute'], check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error processing {pdf_path} with Nougat: {e.stderr}\")\n",
        "        return None\n",
        "    return mmd_path"
      ],
      "metadata": {
        "id": "nougat-cell"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Text Cleaning and Quality Control"
      ],
      "metadata": {
        "id": "markdown-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean the extracted text.\"\"\"\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, lenword=LENWORD):\n",
        "    \"\"\"Check if the text is garbage.\"\"\"\n",
        "    if not text or len(text.split()) < 10:\n",
        "        return True\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            return True\n",
        "    except LangDetectException:\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "cleaning-cell"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Text Chunking"
      ],
      "metadata": {
        "id": "markdown-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(content, max_size=8192):\n",
        "    \"\"\"Chunk the text into smaller segments.\"\"\"\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith((\"#\", \"##\", \"###\")):\n",
        "            if current_segment:\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            current_segment = \"\"\n",
        "        else:\n",
        "            current_segment += line + \" \"\n",
        "\n",
        "    if current_segment:\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    return segments\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    \"\"\"Split a segment into smaller chunks.\"\"\"\n",
        "    sentences = sent_tokenize(segment)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 1 <= max_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_and_chunk_mmd(mmd_path, output_dir):\n",
        "    \"\"\"Process and chunk an MMD file.\"\"\"\n",
        "    if not mmd_path or not os.path.exists(mmd_path):\n",
        "        logger.warning(f\"MMD file not found: {mmd_path}. Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    cleaned_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(mmd_path))[0]}_cleaned.jsonl\")\n",
        "    garbage_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(mmd_path))[0]}_garbage.jsonl\")\n",
        "\n",
        "    with open(mmd_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    chunks = chunk_text(content)\n",
        "    with open(cleaned_jsonl_path, 'w') as cleaned_f, open(garbage_jsonl_path, 'w') as garbage_f:\n",
        "        for chunk in chunks:\n",
        "            cleaned_chunk = clean_text(chunk)\n",
        "            if is_garbage(cleaned_chunk):\n",
        "                garbage_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "            else:\n",
        "                cleaned_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "\n",
        "    return cleaned_jsonl_path, garbage_jsonl_path"
      ],
      "metadata": {
        "id": "chunking-cell"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Hugging Face Integration"
      ],
      "metadata": {
        "id": "markdown-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_huggingface(file_path, repo_id):\n",
        "    \"\"\"Upload a file to a Hugging Face repository.\"\"\"\n",
        "    api = HfApi()\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=file_path,\n",
        "        path_in_repo=os.path.basename(file_path),\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "    logger.info(f\"Uploaded {file_path} to {repo_id}\")"
      ],
      "metadata": {
        "id": "huggingface-cell"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Main Processing Loop"
      ],
      "metadata": {
        "id": "markdown-10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to process the library.\"\"\"\n",
        "    logger.info(f\"Scanning for RAR files at {BASE_URL}\")\n",
        "    rar_files = get_file_list(BASE_URL)\n",
        "    logger.info(f\"Found {len(rar_files)} RAR files.\")\n",
        "\n",
        "    with tqdm(total=len(rar_files), desc=\"Processing RAR Files\") as pbar:\n",
        "        for rar_file_url in rar_files:\n",
        "            rar_filename = rar_file_url.split('/')[-1]\n",
        "            rar_path = sanitize_filename(rar_filename)\n",
        "            extract_path = os.path.splitext(rar_path)[0]\n",
        "\n",
        "            logger.info(f\"Downloading {rar_file_url} to {rar_path}\")\n",
        "            download_file(rar_file_url, rar_path)\n",
        "\n",
        "            logger.info(f\"Extracting {rar_path} to {extract_path}\")\n",
        "            extract_rar(rar_path, extract_path)\n",
        "\n",
        "            pdf_files = [os.path.join(root, file) for root, _, files in os.walk(extract_path) for file in files if file.endswith('.pdf')]\n",
        "            logger.info(f\"Found {len(pdf_files)} PDF files in {extract_path}\")\n",
        "\n",
        "            for pdf_path in pdf_files:\n",
        "                logger.info(f\"Processing PDF: {pdf_path}\")\n",
        "                mmd_path = process_pdf(pdf_path, extract_path)\n",
        "                cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, extract_path)\n",
        "\n",
        "                if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "                    upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO)\n",
        "                ##if garbage_jsonl and os.path.exists(garbage_jsonl):\n",
        "                ##    upload_to_huggingface(garbage_jsonl, HUGGING_FACE_REPO)\n",
        "\n",
        "            os.remove(rar_path)\n",
        "            shutil.rmtree(extract_path)\n",
        "            pbar.update(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "main-loop-cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}