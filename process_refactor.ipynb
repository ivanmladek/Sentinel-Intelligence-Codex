{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanmladek/Sentinel-Intelligence-Codex/blob/main/process_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-1"
      },
      "source": [
        "# Library Processing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYux-yCBECz3"
      },
      "source": [
        "The process of extracting, cleaning, and preparing the text from PDF files for the LLM is a multi-stage pipeline designed to ensure high-quality, structured data. This process is orchestrated by the process_refactor.ipynb notebook.\n",
        "\n",
        "1. Environment Setup and PDF Discovery\n",
        "\n",
        "Dependencies: The process begins by installing necessary Python libraries, including nougat-ocr for text extraction, nltk for natural language processing, and langdetect for language identification.\n",
        "PDF Discovery: The script recursively scans a specified directory (e.g., a Google Drive folder) to locate all PDF files.\n",
        "2. Text Extraction with Nougat\n",
        "\n",
        "Nougat OCR: For each PDF, the nougat command-line tool is used. Nougat is a state-of-the-art OCR tool specifically designed for academic and scientific documents, capable of recognizing and transcribing complex layouts, mathematical equations, and tables into a structured Markdown format (.mmd).\n",
        "Output: The raw extracted text is saved as a .mmd file, preserving the document's structure with Markdown headings.\n",
        "3. Text Cleaning and Garbage Detection\n",
        "\n",
        "This is a critical step to filter out irrelevant or low-quality text.\n",
        "\n",
        "Cleaning: A series of regular expressions and cleaning functions are applied to the raw text to:\n",
        "Remove extra newlines, spaces, and non-ASCII characters.\n",
        "Eliminate academic citations, references to tables/figures, and bracketed content.\n",
        "Sanitize garbled punctuation and symbols.\n",
        "Garbage Detection: Each segment of text is evaluated against a set of criteria to identify and discard \"garbage\" content. This includes:\n",
        "Language Detection: Text that is not identified as English is discarded.\n",
        "Heuristics: Checks for \"jammed\" words (long strings of characters without spaces), an unusually high proportion of single-letter words, and repetitive patterns.\n",
        "Quality Scoring: A text_quality_score is calculated based on the presence of common English words, proper part-of-speech patterns, and other linguistic features. Text falling below a certain threshold is flagged as garbage.\n",
        "4. Tokenization and Chunking\n",
        "\n",
        "Chunking Strategy: The cleaned .mmd content is chunked into smaller, manageable segments suitable for the LLM. The chunking logic is designed to respect the document's structure:\n",
        "The text is split by Markdown headings (#, ##, ###).\n",
        "These larger sections are then further divided into sentences using nltk.sent_tokenize.\n",
        "Size Constraints: The sentences are grouped into chunks with a maximum size (e.g., 8192 characters) to ensure they fit within the model's context window, while avoiding splitting sentences in the middle.\n",
        "Final Output: The cleaned, chunked text is saved to a .jsonl file, with each line containing a JSON object with a single \"text\" key, ready for training the LLM. Garbage text is saved to a separate file for review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-2"
      },
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cybg_6vE2qM",
        "outputId": "bf0d319b-f24b-4d56-8b6f-0325d92c4dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "libmagic-dev is already the newest version (1:5.41-3ubuntu0.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Install System Dependencies\n",
        "!apt-get install -y poppler-utils tesseract-ocr libmagic-dev unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MO_yilG-WRis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599ff0db-2d55-4355-9606-a9cccf1661bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Python Libraries (Part 1)\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1sjFat5tLLfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b6e2df-a84e-4ae2-aedc-ba506c70af17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/nougat\n",
            "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-gl8jihlj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-gl8jihlj\n",
            "  Resolved https://github.com/facebookresearch/nougat to commit 5a92920d342fb6acf05fc9b594ccb4053dbe8e7a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.11/dist-packages (4.38.2)\n",
            "Requirement already satisfied: pyarrow==14.0.1 in /usr/local/lib/python3.11/dist-packages (14.0.1)\n",
            "Requirement already satisfied: timm==0.5.4 in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.11/dist-packages (2.31.0)\n",
            "Requirement already satisfied: albumentations==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (0.21.0+cu124)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2025.6.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (1.15.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (0.25.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (4.11.0.86)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.10.18)\n",
            "Requirement already satisfied: datasets[vision] in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (2.14.4)\n",
            "Requirement already satisfied: lightning<2022,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (2.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.9.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.13.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (0.2.0)\n",
            "Requirement already satisfied: sconf>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (0.2.5)\n",
            "Requirement already satisfied: pypdf>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (5.7.0)\n",
            "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (4.30.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.1.5)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (0.14.3)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (1.7.3)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (2.5.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (0.4)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.11/dist-packages (from sconf>=0.2.3->nougat-ocr==0.1.18) (0.18.14)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from sconf>=0.2.3->nougat-ocr==0.1.18) (4.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.5.4) (1.3.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (1.5.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.20.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.5.4) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.18) (0.2.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.18) (1.17.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Python Libraries (Part 2)\n",
        "!pip install transformers==4.38.2 pyarrow==14.0.1 timm==0.5.4 requests==2.31.0 albumentations==1.0.0 git+https://github.com/facebookresearch/nougat\n",
        "!pip install textblob langdetect beautifulsoup4 huggingface_hub tqdm pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-3"
      },
      "source": [
        "## 2. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8f0908-0319-43fd-9346-e3de24382fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import HfApi\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = \"https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\"\n",
        "HUGGING_FACE_REPO = \"Disperser5601/Sentinel-Intelligence-Codex\"  # Replace with your Hugging Face repo\n",
        "GARBAGE_THRESHOLD = 0.5\n",
        "LENWORD = 50\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.propagate = True # Ensure messages are propagated to the root logger\n",
        "\n",
        "# Explicitly set the logging level and add a handler to print to stdout\n",
        "logger.setLevel(logging.INFO)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "# Avoid adding duplicate handlers if the cell is run multiple times\n",
        "if not logger.handlers:\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# --- Download NLTK Data ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-4"
      },
      "source": [
        "## 3. Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-5"
      },
      "source": [
        "### 3.1. File and Web Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "file-ops-cell"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import logging\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_file_list(url, depth=0, max_depth=3):\n",
        "    \"\"\"Recursively get a list of files from a URL and its subdirectories up to a max depth, avoiding backlinks.\"\"\"\n",
        "    if depth > max_depth:\n",
        "        logger.debug(f\"Max depth ({max_depth}) reached at URL: {url}. Stopping recursion.\")\n",
        "        return []\n",
        "\n",
        "    rar_files = []\n",
        "    # Configure retries\n",
        "    retry_strategy = Retry(\n",
        "        total=3,  # Number of retries\n",
        "        backoff_factor=1, # Factor by which the delay increases\n",
        "        status_forcelist=[429, 500, 502, 503, 504] # HTTP status codes to retry on\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    http = requests.Session()\n",
        "    http.mount(\"http://\", adapter)\n",
        "    http.mount(\"https://\", adapter)\n",
        "\n",
        "    logger.info(f\"Accessing URL: {url} (Depth: {depth})\")\n",
        "    try:\n",
        "        response = http.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                # Handle relative and absolute links\n",
        "                full_url = requests.compat.urljoin(url, href)\n",
        "\n",
        "                # Ensure we only go deeper into subdirectories, avoiding backlinks\n",
        "                if full_url.startswith(url) and len(full_url) > len(url) and full_url.endswith('/'):\n",
        "                     logger.debug(f\"Found subdirectory: {full_url}. Recursing.\")\n",
        "                     rar_files.extend(get_file_list(full_url, depth + 1, max_depth))\n",
        "                elif full_url.endswith('.rar'):\n",
        "                    logger.debug(f\"Found RAR file: {full_url}\")\n",
        "                    rar_files.append(full_url)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error accessing URL {url}: {e}\")\n",
        "    logger.debug(f\"Finished processing URL: {url}. Found {len(rar_files)} RAR files in this branch.\")\n",
        "    return rar_files\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    \"\"\"Download a file from a URL.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        logger.info(f\"{output_path} already exists. Skipping download.\")\n",
        "        return True # Indicate success as file exists\n",
        "    logger.info(f\"Attempting to download {url} to {output_path}\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        logger.info(f\"Successfully downloaded {url} to {output_path}\")\n",
        "        return True # Indicate success\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error downloading file from {url}: {e}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "\n",
        "def extract_rar(file_path, output_path):\n",
        "    \"\"\"Extract a RAR file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"RAR file not found for extraction: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "        logger.debug(f\"Created output directory for extraction: {output_path}\")\n",
        "    logger.info(f\"Attempting to extract {file_path} to {output_path}\")\n",
        "    try:\n",
        "        # Added -o+ to overwrite without prompting\n",
        "        result = subprocess.run(['unrar', 'x', '-o+', file_path, output_path], check=True, capture_output=True, text=True)\n",
        "        logger.info(f\"Successfully extracted {file_path} to {output_path}\")\n",
        "        # Log stdout and stderr for debugging\n",
        "        if result.stdout:\n",
        "            logger.debug(f\"Unrar stdout for {file_path}:\\n{result.stdout}\")\n",
        "        if result.stderr:\n",
        "             logger.debug(f\"Unrar stderr for {file_path}:\\n{result.stderr}\")\n",
        "        return True # Indicate success\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error extracting {file_path}: {e.stderr}\")\n",
        "        return False # Indicate failure\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"Unrar command not found. Please ensure 'unrar' is installed.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during extraction of {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Sanitize a filename.\"\"\"\n",
        "    sanitized = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)\n",
        "    logger.debug(f\"Sanitized filename '{filename}' to '{sanitized}'\")\n",
        "    return sanitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-6"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### 3.2. PDF Processing (Nougat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nougat-cell"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import logging\n",
        "import sys # Import sys for streaming stdout\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def process_pdf(pdf_path, output_dir):\n",
        "    if not os.path.exists(pdf_path):\n",
        "        logger.error(f\"PDF file not found for processing: {pdf_path}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        logger.debug(f\"Created output directory for Nougat: {output_dir}\")\n",
        "\n",
        "    # Construct the expected output filename based on Nougat's default behavior\n",
        "    # Nougat typically replaces the PDF extension with .mmd in the output directory\n",
        "    pdf_filename = os.path.basename(pdf_path)\n",
        "    # MODIFICATION: Construct expected_mmd_filename using the original pdf_filename\n",
        "    expected_mmd_filename = f\"{os.path.splitext(pdf_filename)[0]}.mmd\"\n",
        "    mmd_path = os.path.join(output_dir, expected_mmd_filename)\n",
        "\n",
        "    if os.path.exists(mmd_path):\n",
        "        logger.info(f\"{mmd_path} already exists. Skipping Nougat processing for {pdf_path}.\")\n",
        "        return mmd_path\n",
        "\n",
        "    logger.info(f\"Attempting to process PDF: {pdf_path} with Nougat. Output to {output_dir}\")\n",
        "\n",
        "    try:\n",
        "        # Stream output live using Popen\n",
        "        # Ensure the input path to nougat is the original path, not sanitized\n",
        "        process = subprocess.Popen(\n",
        "            ['nougat', pdf_path, '-o', output_dir, '--no-skipping', '--recompute'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT, # Capture stderr and merge with stdout\n",
        "            bufsize=1,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        # Stream output to stdout\n",
        "        for line in process.stdout:\n",
        "            sys.stdout.write(line)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        process.stdout.close()\n",
        "        return_code = process.wait()\n",
        "\n",
        "        if return_code != 0:\n",
        "            logger.error(f\"Nougat process failed with exit code {return_code}\")\n",
        "            return None\n",
        "\n",
        "        logger.info(f\"Nougat command finished with exit code {return_code}. Checking for output file.\")\n",
        "\n",
        "        # Explicitly check if the expected output file was created\n",
        "        if os.path.exists(mmd_path):\n",
        "            logger.info(f\"Successfully processed {pdf_path} with Nougat. MMD file created at {mmd_path}.\")\n",
        "            return mmd_path\n",
        "        else:\n",
        "            logger.error(f\"Nougat command finished but expected output {mmd_path} not found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during Nougat processing of {pdf_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-7"
      },
      "source": [
        "### 3.3. Text Cleaning and Quality Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cleaning-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfe14f7-d5cb-41ef-d891-947c5e0c924f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:23:35,504 - INFO - NLTK English words corpus loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:NLTK English words corpus loaded.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import logging\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words # Assuming these are downloaded in config-cell\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize # Assuming this is downloaded in config-cell\n",
        "from textblob import TextBlob # Assuming TextBlob is installed\n",
        "import nltk # Import nltk for FreqDist\n",
        "\n",
        "# Assuming GARBAGE_THRESHOLD and LENWORD are defined in config-cell\n",
        "# from .config import GARBAGE_THRESHOLD, LENWORD # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the NLTK words corpus for garbage detection\n",
        "# Ensure this is done after nltk.download('words') in config-cell\n",
        "try:\n",
        "    ENGLISH_WORDS = set(words.words())\n",
        "    logger.info(\"NLTK English words corpus loaded.\")\n",
        "except LookupError:\n",
        "    logger.error(\"NLTK 'words' corpus not found. Please run nltk.download('words').\")\n",
        "    ENGLISH_WORDS = set() # Use an empty set to avoid errors\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean the extracted text.\"\"\"\n",
        "    logger.debug(f\"Cleaning text (first 100 chars): {text[:100]}...\")\n",
        "    initial_len = len(text)\n",
        "    # Remove markdown headings at the beginning of lines\n",
        "    text = re.sub(r'^\\s*#+\\s+.*', '', text, flags=re.MULTILINE)\n",
        "    # Remove markdown emphasis markers (*, _, **, __)\n",
        "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text) # Bold (**word**, __word__)\n",
        "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)   # Italic (*word*, _word_)\n",
        "    # Remove extra newlines, spaces, and non-ASCII characters.\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    # MODIFICATION: Add more general rules for removing content within parentheses and square brackets\n",
        "    # Remove content within square brackets (more general)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    # Remove content within parentheses (more general)\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "\n",
        "    # Remove specific academic citations, references to tables/figures, etc.\n",
        "    # (Keeping these as they might catch patterns not covered by the general rules,\n",
        "    # but the general rules should handle most cases)\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "\n",
        "    # Remove page numbers and stray numerical artifacts (e.g., single numbers on a line or at the end of a line)\n",
        "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE) # Remove lines containing only numbers\n",
        "    text = re.sub(r'\\s+\\d+$', '', text) # Remove numbers at the end of a line\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    # Sanitize garbled punctuation and symbols.\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text)\n",
        "    logger.debug(f\"Cleaned text (first 100 chars, original len {initial_len}): {text[:100]}...\")\n",
        "    return text\n",
        "\n",
        "def calculate_text_quality_score(text):\n",
        "    \"\"\"Calculate a quality score based on English words and sentence structure.\"\"\"\n",
        "    if not text:\n",
        "        return 0.0\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    if not words:\n",
        "        return 0.0\n",
        "\n",
        "    english_word_count = sum(1 for word in words if word.lower() in ENGLISH_WORDS)\n",
        "    english_word_ratio = english_word_count / len(words) if words else 0\n",
        "\n",
        "    # Simple heuristic for sentence structure (check for punctuation at end of sentences)\n",
        "    sentences = sent_tokenize(text)\n",
        "    well_formed_sentences = sum(1 for sent in sentences if sent.strip().endswith(('.', '!', '?')))\n",
        "    sentence_structure_score = well_formed_sentences / len(sentences) if sentences else 0\n",
        "\n",
        "    # Combine ratios - adjust weights as needed\n",
        "    quality_score = (english_word_ratio * 0.7) + (sentence_structure_score * 0.3)\n",
        "\n",
        "    logger.debug(f\"Text quality score calculated: {quality_score} for text (first 50 chars): {text[:50]}...\")\n",
        "    return quality_score\n",
        "\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, lenword=LENWORD):\n",
        "    \"\"\"Check if the text is garbage based on various heuristics.\"\"\"\n",
        "    logger.debug(f\"Checking if text is garbage (first 100 chars): {text[:100]}...\")\n",
        "\n",
        "    # Check for minimal length\n",
        "    if not text or len(text.split()) < 5: # Reduced minimum words\n",
        "        logger.debug(\"Identified as garbage: text too short or empty.\")\n",
        "        return True\n",
        "\n",
        "    # Language detection\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            logger.debug(\"Identified as garbage: language not English.\")\n",
        "            return True\n",
        "    except LangDetectException as e:\n",
        "        logger.debug(f\"Language detection failed for text (first 50 chars): {text[:50]}... Error: {e}. Assuming garbage.\")\n",
        "        return True # Assume garbage if language detection fails\n",
        "\n",
        "    # Check for jammed words (long strings without spaces)\n",
        "    words_list = text.split()\n",
        "    for word in words_list:\n",
        "        if len(word) > lenword and not '-' in word: # Allow hyphens in long words\n",
        "             logger.debug(f\"Identified as garbage: found jammed word '{word[:50]}...'\")\n",
        "             return True\n",
        "\n",
        "    # Check for unusually high proportion of single-letter words\n",
        "    single_letter_words = sum(1 for word in words_list if len(word) == 1)\n",
        "    if len(words_list) > 0 and single_letter_words / len(words_list) > 0.2: # More than 20% single letters\n",
        "        logger.debug(\"Identified as garbage: high proportion of single-letter words.\")\n",
        "        return True\n",
        "\n",
        "    # Check for simple repetitive character patterns\n",
        "    if re.search(r'(.)\\1{4,}', text): # 5 or more of the same character in a row\n",
        "        logger.debug(\"Identified as garbage: found repetitive character pattern.\")\n",
        "        return True\n",
        "\n",
        "    # Check for highly frequent n-grams (repetitive phrases)\n",
        "    words = word_tokenize(text)\n",
        "    if len(words) > 5: # Only check for n-grams if there are enough words\n",
        "        # Check trigrams (sequences of 3 words)\n",
        "        trigrams = list(nltk.ngrams(words, 3))\n",
        "        if trigrams: # Ensure there are trigrams to analyze\n",
        "            fdist = nltk.FreqDist(trigrams)\n",
        "            most_common_trigram, count = fdist.most_common(1)[0]\n",
        "            # Define a threshold for high frequency (e.g., if the most common trigram\n",
        "            # appears more than 10% of the time among all trigrams)\n",
        "            if count / len(trigrams) > 0.10: # Adjusted threshold based on example\n",
        "                 logger.debug(f\"Identified as garbage: found highly frequent trigram '{' '.join(most_common_trigram)}' (count: {count}/{len(trigrams)}).\")\n",
        "                 return True\n",
        "\n",
        "    # Optimized check for significant large-scale repetition (comparing segments with offsets)\n",
        "    text_len = len(text)\n",
        "    # Only perform this check on reasonably long texts to avoid overhead on short chunks\n",
        "    if text_len > 200:\n",
        "        # Check offsets that are significant fractions of the text length\n",
        "        offsets_to_check = [text_len // 2, text_len // 3, text_len // 4]\n",
        "        min_match_len = max(50, text_len // 10) # Require a minimum match length, at least 50 chars or 10% of text length\n",
        "\n",
        "        for offset in offsets_to_check:\n",
        "            if offset > 0 and text_len > offset + min_match_len:\n",
        "                segment1 = text[:text_len - offset]\n",
        "                segment2 = text[offset:text_len]\n",
        "                # Find the length of the longest common prefix between the two segments\n",
        "                match_len = 0\n",
        "                min_segment_len = min(len(segment1), len(segment2))\n",
        "                while match_len < min_segment_len and segment1[match_len] == segment2[match_len]:\n",
        "                    match_len += 1\n",
        "\n",
        "                # If a significant match is found\n",
        "                if match_len >= min_match_len:\n",
        "                     logger.debug(f\"Identified as garbage: found significant repetition with offset {offset}, match length {match_len}.\")\n",
        "                     return True\n",
        "\n",
        "\n",
        "    # Quality scoring\n",
        "    quality_score = calculate_text_quality_score(text)\n",
        "    if quality_score < threshold:\n",
        "        logger.debug(f\"Identified as garbage: quality score {quality_score} below threshold {threshold}.\")\n",
        "        return True\n",
        "\n",
        "    logger.debug(\"Text passed garbage checks.\")\n",
        "    return False\n",
        "\n",
        "# Ensure GARBAGE_THRESHOLD and LENWORD are available if not in this cell\n",
        "# GARBAGE_THRESHOLD = 0.8 # Example default\n",
        "# LENWORD = 50 # Example default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-8"
      },
      "source": [
        "### 3.4. Text Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "chunking-cell"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize # Assuming this is downloaded in config-cell\n",
        "\n",
        "# Assuming clean_text and is_garbage are defined in cleaning-cell and available\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def chunk_text(content, max_size=8192):\n",
        "    \"\"\"Chunk the text into smaller segments, respecting markdown headings.\"\"\"\n",
        "    logger.debug(f\"Starting chunking process with max_size={max_size}.\")\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    lines = content.split('\\n')\n",
        "    logger.debug(f\"Splitting content into {len(lines)} lines.\")\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        # Check for markdown headings\n",
        "        if line.strip().startswith((\"# \", \"## \", \"### \")):\n",
        "            logger.debug(f\"Found markdown heading at line {i}: {line.strip()}\")\n",
        "            # If the current segment is not empty, process it before starting a new one\n",
        "            if current_segment:\n",
        "                logger.debug(f\"Processing previous segment before heading (length: {len(current_segment)}).\")\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            # Start a new segment with the heading line\n",
        "            current_segment = line + \"\\n\" # Keep the heading line in the new segment\n",
        "            logger.debug(\"Starting new segment after heading.\")\n",
        "        else:\n",
        "            # Add non-heading lines to the current segment\n",
        "            current_segment += line + \"\\n\"\n",
        "\n",
        "    # Process any remaining content in the last segment\n",
        "    if current_segment:\n",
        "        logger.debug(f\"Processing final segment (length: {len(current_segment)}).\")\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    logger.info(f\"Chunking complete. Produced {len(segments)} initial segments based on headings.\")\n",
        "    return segments\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    \"\"\"Split a segment (potentially from a heading section) into smaller chunks by sentences.\"\"\"\n",
        "    logger.debug(f\"Splitting segment by sentences (length: {len(segment)}).\")\n",
        "    sentences = sent_tokenize(segment)\n",
        "    logger.debug(f\"Segment split into {len(sentences)} sentences.\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Add a space before adding the new sentence if the current chunk is not empty\n",
        "        sentence_to_add = sentence + \" \" if current_chunk else sentence\n",
        "        # Check if adding the current sentence exceeds the max size\n",
        "        if len(current_chunk) + len(sentence_to_add) <= max_size:\n",
        "            current_chunk += sentence_to_add\n",
        "            logger.debug(f\"Added sentence {i+1}/{len(sentences)} to current chunk (current size: {len(current_chunk)}).\")\n",
        "        else:\n",
        "            # If adding the sentence exceeds max size, add the current chunk to chunks list\n",
        "            if current_chunk: # Add the chunk only if it's not empty\n",
        "                chunks.append(current_chunk.strip())\n",
        "                logger.debug(f\"Chunk completed (size: {len(current_chunk)}). Starting new chunk with sentence {i+1}.\")\n",
        "            # Start a new chunk with the current sentence\n",
        "            current_chunk = sentence + \" \" # Start new chunk with the current sentence\n",
        "\n",
        "    # Add the last current chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "        logger.debug(f\"Added final chunk (size: {len(current_chunk)}).\")\n",
        "\n",
        "    logger.debug(f\"Segment split into {len(chunks)} smaller chunks.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def process_and_chunk_mmd(mmd_path, output_dir):\n",
        "    \"\"\"Process, clean, chunk, and categorize text from an MMD file.\"\"\"\n",
        "    logger.info(f\"Starting processing and chunking for MMD file: {mmd_path}\")\n",
        "\n",
        "    if not mmd_path or not os.path.exists(mmd_path):\n",
        "        logger.warning(f\"MMD file not found or path is invalid: {mmd_path}. Skipping processing and chunking.\")\n",
        "        return None, None\n",
        "\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(mmd_path))\n",
        "    cleaned_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_cleaned.jsonl\")\n",
        "    garbage_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_garbage.jsonl\")\n",
        "\n",
        "    #always reprocess with new cleaning and chunking algos\n",
        "    #if os.path.exists(cleaned_jsonl_path) and os.path.exists(garbage_jsonl_path):\n",
        "    #    logger.info(f\"Output files {cleaned_jsonl_path} and {garbage_jsonl_path} already exist. Skipping processing and chunking for {mmd_path}.\")\n",
        "    #    return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    try:\n",
        "        with open(mmd_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        logger.debug(f\"Successfully read content from {mmd_path} (length: {len(content)}).\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading MMD file {mmd_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    chunks = chunk_text(content)\n",
        "    logger.info(f\"MMD content chunked into {len(chunks)} segments.\")\n",
        "\n",
        "    cleaned_count = 0\n",
        "    garbage_count = 0\n",
        "\n",
        "    try:\n",
        "        with open(cleaned_jsonl_path, 'w', encoding='utf-8') as cleaned_f, \\\n",
        "             open(garbage_jsonl_path, 'w', encoding='utf-8') as garbage_f:\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Processing chunk {i+1}/{len(chunks)} (length: {len(chunk)}).\")\n",
        "                cleaned_chunk = clean_text(chunk)\n",
        "                if is_garbage(cleaned_chunk):\n",
        "                    garbage_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    garbage_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as garbage.\")\n",
        "                else:\n",
        "                    cleaned_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    cleaned_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as cleaned text.\")\n",
        "\n",
        "        logger.info(f\"Finished processing and chunking {mmd_path}. Generated {cleaned_count} cleaned chunks and {garbage_count} garbage chunks.\")\n",
        "        return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during cleaning or writing chunk files for {mmd_path}: {e}\")\n",
        "        # Clean up potentially incomplete files\n",
        "        if os.path.exists(cleaned_jsonl_path):\n",
        "            os.remove(cleaned_jsonl_path)\n",
        "        if os.path.exists(garbage_jsonl_path):\n",
        "            os.remove(garbage_jsonl_path)\n",
        "        return None, None\n",
        "\n",
        "# Ensure sanitize_filename, clean_text, is_garbage are available\n",
        "# from .file_ops import sanitize_filename # Example if in a different file\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-9"
      },
      "source": [
        "### 3.5. Hugging Face Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "huggingface-cell",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378,
          "referenced_widgets": [
            "afd094b67dc941daab8fb2bd5682bca0",
            "50ca2b02bc4e4f0b9211b28bc988d4ad",
            "4e6318dd69644c9dba162722422d1e77",
            "474f5e731cec4281a378e50192728899",
            "9eac803af1ce4d07928d92175385b8fb",
            "d794064f2e644a3a901bee71553ee31b",
            "c7030757c52c4a71bacb480e237731b5",
            "50a7600a6e684ef18798abc58970885a",
            "44af9caf73e5487eae974d36496f7c27",
            "2ff39d1d0a2648cdb972e84406a30b2d",
            "b6b43dcb1bd54adeb57e63bada5665a2",
            "990cb884626941bb9d1fcc1de8931627",
            "09d3bacfcd284f6d948e86ee575a1b75",
            "b883848e739a4628a801d3d0648e5e5c",
            "a0ded3171476457aa8134d5dd4723baf",
            "1587a1e5a92f4c3aaa80d81ad23c38b9",
            "31a43c198b3f4bd6acb63f9f26853b16"
          ]
        },
        "outputId": "553bc2b2-b56f-40c3-df1f-2b2019d35cf9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afd094b67dc941daab8fb2bd5682bca0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "from huggingface_hub import HfApi, Repository,login # Import Repository for better practice if needed for cloning/managing\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "login()\n",
        "\n",
        "def upload_to_huggingface(file_path, repo_id, repo_type=\"dataset\"):\n",
        "    \"\"\"Upload a file to a Hugging Face repository.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"File not found for upload to Hugging Face: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "    logger.info(f\"Attempting to upload {file_path} to Hugging Face repo '{repo_id}' (type: {repo_type}).\")\n",
        "    api = HfApi()\n",
        "\n",
        "    # Log in to the Hugging Face Hub\n",
        "    # This will prompt you to enter your token or use a token from your environment/secrets\n",
        "    try:\n",
        "        # Use create_commit for potentially better handling of multiple files or larger uploads\n",
        "        # This example uses upload_file for simplicity as in the original code\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=os.path.basename(file_path),\n",
        "            repo_id=repo_id,\n",
        "            repo_type=repo_type,\n",
        "            # Optional: add commit_message, token if not using environment variable\n",
        "        )\n",
        "        logger.info(f\"Successfully uploaded {file_path} to {repo_id}\")\n",
        "        return True # Indicate success\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error uploading {file_path} to Hugging Face repo '{repo_id}': {e}\")\n",
        "        return False # Indicate failure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-10"
      },
      "source": [
        "## 4. Main Processing Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8f2ecb"
      },
      "source": [
        "## Scan and process local pdfs\n",
        "\n",
        "### Subtask:\n",
        "Modify the main loop to first search for and process any existing PDF files within the anticipated output directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f5ce23e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09f13f3-9c8d-43cf-fa63-2202f913a0e4"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # Import ThreadPoolExecutor\n",
        "# Assuming helper functions are defined in other cells and available\n",
        "# from .file_ops import get_file_list, download_file, extract_rar, sanitize_filename\n",
        "# from .nougat_processing import process_pdf\n",
        "# from .chunking import process_and_chunk_mmd\n",
        "# from .huggingface_integration import upload_to_huggingface\n",
        "# from .config import BASE_URL, HUGGING_FACE_REPO # Assuming these are defined in config-cell\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define a cache file path\n",
        "RAR_LIST_CACHE = \"rar_list_cache.json\"\n",
        "\n",
        "def process_single_pdf_local(pdf_path, HUGGING_FACE_REPO):\n",
        "    \"\"\"Processes a single local PDF file: processes with Nougat, cleans, chunks, and uploads.\"\"\"\n",
        "    logger.info(f\"--- Processing local PDF: {pdf_path} ---\")\n",
        "\n",
        "    output_dir = os.path.dirname(pdf_path) # Use the PDF's directory as the output directory\n",
        "\n",
        "    # 1. Process PDF with Nougat\n",
        "    mmd_path = process_pdf(pdf_path, output_dir)\n",
        "\n",
        "    if mmd_path:\n",
        "        logger.info(f\"Nougat processing successful for {pdf_path}. MMD file: {mmd_path}\")\n",
        "        # 2. Clean and Chunk MMD file\n",
        "        cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, output_dir)\n",
        "\n",
        "        # 3. Upload to Hugging Face\n",
        "        if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "            logger.info(f\"Uploading cleaned data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "            if upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO):\n",
        "                return 1 # Indicate success\n",
        "            else:\n",
        "                logger.error(f\"Failed to upload cleaned data for {os.path.basename(pdf_path)}.\")\n",
        "                return 0 # Indicate failure\n",
        "        else:\n",
        "            logger.warning(f\"No cleaned data generated for {os.path.basename(pdf_path)}. Skipping upload.\")\n",
        "            return 0 # Indicate no cleaned data\n",
        "\n",
        "    else:\n",
        "        logger.error(f\"Nougat processing failed for {pdf_path}. Skipping cleaning, chunking, and upload.\")\n",
        "        return 0 # Indicate failure\n",
        "\n",
        "\n",
        "def process_single_rar(rar_file_url, HUGGING_FACE_REPO):\n",
        "    \"\"\"Processes a single RAR file: downloads, extracts, processes PDFs, and uploads.\"\"\"\n",
        "    rar_filename = rar_file_url.split('/')[-1]\n",
        "    sanitized_rar_filename = sanitize_filename(rar_filename)\n",
        "    rar_path = sanitized_rar_filename\n",
        "    extract_path = os.path.splitext(rar_path)[0]\n",
        "\n",
        "    logger.info(f\"--- Processing {rar_filename} ---\")\n",
        "\n",
        "    # 1. Download RAR file\n",
        "    # The download_file function now correctly returns True on success\n",
        "    if not download_file(rar_file_url, rar_path):\n",
        "        logger.error(f\"Failed to download RAR file: {rar_file_url}. Skipping.\")\n",
        "        return 0\n",
        "\n",
        "    # 2. Extract RAR file\n",
        "    if not extract_rar(rar_path, extract_path):\n",
        "        logger.error(f\"Failed to extract RAR file: {rar_path}. Cleaning up and skipping.\")\n",
        "        if os.path.exists(rar_path):\n",
        "            os.remove(rar_path)\n",
        "            logger.debug(f\"Removed failed RAR file: {rar_path}\")\n",
        "        return 0\n",
        "\n",
        "    # Clean up the downloaded RAR after successful extraction\n",
        "    if os.path.exists(rar_path):\n",
        "        os.remove(rar_path)\n",
        "        logger.debug(f\"Removed downloaded RAR file: {rar_path}\")\n",
        "\n",
        "\n",
        "    # 3. Find and Process PDF files within the extracted directory\n",
        "    pdf_files = [os.path.join(root, file) for root, _, files in os.walk(extract_path) for file in files if file.lower().endswith('.pdf')]\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files in extracted directory: {extract_path}\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.warning(f\"No PDF files found in {extract_path}. Cleaning up.\")\n",
        "        # Clean up the extracted directory\n",
        "        if os.path.exists(extract_path):\n",
        "            shutil.rmtree(extract_path)\n",
        "            logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "        return 0 # Indicate no PDFs processed\n",
        "\n",
        "    successful_uploads_count = 0\n",
        "    with tqdm(total=len(pdf_files), desc=f\"Processing PDFs in {sanitized_rar_filename}\", leave=False) as pbar_pdfs:\n",
        "        for pdf_path in pdf_files:\n",
        "            logger.info(f\"Processing PDF: {pdf_path}\")\n",
        "\n",
        "            # 4. Process PDF with Nougat\n",
        "            mmd_path = process_pdf(pdf_path, extract_path)\n",
        "\n",
        "            if mmd_path:\n",
        "                logger.info(f\"Nougat processing successful for {pdf_path}. MMD file: {mmd_path}\")\n",
        "                # 5. Clean and Chunk MMD file\n",
        "                cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, extract_path)\n",
        "\n",
        "                # 6. Upload to Hugging Face\n",
        "                if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "                    logger.info(f\"Uploading cleaned data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    if upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO):\n",
        "                        successful_uploads_count += 1\n",
        "                    else:\n",
        "                        logger.error(f\"Failed to upload cleaned data for {os.path.basename(pdf_path)}.\")\n",
        "                    # Optionally upload garbage data\n",
        "                    ##if garbage_jsonl and os.path.exists(garbage_jsonl):\n",
        "                    ##    logger.info(f\"Uploading garbage data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    ##    upload_to_huggingface(garbage_jsonl, HUGGING_FACE_REPO)\n",
        "                else:\n",
        "                    logger.warning(f\"No cleaned data generated for {os.path.basename(pdf_path)}. Skipping upload.\")\n",
        "            else:\n",
        "                logger.error(f\"Nougat processing failed for {pdf_path}. Skipping cleaning, chunking, and upload.\")\n",
        "\n",
        "            pbar_pdfs.update(1) # Update inner progress bar for each PDF\n",
        "\n",
        "    # 7. Clean up extracted directory after processing all PDFs in the RAR\n",
        "    logger.info(f\"Cleaning up extracted directory for {rar_filename}.\")\n",
        "    if os.path.exists(extract_path):\n",
        "        shutil.rmtree(extract_path)\n",
        "        logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "\n",
        "    return successful_uploads_count\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process the library.\"\"\"\n",
        "    logger.info(\"--- Starting Library Processing Pipeline ---\")\n",
        "\n",
        "    total_local_uploads = 0\n",
        "    logger.info(\"--- Processing Local PDF Files ---\")\n",
        "\n",
        "    local_pdf_files = []\n",
        "    for root, _, files in os.walk(\".\"): # Scan current directory and subdirectories\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                local_pdf_files.append(os.path.join(root, file))\n",
        "\n",
        "    logger.info(f\"Found {len(local_pdf_files)} local PDF files.\")\n",
        "\n",
        "    with tqdm(total=len(local_pdf_files), desc=\"Processing Local PDFs\") as pbar_local_pdfs:\n",
        "        for pdf_path in local_pdf_files:\n",
        "            total_local_uploads += process_single_pdf_local(pdf_path, HUGGING_FACE_REPO)\n",
        "            pbar_local_pdfs.update(1)\n",
        "\n",
        "    logger.info(f\"Finished processing local PDF files. Successfully uploaded cleaned data for {total_local_uploads} files.\")\n",
        "    logger.info(\"--- Finished Processing Local PDF Files ---\")\n",
        "\n",
        "\n",
        "    rar_files = []\n",
        "    if os.path.exists(RAR_LIST_CACHE):\n",
        "        logger.info(f\"Loading RAR file list from cache: {RAR_LIST_CACHE}\")\n",
        "        try:\n",
        "            with open(RAR_LIST_CACHE, 'r') as f:\n",
        "                rar_files = json.load(f)\n",
        "            logger.info(f\"Loaded {len(rar_files)} RAR files from cache.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading RAR file list from cache: {e}. Rescanning.\")\n",
        "            # If loading fails, proceed to rescan\n",
        "            rar_files = []\n",
        "\n",
        "    if not rar_files:\n",
        "        logger.info(f\"Scanning for RAR files at {BASE_URL}\")\n",
        "        try:\n",
        "            rar_files = get_file_list(BASE_URL)\n",
        "            logger.info(f\"Found {len(rar_files)} RAR files.\")\n",
        "            # Save the list to cache\n",
        "            try:\n",
        "                with open(RAR_LIST_CACHE, 'w') as f:\n",
        "                    json.dump(rar_files, f)\n",
        "                logger.info(f\"Saved RAR file list to cache: {RAR_LIST_CACHE}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving RAR file list to cache: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get RAR file list from {BASE_URL}: {e}\")\n",
        "            # Continue processing even if initial scan fails, as local processing is done\n",
        "            pass # Changed from return to pass\n",
        "\n",
        "    total_rar_uploads = 0\n",
        "\n",
        "    if rar_files:\n",
        "        logger.info(\"--- Processing Downloaded RAR Files ---\")\n",
        "        # Use ThreadPoolExecutor to process RAR files in parallel\n",
        "        # Adjust max_workers based on your runtime's capabilities and the task\n",
        "        max_workers = 1 # Example: process 1 RAR at a time to avoid resource issues\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit tasks to the executor\n",
        "            future_to_rar = {executor.submit(process_single_rar, rar_file_url, HUGGING_FACE_REPO): rar_file_url for rar_file_url in rar_files}\n",
        "\n",
        "            # Use tqdm to track overall progress\n",
        "            with tqdm(total=len(rar_files), desc=\"Overall RAR Processing\") as pbar_overall:\n",
        "                for future in as_completed(future_to_rar):\n",
        "                    rar_file_url = future_to_rar[future]\n",
        "                    try:\n",
        "                        successful_uploads_count = future.result()\n",
        "                        total_rar_uploads += successful_uploads_count\n",
        "                    except Exception as exc:\n",
        "                        logger.error(f'{rar_file_url} generated an exception: {exc}')\n",
        "\n",
        "                    pbar_overall.update(1) # Update outer progress bar for each completed RAR\n",
        "        logger.info(f\"Finished processing downloaded RAR files. Successfully uploaded cleaned data for {total_rar_uploads} PDF files.\")\n",
        "        logger.info(\"--- Finished Processing Downloaded RAR Files ---\")\n",
        "\n",
        "\n",
        "    logger.info(\"--- Library Processing Pipeline Finished ---\")\n",
        "    logger.info(f\"Total successfully uploaded cleaned data for {total_local_uploads + total_rar_uploads} PDF files to {HUGGING_FACE_REPO}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:04,973 - INFO - --- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:04,975 - INFO - --- Processing Local PDF Files ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Local PDF Files ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:04,980 - INFO - Found 329 local PDF files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Found 329 local PDF files.\n",
            "Processing Local PDFs:   0%|          | 0/329 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:04,983 - INFO - --- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:04,984 - INFO - ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd already exists. Skipping Nougat processing for ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd already exists. Skipping Nougat processing for ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:04,986 - INFO - Nougat processing successful for ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat processing successful for ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:04,988 - INFO - Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:05,181 - INFO - Chunking complete. Produced 202 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Chunking complete. Produced 202 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:05,183 - INFO - MMD content chunked into 202 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:MMD content chunked into 202 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:09,898 - INFO - Finished processing and chunking ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd. Generated 103 cleaned chunks and 99 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Finished processing and chunking ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].mmd. Generated 103 cleaned chunks and 99 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:09,902 - INFO - Uploading cleaned data for Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Uploading cleaned data for Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:09,906 - INFO - Attempting to upload ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid_Fuglestvedt_-_Rock_Art_and_the_Wild_Mind._Visual_Imagery_in_Mesolithic_Northern_Europe__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to upload ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid_Fuglestvedt_-_Rock_Art_and_the_Wild_Mind._Visual_Imagery_in_Mesolithic_Northern_Europe__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:10,358 - INFO - Successfully uploaded ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid_Fuglestvedt_-_Rock_Art_and_the_Wild_Mind._Visual_Imagery_in_Mesolithic_Northern_Europe__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully uploaded ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid_Fuglestvedt_-_Rock_Art_and_the_Wild_Mind._Visual_Imagery_in_Mesolithic_Northern_Europe__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n",
            "Processing Local PDFs:   0%|          | 1/329 [00:05<29:23,  5.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:10,361 - INFO - --- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:10,362 - INFO - ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd already exists. Skipping Nougat processing for ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd already exists. Skipping Nougat processing for ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:10,363 - INFO - Nougat processing successful for ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat processing successful for ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:10,365 - INFO - Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:10,811 - INFO - Chunking complete. Produced 298 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Chunking complete. Produced 298 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:10,813 - INFO - MMD content chunked into 298 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:MMD content chunked into 298 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:15,809 - INFO - Finished processing and chunking ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd. Generated 183 cleaned chunks and 115 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Finished processing and chunking ./1._20Prehistory/1. Prehistory/Mesolithic/Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].mmd. Generated 183 cleaned chunks and 115 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:15,811 - INFO - Uploading cleaned data for Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Uploading cleaned data for Peter Woodman - Ireland's First Settlers. Time and the Mesolithic [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:15,813 - INFO - Attempting to upload ./1._20Prehistory/1. Prehistory/Mesolithic/Peter_Woodman_-_Ireland_s_First_Settlers._Time_and_the_Mesolithic__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to upload ./1._20Prehistory/1. Prehistory/Mesolithic/Peter_Woodman_-_Ireland_s_First_Settlers._Time_and_the_Mesolithic__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:16,157 - INFO - Successfully uploaded ./1._20Prehistory/1. Prehistory/Mesolithic/Peter_Woodman_-_Ireland_s_First_Settlers._Time_and_the_Mesolithic__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully uploaded ./1._20Prehistory/1. Prehistory/Mesolithic/Peter_Woodman_-_Ireland_s_First_Settlers._Time_and_the_Mesolithic__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n",
            "Processing Local PDFs:   1%|          | 2/329 [00:11<30:39,  5.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:16,159 - INFO - --- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 18:29:16,162 - INFO - Attempting to process PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/Mesolithic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to process PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/Mesolithic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/114 [00:00<?, ?it/s]INFO:root:Processing file 1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf with 455 pages\n",
            "\n",
            "  1%|          | 1/114 [00:53<1:40:23, 53.31s/it]\n",
            "  2%|▏         | 2/114 [01:40<1:32:22, 49.48s/it]\n",
            "  3%|▎         | 3/114 [01:48<56:42, 30.66s/it]  \n",
            "  4%|▎         | 4/114 [01:55<39:12, 21.39s/it]\n",
            "  4%|▍         | 5/114 [02:03<29:59, 16.51s/it]\n",
            "  5%|▌         | 6/114 [02:50<48:10, 26.76s/it]\n",
            "  6%|▌         | 7/114 [03:01<38:38, 21.67s/it]\n",
            "  7%|▋         | 8/114 [03:12<32:35, 18.44s/it]\n",
            "  8%|▊         | 9/114 [03:23<28:11, 16.11s/it]\n",
            "  9%|▉         | 10/114 [03:42<29:28, 17.00s/it]\n",
            " 10%|▉         | 11/114 [04:29<44:40, 26.02s/it]\n",
            " 11%|█         | 12/114 [04:48<40:39, 23.92s/it]\n",
            " 11%|█▏        | 13/114 [05:00<34:12, 20.33s/it]\n",
            " 12%|█▏        | 14/114 [05:09<28:25, 17.06s/it]\n",
            " 13%|█▎        | 15/114 [05:56<42:47, 25.93s/it]\n",
            " 14%|█▍        | 16/114 [06:15<39:03, 23.92s/it]\n",
            " 15%|█▍        | 17/114 [06:34<36:26, 22.54s/it]\n",
            " 16%|█▌        | 18/114 [06:44<29:34, 18.48s/it]\n",
            " 17%|█▋        | 19/114 [07:30<42:44, 27.00s/it]\n",
            " 18%|█▊        | 20/114 [08:18<51:55, 33.14s/it]\n",
            " 18%|█▊        | 21/114 [08:28<40:35, 26.19s/it]\n",
            " 19%|█▉        | 22/114 [08:53<39:32, 25.78s/it]\n",
            " 20%|██        | 23/114 [09:14<37:13, 24.54s/it]\n",
            " 21%|██        | 24/114 [10:01<46:39, 31.10s/it]\n",
            " 22%|██▏       | 25/114 [10:11<36:42, 24.75s/it]\n",
            " 23%|██▎       | 26/114 [10:21<29:48, 20.32s/it]\n",
            " 24%|██▎       | 27/114 [11:08<41:13, 28.43s/it]\n",
            " 25%|██▍       | 28/114 [11:55<48:43, 34.00s/it]\n",
            " 25%|██▌       | 29/114 [12:04<37:43, 26.63s/it]\n",
            " 26%|██▋       | 30/114 [12:23<33:45, 24.12s/it]\n",
            " 27%|██▋       | 31/114 [12:29<25:54, 18.73s/it]\n",
            " 28%|██▊       | 32/114 [13:15<37:00, 27.08s/it]\n",
            " 29%|██▉       | 33/114 [13:26<29:49, 22.09s/it]\n",
            " 30%|██▉       | 34/114 [13:35<24:23, 18.29s/it]\n",
            " 31%|███       | 35/114 [14:22<35:15, 26.78s/it]\n",
            " 32%|███▏      | 36/114 [14:29<27:20, 21.03s/it]\n",
            " 32%|███▏      | 37/114 [14:39<22:36, 17.62s/it]\n",
            " 33%|███▎      | 38/114 [15:26<33:23, 26.36s/it]\n",
            " 34%|███▍      | 39/114 [15:35<26:20, 21.07s/it]\n",
            " 35%|███▌      | 40/114 [15:47<22:49, 18.50s/it]\n",
            " 36%|███▌      | 41/114 [16:00<20:18, 16.69s/it]\n",
            " 37%|███▋      | 42/114 [16:46<30:50, 25.70s/it]\n",
            " 38%|███▊      | 43/114 [16:58<25:24, 21.47s/it]\n",
            " 39%|███▊      | 44/114 [17:08<20:54, 17.92s/it]\n",
            " 39%|███▉      | 45/114 [17:23<19:40, 17.11s/it]\n",
            " 40%|████      | 46/114 [17:33<16:56, 14.94s/it]\n",
            " 41%|████      | 47/114 [17:42<14:52, 13.32s/it]\n",
            " 42%|████▏     | 48/114 [17:49<12:33, 11.41s/it]\n",
            " 43%|████▎     | 49/114 [17:59<11:52, 10.96s/it]\n",
            " 44%|████▍     | 50/114 [18:17<14:04, 13.20s/it]\n",
            " 45%|████▍     | 51/114 [18:31<13:52, 13.22s/it]\n",
            " 46%|████▌     | 52/114 [18:42<13:04, 12.65s/it]\n",
            " 46%|████▋     | 53/114 [18:52<11:54, 11.72s/it]\n",
            " 47%|████▋     | 54/114 [18:58<10:04, 10.08s/it]\n",
            " 48%|████▊     | 55/114 [19:17<12:42, 12.92s/it]\n",
            " 49%|████▉     | 56/114 [20:04<22:20, 23.10s/it]\n",
            " 50%|█████     | 57/114 [20:14<18:07, 19.08s/it]\n",
            " 51%|█████     | 58/114 [20:24<15:24, 16.50s/it]\n",
            " 52%|█████▏    | 59/114 [20:35<13:27, 14.67s/it]\n",
            " 53%|█████▎    | 60/114 [20:44<11:44, 13.05s/it]\n",
            " 54%|█████▎    | 61/114 [20:53<10:31, 11.92s/it]\n",
            " 54%|█████▍    | 62/114 [21:40<19:22, 22.35s/it]\n",
            " 55%|█████▌    | 63/114 [21:46<14:49, 17.45s/it]\n",
            " 56%|█████▌    | 64/114 [21:57<12:49, 15.40s/it]\n",
            " 57%|█████▋    | 65/114 [22:05<10:44, 13.16s/it]\n",
            " 58%|█████▊    | 66/114 [22:15<09:52, 12.34s/it]\n",
            " 59%|█████▉    | 67/114 [22:29<09:59, 12.75s/it]\n",
            " 60%|█████▉    | 68/114 [22:40<09:25, 12.30s/it]\n",
            " 61%|██████    | 69/114 [22:50<08:42, 11.60s/it]\n",
            " 61%|██████▏   | 70/114 [22:59<08:02, 10.96s/it]\n",
            " 62%|██████▏   | 71/114 [23:21<10:03, 14.05s/it]\n",
            " 63%|██████▎   | 72/114 [23:43<11:29, 16.42s/it]\n",
            " 64%|██████▍   | 73/114 [23:53<09:55, 14.53s/it]\n",
            " 65%|██████▍   | 74/114 [24:05<09:10, 13.76s/it]\n",
            " 66%|██████▌   | 75/114 [24:16<08:31, 13.11s/it]\n",
            " 67%|██████▋   | 76/114 [25:03<14:38, 23.11s/it]\n",
            " 68%|██████▊   | 77/114 [25:49<18:33, 30.11s/it]\n",
            " 68%|██████▊   | 78/114 [25:59<14:28, 24.14s/it]\n",
            " 69%|██████▉   | 79/114 [26:21<13:36, 23.34s/it]\n",
            " 70%|███████   | 80/114 [27:08<17:16, 30.48s/it]\n",
            " 71%|███████   | 81/114 [27:54<19:23, 35.25s/it]\n",
            " 72%|███████▏  | 82/114 [28:03<14:32, 27.25s/it]\n",
            " 73%|███████▎  | 83/114 [28:18<12:14, 23.69s/it]\n",
            " 74%|███████▎  | 84/114 [28:29<09:51, 19.72s/it]\n",
            " 75%|███████▍  | 85/114 [28:48<09:26, 19.53s/it]\n",
            " 75%|███████▌  | 86/114 [29:10<09:26, 20.22s/it]\n",
            " 76%|███████▋  | 87/114 [29:22<07:57, 17.68s/it]\n",
            " 77%|███████▋  | 88/114 [30:08<11:23, 26.30s/it]\n",
            " 78%|███████▊  | 89/114 [30:28<10:08, 24.36s/it]\n",
            " 79%|███████▉  | 90/114 [30:46<09:03, 22.63s/it]\n",
            " 80%|███████▉  | 91/114 [30:53<06:50, 17.86s/it]\n",
            " 81%|████████  | 92/114 [31:02<05:32, 15.11s/it]\n",
            " 82%|████████▏ | 93/114 [31:11<04:42, 13.46s/it]\n",
            " 82%|████████▏ | 94/114 [31:21<04:04, 12.22s/it]\n",
            " 83%|████████▎ | 95/114 [31:41<04:39, 14.69s/it]\n",
            " 84%|████████▍ | 96/114 [31:51<03:56, 13.17s/it]\n",
            " 85%|████████▌ | 97/114 [32:01<03:29, 12.30s/it]\n",
            " 86%|████████▌ | 98/114 [32:08<02:50, 10.65s/it]\n",
            " 87%|████████▋ | 99/114 [32:17<02:34, 10.33s/it]\n",
            " 88%|████████▊ | 100/114 [32:26<02:16,  9.77s/it]\n",
            " 89%|████████▊ | 101/114 [33:12<04:30, 20.79s/it]\n",
            " 89%|████████▉ | 102/114 [33:22<03:28, 17.42s/it]\n",
            " 90%|█████████ | 103/114 [34:09<04:48, 26.22s/it]\n",
            " 91%|█████████ | 104/114 [34:31<04:09, 24.95s/it]\n",
            " 92%|█████████▏| 105/114 [34:39<03:00, 20.01s/it]\n",
            " 93%|█████████▎| 106/114 [35:26<03:43, 27.98s/it]\n",
            " 94%|█████████▍| 107/114 [35:45<02:57, 25.41s/it]\n",
            " 95%|█████████▍| 108/114 [35:56<02:06, 21.02s/it]\n",
            " 96%|█████████▌| 109/114 [36:17<01:44, 20.90s/it]\n",
            " 96%|█████████▋| 110/114 [36:39<01:25, 21.38s/it]\n",
            " 97%|█████████▋| 111/114 [37:08<01:11, 23.71s/it]\n",
            " 98%|█████████▊| 112/114 [37:37<00:50, 25.14s/it]\n",
            " 99%|█████████▉| 113/114 [38:02<00:25, 25.12s/it]\n",
            "100%|██████████| 114/114 [38:24<00:00, 24.40s/it]\n",
            "100%|██████████| 114/114 [38:24<00:00, 20.22s/it]\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "2025-07-02 19:07:51,964 - INFO - Nougat command finished with exit code 0. Checking for output file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat command finished with exit code 0. Checking for output file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:51,967 - INFO - Successfully processed ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf with Nougat. MMD file created at ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully processed ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf with Nougat. MMD file created at ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:51,968 - INFO - Nougat processing successful for ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat processing successful for ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:51,970 - INFO - Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:52,243 - INFO - Chunking complete. Produced 220 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Chunking complete. Produced 220 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:52,250 - INFO - MMD content chunked into 220 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:MMD content chunked into 220 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:56,821 - INFO - Finished processing and chunking ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd. Generated 71 cleaned chunks and 149 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Finished processing and chunking ./1._20Prehistory/1. Prehistory/Mesolithic/Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].mmd. Generated 71 cleaned chunks and 149 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:56,822 - INFO - Uploading cleaned data for Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Uploading cleaned data for Almut Schülke - Coastal Landscapes of the Mesolithic. Human Engagement with the Coast from the Atlantic to the Baltic Sea [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:56,824 - INFO - Attempting to upload ./1._20Prehistory/1. Prehistory/Mesolithic/Almut_Sch_lke_-_Coastal_Landscapes_of_the_Mesolithic._Human_Engagement_with_the_Coast_from_the_Atlantic_to_the_Baltic_Sea__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to upload ./1._20Prehistory/1. Prehistory/Mesolithic/Almut_Sch_lke_-_Coastal_Landscapes_of_the_Mesolithic._Human_Engagement_with_the_Coast_from_the_Atlantic_to_the_Baltic_Sea__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:57,540 - INFO - Successfully uploaded ./1._20Prehistory/1. Prehistory/Mesolithic/Almut_Sch_lke_-_Coastal_Landscapes_of_the_Mesolithic._Human_Engagement_with_the_Coast_from_the_Atlantic_to_the_Baltic_Sea__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully uploaded ./1._20Prehistory/1. Prehistory/Mesolithic/Almut_Sch_lke_-_Coastal_Landscapes_of_the_Mesolithic._Human_Engagement_with_the_Coast_from_the_Atlantic_to_the_Baltic_Sea__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n",
            "Processing Local PDFs:   1%|          | 3/329 [38:52<96:15:53, 1063.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:57,542 - INFO - --- Processing local PDF: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing local PDF: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:07:57,544 - INFO - Attempting to process PDF: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/America\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to process PDF: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/America\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/77 [00:00<?, ?it/s]INFO:root:Processing file 1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf with 305 pages\n",
            "\n",
            "  1%|▏         | 1/77 [01:30<1:55:06, 90.88s/it]\n",
            "  3%|▎         | 2/77 [01:37<51:43, 41.38s/it]  \n",
            "  4%|▍         | 3/77 [01:43<30:52, 25.03s/it]\n",
            "  5%|▌         | 4/77 [01:51<22:22, 18.39s/it]\n",
            "  6%|▋         | 5/77 [02:01<18:18, 15.25s/it]\n",
            "  8%|▊         | 6/77 [02:10<15:51, 13.40s/it]\n",
            "  9%|▉         | 7/77 [02:57<28:21, 24.30s/it]\n",
            " 10%|█         | 8/77 [03:06<22:23, 19.46s/it]\n",
            " 12%|█▏        | 9/77 [03:15<18:19, 16.16s/it]\n",
            " 13%|█▎        | 10/77 [03:25<15:44, 14.09s/it]\n",
            " 14%|█▍        | 11/77 [03:34<13:50, 12.59s/it]\n",
            " 16%|█▌        | 12/77 [03:43<12:31, 11.56s/it]\n",
            " 17%|█▋        | 13/77 [03:52<11:32, 10.82s/it]\n",
            " 18%|█▊        | 14/77 [04:01<10:53, 10.38s/it]\n",
            " 19%|█▉        | 15/77 [04:13<10:58, 10.62s/it]\n",
            " 21%|██        | 16/77 [04:22<10:25, 10.25s/it]\n",
            " 22%|██▏       | 17/77 [04:31<09:49,  9.82s/it]\n",
            " 23%|██▎       | 18/77 [04:41<09:41,  9.85s/it]\n",
            " 25%|██▍       | 19/77 [04:51<09:34,  9.91s/it]\n",
            " 26%|██▌       | 20/77 [05:38<20:07, 21.18s/it]\n",
            " 27%|██▋       | 21/77 [05:48<16:30, 17.69s/it]\n",
            " 29%|██▊       | 22/77 [05:57<13:55, 15.20s/it]\n",
            " 30%|██▉       | 23/77 [06:06<12:02, 13.37s/it]\n",
            " 31%|███       | 24/77 [06:15<10:38, 12.04s/it]\n",
            " 32%|███▏      | 25/77 [06:24<09:42, 11.19s/it]\n",
            " 34%|███▍      | 26/77 [06:33<08:55, 10.50s/it]\n",
            " 35%|███▌      | 27/77 [06:43<08:25, 10.11s/it]\n",
            " 36%|███▋      | 28/77 [06:52<08:09,  9.98s/it]\n",
            " 38%|███▊      | 29/77 [07:02<07:53,  9.86s/it]\n",
            " 39%|███▉      | 30/77 [07:11<07:32,  9.63s/it]\n",
            " 40%|████      | 31/77 [07:20<07:21,  9.59s/it]\n",
            " 42%|████▏     | 32/77 [08:07<15:32, 20.72s/it]\n",
            " 43%|████▎     | 33/77 [08:54<21:01, 28.68s/it]\n",
            " 44%|████▍     | 34/77 [09:03<16:18, 22.75s/it]\n",
            " 45%|████▌     | 35/77 [09:14<13:27, 19.22s/it]\n",
            " 47%|████▋     | 36/77 [10:01<18:49, 27.56s/it]\n",
            " 48%|████▊     | 37/77 [10:10<14:41, 22.03s/it]\n",
            " 49%|████▉     | 38/77 [10:20<11:49, 18.20s/it]\n",
            " 51%|█████     | 39/77 [11:06<16:56, 26.76s/it]\n",
            " 52%|█████▏    | 40/77 [11:16<13:22, 21.68s/it]\n",
            " 53%|█████▎    | 41/77 [12:03<17:28, 29.14s/it]\n",
            " 55%|█████▍    | 42/77 [12:14<13:51, 23.76s/it]\n",
            " 56%|█████▌    | 43/77 [12:26<11:23, 20.10s/it]\n",
            " 57%|█████▋    | 44/77 [12:37<09:36, 17.48s/it]\n",
            " 58%|█████▊    | 45/77 [12:47<08:05, 15.17s/it]\n",
            " 60%|█████▉    | 46/77 [12:57<07:04, 13.68s/it]\n",
            " 61%|██████    | 47/77 [13:07<06:17, 12.58s/it]\n",
            " 62%|██████▏   | 48/77 [13:17<05:40, 11.73s/it]\n",
            " 64%|██████▎   | 49/77 [13:25<05:04, 10.87s/it]\n",
            " 65%|██████▍   | 50/77 [13:36<04:47, 10.63s/it]\n",
            " 66%|██████▌   | 51/77 [13:45<04:24, 10.17s/it]\n",
            " 68%|██████▊   | 52/77 [13:54<04:07,  9.92s/it]\n",
            " 69%|██████▉   | 53/77 [14:03<03:51,  9.63s/it]\n",
            " 70%|███████   | 54/77 [14:50<08:01, 20.95s/it]\n",
            " 71%|███████▏  | 55/77 [14:59<06:21, 17.33s/it]\n",
            " 73%|███████▎  | 56/77 [15:46<09:08, 26.11s/it]\n",
            " 74%|███████▍  | 57/77 [16:32<10:45, 32.27s/it]\n",
            " 75%|███████▌  | 58/77 [17:19<11:35, 36.61s/it]\n",
            " 77%|███████▋  | 59/77 [17:28<08:29, 28.29s/it]\n",
            " 78%|███████▊  | 60/77 [17:37<06:22, 22.51s/it]\n",
            " 79%|███████▉  | 61/77 [17:46<04:53, 18.34s/it]\n",
            " 81%|████████  | 62/77 [17:58<04:09, 16.66s/it]\n",
            " 82%|████████▏ | 63/77 [18:45<05:58, 25.60s/it]\n",
            " 83%|████████▎ | 64/77 [18:55<04:34, 21.08s/it]\n",
            " 84%|████████▍ | 65/77 [19:06<03:34, 17.89s/it]\n",
            " 86%|████████▌ | 66/77 [19:18<02:56, 16.06s/it]\n",
            " 87%|████████▋ | 67/77 [19:29<02:25, 14.59s/it]\n",
            " 88%|████████▊ | 68/77 [19:40<02:01, 13.51s/it]\n",
            " 90%|████████▉ | 69/77 [19:51<01:41, 12.70s/it]\n",
            " 91%|█████████ | 70/77 [20:01<01:23, 12.00s/it]\n",
            " 92%|█████████▏| 71/77 [20:14<01:14, 12.35s/it]\n",
            " 94%|█████████▎| 72/77 [20:40<01:21, 16.31s/it]\n",
            " 95%|█████████▍| 73/77 [21:26<01:41, 25.44s/it]\n",
            " 96%|█████████▌| 74/77 [21:48<01:12, 24.25s/it]\n",
            " 97%|█████████▋| 75/77 [22:10<00:47, 23.72s/it]\n",
            " 99%|█████████▊| 76/77 [22:58<00:30, 30.91s/it]\n",
            "100%|██████████| 77/77 [23:05<00:00, 23.85s/it]\n",
            "100%|██████████| 77/77 [23:05<00:00, 18.00s/it]\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "2025-07-02 19:31:14,173 - INFO - Nougat command finished with exit code 0. Checking for output file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat command finished with exit code 0. Checking for output file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:14,175 - INFO - Successfully processed ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf with Nougat. MMD file created at ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully processed ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf with Nougat. MMD file created at ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:14,176 - INFO - Nougat processing successful for ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat processing successful for ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:14,177 - INFO - Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:14,341 - INFO - Chunking complete. Produced 138 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Chunking complete. Produced 138 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:14,343 - INFO - MMD content chunked into 138 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:MMD content chunked into 138 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:18,906 - INFO - Finished processing and chunking ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd. Generated 23 cleaned chunks and 115 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Finished processing and chunking ./1._20Prehistory/1. Prehistory/America/Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].mmd. Generated 23 cleaned chunks and 115 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:18,909 - INFO - Uploading cleaned data for Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Uploading cleaned data for Alice Beck Kehoe - The Land of Prehistory. A Critical History of American Archaeology [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:18,913 - INFO - Attempting to upload ./1._20Prehistory/1. Prehistory/America/Alice_Beck_Kehoe_-_The_Land_of_Prehistory._A_Critical_History_of_American_Archaeology__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to upload ./1._20Prehistory/1. Prehistory/America/Alice_Beck_Kehoe_-_The_Land_of_Prehistory._A_Critical_History_of_American_Archaeology__Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:19,580 - INFO - Successfully uploaded ./1._20Prehistory/1. Prehistory/America/Alice_Beck_Kehoe_-_The_Land_of_Prehistory._A_Critical_History_of_American_Archaeology__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully uploaded ./1._20Prehistory/1. Prehistory/America/Alice_Beck_Kehoe_-_The_Land_of_Prehistory._A_Critical_History_of_American_Archaeology__Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n",
            "Processing Local PDFs:   1%|          | 4/329 [1:02:14<108:03:05, 1196.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:19,583 - INFO - --- Processing local PDF: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing local PDF: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:31:19,584 - INFO - Attempting to process PDF: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/America\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to process PDF: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/America\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/76 [00:00<?, ?it/s]INFO:root:Processing file 1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf with 301 pages\n",
            "\n",
            "  1%|▏         | 1/76 [00:04<05:21,  4.28s/it]\n",
            "  3%|▎         | 2/76 [00:09<06:18,  5.11s/it]\n",
            "  4%|▍         | 3/76 [00:57<29:33, 24.29s/it]\n",
            "  5%|▌         | 4/76 [01:43<39:45, 33.13s/it]\n",
            "  7%|▋         | 5/76 [01:54<29:28, 24.91s/it]\n",
            "  8%|▊         | 6/76 [02:03<22:49, 19.56s/it]\n",
            "  9%|▉         | 7/76 [02:12<18:26, 16.03s/it]\n",
            " 11%|█         | 8/76 [02:20<15:34, 13.74s/it]\n",
            " 12%|█▏        | 9/76 [02:29<13:36, 12.19s/it]\n",
            " 13%|█▎        | 10/76 [02:39<12:40, 11.52s/it]\n",
            " 14%|█▍        | 11/76 [02:49<12:03, 11.13s/it]\n",
            " 16%|█▌        | 12/76 [02:58<11:03, 10.37s/it]\n",
            " 17%|█▋        | 13/76 [03:08<10:38, 10.14s/it]\n",
            " 18%|█▊        | 14/76 [03:16<09:48,  9.50s/it]\n",
            " 20%|█▉        | 15/76 [03:25<09:45,  9.59s/it]\n",
            " 21%|██        | 16/76 [03:35<09:32,  9.54s/it]\n",
            " 22%|██▏       | 17/76 [03:41<08:22,  8.52s/it]\n",
            " 24%|██▎       | 18/76 [03:51<08:44,  9.05s/it]\n",
            " 25%|██▌       | 19/76 [04:00<08:25,  8.87s/it]\n",
            " 26%|██▋       | 20/76 [04:07<07:56,  8.51s/it]\n",
            " 28%|██▊       | 21/76 [04:18<08:20,  9.10s/it]\n",
            " 29%|██▉       | 22/76 [04:28<08:19,  9.25s/it]\n",
            " 30%|███       | 23/76 [04:36<08:01,  9.09s/it]\n",
            " 32%|███▏      | 24/76 [04:45<07:52,  9.08s/it]\n",
            " 33%|███▎      | 25/76 [04:54<07:30,  8.83s/it]\n",
            " 34%|███▍      | 26/76 [05:02<07:14,  8.69s/it]\n",
            " 36%|███▌      | 27/76 [05:11<07:10,  8.79s/it]\n",
            " 37%|███▋      | 28/76 [05:19<06:48,  8.51s/it]\n",
            " 38%|███▊      | 29/76 [05:26<06:21,  8.12s/it]\n",
            " 39%|███▉      | 30/76 [05:34<06:16,  8.18s/it]\n",
            " 41%|████      | 31/76 [05:43<06:08,  8.20s/it]\n",
            " 42%|████▏     | 32/76 [05:52<06:12,  8.45s/it]\n",
            " 43%|████▎     | 33/76 [06:02<06:24,  8.95s/it]\n",
            " 45%|████▍     | 34/76 [06:12<06:28,  9.26s/it]\n",
            " 46%|████▌     | 35/76 [06:21<06:21,  9.30s/it]\n",
            " 47%|████▋     | 36/76 [06:31<06:16,  9.42s/it]\n",
            " 49%|████▊     | 37/76 [06:40<06:05,  9.37s/it]\n",
            " 50%|█████     | 38/76 [06:48<05:40,  8.97s/it]\n",
            " 51%|█████▏    | 39/76 [07:35<12:30, 20.28s/it]\n",
            " 53%|█████▎    | 40/76 [07:42<09:44, 16.24s/it]\n",
            " 54%|█████▍    | 41/76 [07:48<07:50, 13.43s/it]\n",
            " 55%|█████▌    | 42/76 [07:57<06:43, 11.88s/it]\n",
            " 57%|█████▋    | 43/76 [08:43<12:15, 22.28s/it]\n",
            " 58%|█████▊    | 44/76 [09:03<11:33, 21.67s/it]\n",
            " 59%|█████▉    | 45/76 [09:14<09:23, 18.18s/it]\n",
            " 61%|██████    | 46/76 [09:23<07:46, 15.54s/it]\n",
            " 62%|██████▏   | 47/76 [10:10<12:07, 25.08s/it]\n",
            " 63%|██████▎   | 48/76 [10:18<09:15, 19.82s/it]\n",
            " 64%|██████▍   | 49/76 [10:23<06:53, 15.33s/it]\n",
            " 66%|██████▌   | 50/76 [10:30<05:39, 13.05s/it]\n",
            " 67%|██████▋   | 51/76 [10:38<04:43, 11.33s/it]\n",
            " 68%|██████▊   | 52/76 [10:46<04:06, 10.29s/it]\n",
            " 70%|██████▉   | 53/76 [10:53<03:34,  9.31s/it]\n",
            " 71%|███████   | 54/76 [11:02<03:25,  9.33s/it]\n",
            " 72%|███████▏  | 55/76 [11:14<03:32, 10.11s/it]\n",
            " 74%|███████▎  | 56/76 [12:01<07:04, 21.24s/it]\n",
            " 75%|███████▌  | 57/76 [12:15<05:59, 18.90s/it]\n",
            " 76%|███████▋  | 58/76 [12:27<05:07, 17.06s/it]\n",
            " 78%|███████▊  | 59/76 [12:37<04:10, 14.72s/it]\n",
            " 79%|███████▉  | 60/76 [12:47<03:32, 13.30s/it]\n",
            " 80%|████████  | 61/76 [12:56<03:02, 12.14s/it]\n",
            " 82%|████████▏ | 62/76 [13:05<02:37, 11.27s/it]\n",
            " 83%|████████▎ | 63/76 [13:52<04:44, 21.86s/it]\n",
            " 84%|████████▍ | 64/76 [14:39<05:54, 29.56s/it]\n",
            " 86%|████████▌ | 65/76 [14:52<04:28, 24.39s/it]\n",
            " 87%|████████▋ | 66/76 [15:39<05:12, 31.22s/it]\n",
            " 88%|████████▊ | 67/76 [16:25<05:22, 35.85s/it]\n",
            " 89%|████████▉ | 68/76 [16:38<03:49, 28.72s/it]\n",
            " 91%|█████████ | 69/76 [17:24<03:58, 34.10s/it]\n",
            " 92%|█████████▏| 70/76 [18:11<03:48, 38.04s/it]\n",
            " 93%|█████████▎| 71/76 [18:24<02:31, 30.36s/it]\n",
            " 95%|█████████▍| 72/76 [19:10<02:20, 35.15s/it]\n",
            " 96%|█████████▌| 73/76 [19:36<01:37, 32.45s/it]\n",
            " 97%|█████████▋| 74/76 [20:01<01:00, 30.21s/it]\n",
            " 99%|█████████▊| 75/76 [20:26<00:28, 28.64s/it]\n",
            "100%|██████████| 76/76 [20:46<00:00, 25.97s/it]\n",
            "100%|██████████| 76/76 [20:46<00:00, 16.40s/it]\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "2025-07-02 19:52:16,144 - INFO - Nougat command finished with exit code 0. Checking for output file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat command finished with exit code 0. Checking for output file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:16,145 - INFO - Successfully processed ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf with Nougat. MMD file created at ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully processed ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf with Nougat. MMD file created at ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:16,147 - INFO - Nougat processing successful for ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Nougat processing successful for ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf. MMD file: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:16,148 - INFO - Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Starting processing and chunking for MMD file: ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:16,272 - INFO - Chunking complete. Produced 128 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Chunking complete. Produced 128 initial segments based on headings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:16,275 - INFO - MMD content chunked into 128 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:MMD content chunked into 128 segments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:18,848 - INFO - Finished processing and chunking ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd. Generated 21 cleaned chunks and 107 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Finished processing and chunking ./1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].mmd. Generated 21 cleaned chunks and 107 garbage chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:18,852 - INFO - Uploading cleaned data for Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Uploading cleaned data for Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf to Hugging Face.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:18,853 - INFO - Attempting to upload ./1._20Prehistory/1. Prehistory/America/Michelle_Hayward__Lesley-Gail_Atkinson__Michael_A._Cinquino_-_Rock_Art_of_the_Caribbean__Caribbean_Archaeology_and_Ethnohistory___Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to upload ./1._20Prehistory/1. Prehistory/America/Michelle_Hayward__Lesley-Gail_Atkinson__Michael_A._Cinquino_-_Rock_Art_of_the_Caribbean__Caribbean_Archaeology_and_Ethnohistory___Retail__cleaned.jsonl to Hugging Face repo 'Disperser5601/Sentinel-Intelligence-Codex' (type: dataset).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:19,549 - INFO - Successfully uploaded ./1._20Prehistory/1. Prehistory/America/Michelle_Hayward__Lesley-Gail_Atkinson__Michael_A._Cinquino_-_Rock_Art_of_the_Caribbean__Caribbean_Archaeology_and_Ethnohistory___Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Successfully uploaded ./1._20Prehistory/1. Prehistory/America/Michelle_Hayward__Lesley-Gail_Atkinson__Michael_A._Cinquino_-_Rock_Art_of_the_Caribbean__Caribbean_Archaeology_and_Ethnohistory___Retail__cleaned.jsonl to Disperser5601/Sentinel-Intelligence-Codex\n",
            "Processing Local PDFs:   2%|▏         | 5/329 [1:23:14<109:45:59, 1219.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:19,551 - INFO - --- Processing local PDF: ./1._20Prehistory/1. Prehistory/America/Barbara A. Purdy - The Art and Archaeology of Florida's Wetlands (Retail).pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing local PDF: ./1._20Prehistory/1. Prehistory/America/Barbara A. Purdy - The Art and Archaeology of Florida's Wetlands (Retail).pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 19:52:19,553 - INFO - Attempting to process PDF: ./1._20Prehistory/1. Prehistory/America/Barbara A. Purdy - The Art and Archaeology of Florida's Wetlands (Retail).pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/America\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to process PDF: ./1._20Prehistory/1. Prehistory/America/Barbara A. Purdy - The Art and Archaeology of Florida's Wetlands (Retail).pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/America\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/83 [00:00<?, ?it/s]INFO:root:Processing file 1._20Prehistory/1. Prehistory/America/Barbara A. Purdy - The Art and Archaeology of Florida's Wetlands (Retail).pdf with 330 pages\n",
            "\n",
            "  1%|          | 1/83 [00:41<56:10, 41.10s/it]\n",
            "  2%|▏         | 2/83 [01:27<1:00:00, 44.45s/it]\n",
            "  4%|▎         | 3/83 [03:12<1:36:09, 72.12s/it]\n",
            "  5%|▍         | 4/83 [03:19<1:00:57, 46.29s/it]\n",
            "  6%|▌         | 5/83 [04:06<1:00:28, 46.51s/it]\n",
            "  7%|▋         | 6/83 [04:14<43:03, 33.56s/it]  \n",
            "  8%|▊         | 7/83 [04:24<32:21, 25.54s/it]\n",
            " 10%|▉         | 8/83 [04:32<25:09, 20.13s/it]\n",
            " 11%|█         | 9/83 [04:48<23:06, 18.73s/it]\n",
            " 12%|█▏        | 10/83 [04:57<19:17, 15.86s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0567ee4d"
      },
      "source": [
        "**Reasoning**:\n",
        "Search online for Nougat's Python API documentation or examples to determine if it can be used without subprocess.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afd094b67dc941daab8fb2bd5682bca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50ca2b02bc4e4f0b9211b28bc988d4ad",
              "IPY_MODEL_4e6318dd69644c9dba162722422d1e77",
              "IPY_MODEL_474f5e731cec4281a378e50192728899",
              "IPY_MODEL_9eac803af1ce4d07928d92175385b8fb",
              "IPY_MODEL_d794064f2e644a3a901bee71553ee31b"
            ],
            "layout": "IPY_MODEL_c7030757c52c4a71bacb480e237731b5"
          }
        },
        "50ca2b02bc4e4f0b9211b28bc988d4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50a7600a6e684ef18798abc58970885a",
            "placeholder": "​",
            "style": "IPY_MODEL_44af9caf73e5487eae974d36496f7c27",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "4e6318dd69644c9dba162722422d1e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2ff39d1d0a2648cdb972e84406a30b2d",
            "placeholder": "​",
            "style": "IPY_MODEL_b6b43dcb1bd54adeb57e63bada5665a2",
            "value": ""
          }
        },
        "474f5e731cec4281a378e50192728899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_990cb884626941bb9d1fcc1de8931627",
            "style": "IPY_MODEL_09d3bacfcd284f6d948e86ee575a1b75",
            "value": true
          }
        },
        "9eac803af1ce4d07928d92175385b8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b883848e739a4628a801d3d0648e5e5c",
            "style": "IPY_MODEL_a0ded3171476457aa8134d5dd4723baf",
            "tooltip": ""
          }
        },
        "d794064f2e644a3a901bee71553ee31b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1587a1e5a92f4c3aaa80d81ad23c38b9",
            "placeholder": "​",
            "style": "IPY_MODEL_31a43c198b3f4bd6acb63f9f26853b16",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "c7030757c52c4a71bacb480e237731b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "50a7600a6e684ef18798abc58970885a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44af9caf73e5487eae974d36496f7c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ff39d1d0a2648cdb972e84406a30b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6b43dcb1bd54adeb57e63bada5665a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "990cb884626941bb9d1fcc1de8931627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d3bacfcd284f6d948e86ee575a1b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b883848e739a4628a801d3d0648e5e5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ded3171476457aa8134d5dd4723baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1587a1e5a92f4c3aaa80d81ad23c38b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31a43c198b3f4bd6acb63f9f26853b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}