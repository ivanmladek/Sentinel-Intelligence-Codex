{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanmladek/Sentinel-Intelligence-Codex/blob/main/process_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-1"
      },
      "source": [
        "# Library Processing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYux-yCBECz3"
      },
      "source": [
        "The process of extracting, cleaning, and preparing the text from PDF files for the LLM is a multi-stage pipeline designed to ensure high-quality, structured data. This process is orchestrated by the process_refactor.ipynb notebook.\n",
        "\n",
        "1. Environment Setup and PDF Discovery\n",
        "\n",
        "Dependencies: The process begins by installing necessary Python libraries, including nougat-ocr for text extraction, nltk for natural language processing, and langdetect for language identification.\n",
        "PDF Discovery: The script recursively scans a specified directory (e.g., a Google Drive folder) to locate all PDF files.\n",
        "2. Text Extraction with Nougat\n",
        "\n",
        "Nougat OCR: For each PDF, the nougat command-line tool is used. Nougat is a state-of-the-art OCR tool specifically designed for academic and scientific documents, capable of recognizing and transcribing complex layouts, mathematical equations, and tables into a structured Markdown format (.mmd).\n",
        "Output: The raw extracted text is saved as a .mmd file, preserving the document's structure with Markdown headings.\n",
        "3. Text Cleaning and Garbage Detection\n",
        "\n",
        "This is a critical step to filter out irrelevant or low-quality text.\n",
        "\n",
        "Cleaning: A series of regular expressions and cleaning functions are applied to the raw text to:\n",
        "Remove extra newlines, spaces, and non-ASCII characters.\n",
        "Eliminate academic citations, references to tables/figures, and bracketed content.\n",
        "Sanitize garbled punctuation and symbols.\n",
        "Garbage Detection: Each segment of text is evaluated against a set of criteria to identify and discard \"garbage\" content. This includes:\n",
        "Language Detection: Text that is not identified as English is discarded.\n",
        "Heuristics: Checks for \"jammed\" words (long strings of characters without spaces), an unusually high proportion of single-letter words, and repetitive patterns.\n",
        "Quality Scoring: A text_quality_score is calculated based on the presence of common English words, proper part-of-speech patterns, and other linguistic features. Text falling below a certain threshold is flagged as garbage.\n",
        "4. Tokenization and Chunking\n",
        "\n",
        "Chunking Strategy: The cleaned .mmd content is chunked into smaller, manageable segments suitable for the LLM. The chunking logic is designed to respect the document's structure:\n",
        "The text is split by Markdown headings (#, ##, ###).\n",
        "These larger sections are then further divided into sentences using nltk.sent_tokenize.\n",
        "Size Constraints: The sentences are grouped into chunks with a maximum size (e.g., 8192 characters) to ensure they fit within the model's context window, while avoiding splitting sentences in the middle.\n",
        "Final Output: The cleaned, chunked text is saved to a .jsonl file, with each line containing a JSON object with a single \"text\" key, ready for training the LLM. Garbage text is saved to a separate file for review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-2"
      },
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cybg_6vE2qM",
        "outputId": "7826177f-003a-40da-8809-5a285cdf9cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "libmagic-dev is already the newest version (1:5.41-3ubuntu0.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Install System Dependencies\n",
        "!apt-get install -y poppler-utils tesseract-ocr libmagic-dev unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_yilG-WRis",
        "outputId": "db51aa6e-0a35-42c2-80af-df3314bccc5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Python Libraries (Part 1)\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sjFat5tLLfi",
        "outputId": "3d5a01e9-4e11-40a1-8ddf-736e077da6d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.11/dist-packages (4.38.2)\n",
            "Requirement already satisfied: pyarrow==14.0.1 in /usr/local/lib/python3.11/dist-packages (14.0.1)\n",
            "Requirement already satisfied: timm==0.5.4 in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.11/dist-packages (2.31.0)\n",
            "Requirement already satisfied: albumentations==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Collecting git+https://github.com/facebookresearch/nougat\n",
            "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-rajd8_yf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-rajd8_yf\n",
            "  Resolved https://github.com/facebookresearch/nougat to commit 5a92920d342fb6acf05fc9b594ccb4053dbe8e7a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Python Libraries (Part 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-3"
      },
      "source": [
        "## 2. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "config-cell",
        "outputId": "4f7219fa-ed79-4199-d5a5-b6ec52d48954"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import HfApi\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = \"https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\"\n",
        "HUGGING_FACE_REPO = \"Disperser5601/Sentinel-Intelligence-Codex\"  # Replace with your Hugging Face repo\n",
        "GARBAGE_THRESHOLD = 0.5\n",
        "LENWORD = 50\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.propagate = True # Ensure messages are propagated to the root logger\n",
        "\n",
        "# Explicitly set the logging level and add a handler to print to stdout\n",
        "logger.setLevel(logging.INFO)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "# Avoid adding duplicate handlers if the cell is run multiple times\n",
        "if not logger.handlers:\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# --- Download NLTK Data ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-4"
      },
      "source": [
        "## 3. Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-5"
      },
      "source": [
        "### 3.1. File and Web Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "file-ops-cell"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import logging\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_file_list(url, depth=0, max_depth=3):\n",
        "    \"\"\"Recursively get a list of files from a URL and its subdirectories up to a max depth, avoiding backlinks.\"\"\"\n",
        "    if depth > max_depth:\n",
        "        logger.debug(f\"Max depth ({max_depth}) reached at URL: {url}. Stopping recursion.\")\n",
        "        return []\n",
        "\n",
        "    rar_files = []\n",
        "    # Configure retries\n",
        "    retry_strategy = Retry(\n",
        "        total=3,  # Number of retries\n",
        "        backoff_factor=1, # Factor by which the delay increases\n",
        "        status_forcelist=[429, 500, 502, 503, 504] # HTTP status codes to retry on\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    http = requests.Session()\n",
        "    http.mount(\"http://\", adapter)\n",
        "    http.mount(\"https://\", adapter)\n",
        "\n",
        "    logger.info(f\"Accessing URL: {url} (Depth: {depth})\")\n",
        "    try:\n",
        "        response = http.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                # Handle relative and absolute links\n",
        "                full_url = requests.compat.urljoin(url, href)\n",
        "\n",
        "                # Ensure we only go deeper into subdirectories, avoiding backlinks\n",
        "                if full_url.startswith(url) and len(full_url) > len(url) and full_url.endswith('/'):\n",
        "                     logger.debug(f\"Found subdirectory: {full_url}. Recursing.\")\n",
        "                     rar_files.extend(get_file_list(full_url, depth + 1, max_depth))\n",
        "                elif full_url.endswith('.rar'):\n",
        "                    logger.debug(f\"Found RAR file: {full_url}\")\n",
        "                    rar_files.append(full_url)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error accessing URL {url}: {e}\")\n",
        "    logger.debug(f\"Finished processing URL: {url}. Found {len(rar_files)} RAR files in this branch.\")\n",
        "    return rar_files\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    \"\"\"Download a file from a URL.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        logger.info(f\"{output_path} already exists. Skipping download.\")\n",
        "        return True # Indicate success as file exists\n",
        "    logger.info(f\"Attempting to download {url} to {output_path}\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        logger.info(f\"Successfully downloaded {url} to {output_path}\")\n",
        "        return True # Indicate success\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error downloading file from {url}: {e}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "\n",
        "def extract_rar(file_path, output_path):\n",
        "    \"\"\"Extract a RAR file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"RAR file not found for extraction: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "        logger.debug(f\"Created output directory for extraction: {output_path}\")\n",
        "    logger.info(f\"Attempting to extract {file_path} to {output_path}\")\n",
        "    try:\n",
        "        # Added -o+ to overwrite without prompting\n",
        "        result = subprocess.run(['unrar', 'x', '-o+', file_path, output_path], check=True, capture_output=True, text=True)\n",
        "        logger.info(f\"Successfully extracted {file_path} to {output_path}\")\n",
        "        # Log stdout and stderr for debugging\n",
        "        if result.stdout:\n",
        "            logger.debug(f\"Unrar stdout for {file_path}:\\n{result.stdout}\")\n",
        "        if result.stderr:\n",
        "             logger.debug(f\"Unrar stderr for {file_path}:\\n{result.stderr}\")\n",
        "        return True # Indicate success\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error extracting {file_path}: {e.stderr}\")\n",
        "        return False # Indicate failure\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"Unrar command not found. Please ensure 'unrar' is installed.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during extraction of {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Sanitize a filename.\"\"\"\n",
        "    sanitized = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)\n",
        "    logger.debug(f\"Sanitized filename '{filename}' to '{sanitized}'\")\n",
        "    return sanitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-6"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### 3.2. PDF Processing (Nougat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nougat-cell"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import logging\n",
        "import sys # Import sys for streaming stdout\n",
        "from urllib.parse import quote # Import quote here\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def process_pdf(pdf_path, output_dir):\n",
        "    if not os.path.exists(pdf_path):\n",
        "        logger.error(f\"PDF file not found for processing: {pdf_path}\")\n",
        "        return None\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        logger.debug(f\"Created output directory for Nougat: {output_dir}\")\n",
        "\n",
        "    # Construct the expected output filename based on Nougat's default behavior\n",
        "    # Nougat typically replaces the PDF extension with .mmd in the output directory\n",
        "    pdf_filename = os.path.basename(pdf_path)\n",
        "    # MODIFICATION: Construct expected_mmd_filename using the original pdf_filename\n",
        "    expected_mmd_filename = f\"{os.path.splitext(pdf_filename)[0]}.mmd\"\n",
        "    mmd_path = os.path.join(output_dir, expected_mmd_filename)\n",
        "\n",
        "    # Construct the expected jsonl file path based on the pdf_path\n",
        "    base_name = os.path.basename(pdf_path).replace('.pdf', '')\n",
        "    jsonl_file_name = f\"{base_name}_cleaned.jsonl\"\n",
        "    huggingface_base_url = \"https://huggingface.co/datasets/Disperser5601/Sentinel-Intelligence-Codex/blob/main/\"\n",
        "    encoded_jsonl_file_name = quote(jsonl_file_name)\n",
        "    huggingface_jsonl_url = f\"{huggingface_base_url}{encoded_jsonl_file_name}\"\n",
        "\n",
        "    # Check if the jsonl file already exists in the Hugging Face dataset\n",
        "    logging.info(f\"Checking for existing jsonl file at: {huggingface_jsonl_url}\")\n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.head(huggingface_jsonl_url)\n",
        "        if response.status_code == 200:\n",
        "            logging.info(f\"Jsonl file already exists for {os.path.basename(pdf_path)}. Skipping processing.\")\n",
        "            return huggingface_jsonl_url # Return the URL if the file exists\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error checking Hugging Face dataset: {e}\")\n",
        "        # Continue with processing if there's an error checking the dataset\n",
        "\n",
        "\n",
        "    if os.path.exists(mmd_path):\n",
        "        logger.info(f\"{mmd_path} already exists. Skipping Nougat processing for {pdf_path}.\")\n",
        "        return mmd_path\n",
        "\n",
        "    logger.info(f\"Attempting to process PDF: {pdf_path} with Nougat. Output to {output_dir}\")\n",
        "\n",
        "    try:\n",
        "        # Stream output live using Popen\n",
        "        # Ensure the input path to nougat is the original path, not sanitized\n",
        "        process = subprocess.Popen(\n",
        "            ['nougat', pdf_path, '-o', output_dir, '--no-skipping', '--recompute'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT, # Capture stderr and merge with stdout\n",
        "            bufsize=1,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        # Stream output to stdout\n",
        "        for line in process.stdout:\n",
        "            sys.stdout.write(line)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        process.stdout.close()\n",
        "        return_code = process.wait()\n",
        "\n",
        "        if return_code != 0:\n",
        "            logger.error(f\"Nougat process failed with exit code {return_code}\")\n",
        "            return None\n",
        "\n",
        "        logger.info(f\"Nougat command finished with exit code {return_code}. Checking for output file.\")\n",
        "\n",
        "        # Explicitly check if the expected output file was created\n",
        "        if os.path.exists(mmd_path):\n",
        "            logger.info(f\"Successfully processed {pdf_path} with Nougat. MMD file created at {mmd_path}.\")\n",
        "            return mmd_path\n",
        "        else:\n",
        "            logger.error(f\"Nougat command finished but expected output {mmd_path} not found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during Nougat processing of {pdf_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-7"
      },
      "source": [
        "### 3.3. Text Cleaning and Quality Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cleaning-cell",
        "outputId": "e621fffe-1885-4e36-b41c-9f7fd658b6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,525 - INFO - NLTK 'punkt_tab' resource found.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:NLTK 'punkt_tab' resource found.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,631 - INFO - NLTK English words corpus loaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:NLTK English words corpus loaded.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import logging\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words # Assuming these are downloaded in config-cell\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize # Assuming this is downloaded in config-cell\n",
        "from textblob import TextBlob # Assuming TextBlob is installed\n",
        "import nltk # Import nltk for FreqDist\n",
        "\n",
        "# Download necessary NLTK data if not already present\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/words')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('words')\n",
        "# Ensure punkt_tab is downloaded right before potential use\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "# Verify punkt_tab is available after download attempt\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "    logger.info(\"NLTK 'punkt_tab' resource found.\")\n",
        "except LookupError:\n",
        "    logger.error(\"NLTK 'punkt_tab' resource still not found after download attempt. Sentence tokenization may fail.\")\n",
        "\n",
        "\n",
        "# Assuming GARBAGE_THRESHOLD and LENWORD are defined in config-cell\n",
        "# from .config import GARBAGE_THRESHOLD, LENWORD # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the NLTK words corpus for garbage detection\n",
        "# Ensure this is done after nltk.download('words') in config-cell\n",
        "try:\n",
        "    ENGLISH_WORDS = set(words.words())\n",
        "    logger.info(\"NLTK English words corpus loaded.\")\n",
        "except LookupError:\n",
        "    logger.error(\"NLTK 'words' corpus not found. Please run nltk.download('words').\")\n",
        "    ENGLISH_WORDS = set() # Use an empty set to avoid errors\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean the extracted text, including removing LaTeX artifacts.\"\"\"\n",
        "    logger.debug(f\"Cleaning text (first 100 chars): {text[:100]}...\")\n",
        "    initial_len = len(text)\n",
        "    # Remove markdown headings at the beginning of lines\n",
        "    text = re.sub(r'^\\s*#+\\s+.*', '', text, flags=re.MULTILINE)\n",
        "    # Remove markdown emphasis markers (*, _, **, __)\n",
        "    text = re.sub(r'(\\*\\*|__)(.*?)\\1', r'\\2', text) # Bold (**word**, __word__)\n",
        "    text = re.sub(r'(\\*|_)(.*?)\\1', r'\\2', text)   # Italic (*word*, _word_)\n",
        "    # Remove extra newlines, spaces, and non-ASCII characters.\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    # Add more general rules for removing content within parentheses and square brackets\n",
        "    # Remove content within square brackets (more general)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    # Remove content within parentheses (more general)\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "\n",
        "    # Remove specific academic citations, references to tables/figures, etc.\n",
        "    # (Keeping these as they might catch patterns not covered by the general rules,\n",
        "    # but the general rules should handle most cases)\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE) # This is now redundant with the general rule above\n",
        "\n",
        "    # MODIFICATION: Remove references to figures in the format \"Figure xx.yyy\" (case-insensitive)\n",
        "    text = re.sub(r'Figure\\s+\\d+\\.\\d+\\s*[\\.,;:]*', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Add extensive cleaning of LaTeX artifacts\n",
        "    # Remove LaTeX environments like \\begin{...} and \\end{...} and everything in between\n",
        "    text = re.sub(r'\\\\begin\\{.*?\\}\\s*.*?\\\\end\\{.*?\\}', '', text, flags=re.DOTALL)\n",
        "    # Remove common LaTeX commands (e.g., \\section, \\textbf, \\caption, \\label, \\cite, \\ref)\n",
        "    # This regex targets backslashes followed by letters, optionally with braces or brackets\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+\\s', ' ', text) # Remove commands followed by a space\n",
        "    # Remove mathematical environments like $...$, $$...$$, \\[...\\], \\(...\\)\n",
        "    text = re.sub(r'\\$.*?\\$', '', text)\n",
        "    text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
        "    text = re.sub(r'\\\\\\[.*?\\\\\\]', '', text, flags=re.DOTALL)\n",
        "    text = re.sub(r'\\\\\\(.*?\\\\\\)', '', text, flags=re.DOTALL)\n",
        "    # Remove other common symbols and commands\n",
        "    text = text.replace('\\\\\\\\', ' ') # Replace double backslash (newline in LaTeX) with space\n",
        "    text = text.replace('\\\\%', '%') # Unescape percent sign if needed\n",
        "    text = text.replace('\\\\&', '&') # Unescape ampersand if needed\n",
        "    text = text.replace('\\\\$', '$') # Unescape dollar sign if needed\n",
        "    text = text.replace('\\\\#', '#') # Unescape hash if needed\n",
        "    text = text.replace('\\\\_', '_') # Unescape underscore if needed\n",
        "    text = text.replace('\\\\{', '{') # Unescape brace if needed\n",
        "    text = text.replace('\\\\}', '}') # Unescape brace if needed\n",
        "    text = text.replace('\\\\textbullet', '') # Remove bullet point command\n",
        "    text = text.replace('\\\\par', ' ') # Remove paragraph command\n",
        "    text = text.replace('\\\\noindent', ' ') # Remove no indent command\n",
        "    text = text.replace('\\\\hfill', ' ') # Remove horizontal fill command\n",
        "    text = text.replace('\\\\vspace{.*?}', ' ') # Remove vertical space command\n",
        "    text = text.replace('\\\\hspace{.*?}', ' ') # Remove horizontal space command\n",
        "    text = text.replace('\\\\centering', ' ') # Remove centering command\n",
        "    text = text.replace('\\\\raggedright', ' ') # Remove ragged right command\n",
        "    text = text.replace('\\\\hline', ' ') # Remove table horizontal line command\n",
        "    text = text.replace('\\\\arraystretch{.*?}', ' ') # Remove array stretch command\n",
        "    text = text.replace('\\\\documentclass{.*?}', ' ') # Remove document class\n",
        "    text = text.replace('\\\\usepackage{.*?}', ' ') # Remove usepackage\n",
        "    text = text.replace('\\\\begin{document}', ' ') # Remove begin document\n",
        "    text = text.replace('\\\\end{document}', ' ') # Remove end document\n",
        "\n",
        "    # Remove page numbers and stray numerical artifacts (e.g., single numbers on a line or at the end of a line)\n",
        "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE) # Remove lines containing only numbers\n",
        "    text = re.sub(r'\\s+\\d+$', '', text) # Remove numbers at the end of a line\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    # Sanitize garbled punctuation and symbols.\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text)\n",
        "\n",
        "    # Clean up extra spaces potentially introduced by removing LaTeX\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    logger.debug(f\"Cleaned text (first 100 chars, original len {initial_len}): {text[:100]}...\")\n",
        "    return text\n",
        "\n",
        "def calculate_text_quality_score(text):\n",
        "    \"\"\"Calculate a quality score based on English words and sentence structure.\"\"\"\n",
        "    if not text:\n",
        "        return 0.0\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    if not words:\n",
        "        return 0.0\n",
        "\n",
        "    english_word_count = sum(1 for word in words if word.lower() in ENGLISH_WORDS)\n",
        "    english_word_ratio = english_word_count / len(words) if words else 0\n",
        "\n",
        "    # Simple heuristic for sentence structure (check for punctuation at end of sentences)\n",
        "    # Ensure punkt_tab is available before calling sent_tokenize\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "    except LookupError:\n",
        "        logger.error(\"NLTK 'punkt_tab' resource not found for sentence tokenization.\")\n",
        "        return 0.0 # Cannot calculate sentence structure score without the resource\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    well_formed_sentences = sum(1 for sent in sentences if sent.strip().endswith(('.', '!', '?')))\n",
        "    sentence_structure_score = well_formed_sentences / len(sentences) if sentences else 0\n",
        "\n",
        "    # Combine ratios - adjust weights as needed\n",
        "    quality_score = (english_word_ratio * 0.7) + (sentence_structure_score * 0.3)\n",
        "\n",
        "    logger.debug(f\"Text quality score calculated: {quality_score} for text (first 50 chars): {text[:50]}...\")\n",
        "    return quality_score\n",
        "\n",
        "# --- Helper functions for is_garbage ---\n",
        "\n",
        "def _check_single_letter_ratio(words_list, threshold=0.2):\n",
        "    \"\"\"Check for unusually high proportion of single-letter words.\"\"\"\n",
        "    if not words_list:\n",
        "        return False\n",
        "    single_letter_words = sum(1 for word in words_list if len(word) == 1)\n",
        "    if single_letter_words / len(words_list) > threshold:\n",
        "        logger.debug(\"Identified as garbage: high proportion of single-letter words.\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _check_repetitive_chars(text, min_repeat=5):\n",
        "    \"\"\"Check for simple repetitive character patterns.\"\"\"\n",
        "    if re.search(r'(.)\\1{' + str(min_repeat - 1) + r',}', text):\n",
        "        logger.debug(\"Identified as garbage: found repetitive character pattern.\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _check_high_frequency_ngrams(words, n_size, threshold):\n",
        "    \"\"\"Check for highly frequent n-grams (repetitive phrases).\"\"\"\n",
        "    if len(words) < n_size:\n",
        "        return False\n",
        "\n",
        "    ngrams = list(nltk.ngrams(words, n_size))\n",
        "    if not ngrams:\n",
        "        return False\n",
        "\n",
        "    fdist = nltk.FreqDist(ngrams)\n",
        "    most_common_ngram, count = fdist.most_common(1)[0]\n",
        "\n",
        "    if len(ngrams) > 0 and count / len(ngrams) > threshold:\n",
        "         logger.debug(f\"Identified as garbage: found highly frequent {n_size}-gram '{' '.join(most_common_ngram)}' (count: {count}/{len(ngrams)}).\")\n",
        "         return True\n",
        "    return False\n",
        "\n",
        "def _check_large_scale_repetition(text, min_match_len_ratio=0.1, min_match_len_abs=50, offsets_to_check_ratios=[1/2, 1/3, 1/4]):\n",
        "    \"\"\"Optimized check for significant large-scale repetition (comparing segments with offsets).\"\"\"\n",
        "    text_len = len(text)\n",
        "    # Only perform this check on reasonably long texts to avoid overhead on short chunks\n",
        "    if text_len < 200:\n",
        "        return False\n",
        "\n",
        "    min_match_len = max(min_match_len_abs, int(text_len * min_match_len_ratio)) # Require a minimum match length\n",
        "\n",
        "    for ratio in offsets_to_check_ratios:\n",
        "        offset = int(text_len * ratio)\n",
        "        if offset > 0 and text_len > offset + min_match_len:\n",
        "            segment1 = text[:text_len - offset]\n",
        "            segment2 = text[offset:text_len]\n",
        "            # Find the length of the longest common prefix between the two segments\n",
        "            match_len = 0\n",
        "            min_segment_len = min(len(segment1), len(segment2))\n",
        "            while match_len < min_segment_len and segment1[match_len] == segment2[match_len]:\n",
        "                match_len += 1\n",
        "\n",
        "            # If a significant match is found\n",
        "            if match_len >= min_match_len:\n",
        "                 logger.debug(f\"Identified as garbage: found significant repetition with offset ratio {ratio}, match length {match_len}.\")\n",
        "                 return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# --- Main is_garbage function ---\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, lenword=LENWORD):\n",
        "    \"\"\"Check if the text is garbage based on various heuristics.\"\"\"\n",
        "    logger.debug(f\"Checking if text is garbage (first 100 chars): {text[:100]}...\")\n",
        "\n",
        "    # Check for minimal length\n",
        "    if not text or len(text.split()) < 5: # Reduced minimum words\n",
        "        logger.debug(\"Identified as garbage: text too short or empty.\")\n",
        "        return True\n",
        "\n",
        "    # Language detection\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            logger.debug(\"Identified as garbage: language not English.\")\n",
        "            return True\n",
        "    except LangDetectException as e:\n",
        "        logger.debug(f\"Language detection failed for text (first 50 chars): {text[:50]}... Error: {e}. Assuming garbage.\")\n",
        "        return True # Assume garbage if language detection fails\n",
        "\n",
        "    # Check for jammed words (long strings without spaces)\n",
        "    words_list = text.split()\n",
        "    for word in words_list:\n",
        "        if len(word) > lenword and not '-' in word: # Allow hyphens in long words\n",
        "             logger.debug(f\"Identified as garbage: found jammed word '{word[:50]}...'\")\n",
        "             return True\n",
        "\n",
        "    # Use helper functions for modularity and generalization\n",
        "    if _check_single_letter_ratio(words_list):\n",
        "        return True\n",
        "\n",
        "    if _check_repetitive_chars(text):\n",
        "        return True\n",
        "\n",
        "    words_tokenized = word_tokenize(text)\n",
        "    if len(words_tokenized) > 5: # Only check for n-grams if there are enough words\n",
        "        # Check for highly frequent n-grams of different sizes\n",
        "        if _check_high_frequency_ngrams(words_tokenized, 3, 0.10): # Trigrams with 10% threshold\n",
        "            return True\n",
        "        if _check_high_frequency_ngrams(words_tokenized, 4, 0.07): # 4-grams with 7% threshold\n",
        "             return True\n",
        "        if _check_high_frequency_ngrams(words_tokenized, 5, 0.05): # 5-grams with 5% threshold\n",
        "             return True\n",
        "\n",
        "    # Check for significant large-scale repetition\n",
        "    if _check_large_scale_repetition(text):\n",
        "        return True\n",
        "\n",
        "\n",
        "    # Quality scoring\n",
        "    quality_score = calculate_text_quality_score(text)\n",
        "    if quality_score < threshold:\n",
        "        logger.debug(f\"Identified as garbage: quality score {quality_score} below threshold {threshold}.\")\n",
        "        return True\n",
        "\n",
        "    logger.debug(\"Text passed garbage checks.\")\n",
        "    return False\n",
        "\n",
        "# Ensure GARBAGE_THRESHOLD and LENWORD are available if not in this cell\n",
        "# GARBAGE_THRESHOLD = 0.8 # Example default\n",
        "# LENWORD = 50 # Example default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-8"
      },
      "source": [
        "### 3.4. Text Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "chunking-cell"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize # Assuming this is downloaded in config-cell\n",
        "\n",
        "# Assuming clean_text and is_garbage are defined in cleaning-cell and available\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def chunk_text(content, max_size=8192):\n",
        "    \"\"\"Chunk the text into smaller segments, respecting markdown headings.\"\"\"\n",
        "    logger.debug(f\"Starting chunking process with max_size={max_size}.\")\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    lines = content.split('\\n')\n",
        "    logger.debug(f\"Splitting content into {len(lines)} lines.\")\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        # Check for markdown headings\n",
        "        if line.strip().startswith((\"# \", \"## \", \"### \")):\n",
        "            logger.debug(f\"Found markdown heading at line {i}: {line.strip()}\")\n",
        "            # If the current segment is not empty, process it before starting a new one\n",
        "            if current_segment:\n",
        "                logger.debug(f\"Processing previous segment before heading (length: {len(current_segment)}).\")\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            # Start a new segment with the heading line\n",
        "            current_segment = line + \"\\n\" # Keep the heading line in the new segment\n",
        "            logger.debug(\"Starting new segment after heading.\")\n",
        "        else:\n",
        "            # Add non-heading lines to the current segment\n",
        "            current_segment += line + \"\\n\"\n",
        "\n",
        "    # Process any remaining content in the last segment\n",
        "    if current_segment:\n",
        "        logger.debug(f\"Processing final segment (length: {len(current_segment)}).\")\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    logger.info(f\"Chunking complete. Produced {len(segments)} initial segments based on headings.\")\n",
        "    return segments\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    \"\"\"Split a segment (potentially from a heading section) into smaller chunks by sentences.\"\"\"\n",
        "    logger.debug(f\"Splitting segment by sentences (length: {len(segment)}).\")\n",
        "    sentences = sent_tokenize(segment)\n",
        "    logger.debug(f\"Segment split into {len(sentences)} sentences.\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Add a space before adding the new sentence if the current chunk is not empty\n",
        "        sentence_to_add = sentence + \" \" if current_chunk else sentence\n",
        "        # Check if adding the current sentence exceeds the max size\n",
        "        if len(current_chunk) + len(sentence_to_add) <= max_size:\n",
        "            current_chunk += sentence_to_add\n",
        "            logger.debug(f\"Added sentence {i+1}/{len(sentences)} to current chunk (current size: {len(current_chunk)}).\")\n",
        "        else:\n",
        "            # If adding the sentence exceeds max size, add the current chunk to chunks list\n",
        "            if current_chunk: # Add the chunk only if it's not empty\n",
        "                chunks.append(current_chunk.strip())\n",
        "                logger.debug(f\"Chunk completed (size: {len(current_chunk)}). Starting new chunk with sentence {i+1}.\")\n",
        "            # Start a new chunk with the current sentence\n",
        "            current_chunk = sentence + \" \" # Start new chunk with the current sentence\n",
        "\n",
        "    # Add the last current chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "        logger.debug(f\"Added final chunk (size: {len(current_chunk)}).\")\n",
        "\n",
        "    logger.debug(f\"Segment split into {len(chunks)} smaller chunks.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def process_and_chunk_mmd(mmd_path, output_dir):\n",
        "    \"\"\"Process, clean, chunk, and categorize text from an MMD file.\"\"\"\n",
        "    logger.info(f\"Starting processing and chunking for MMD file: {mmd_path}\")\n",
        "\n",
        "    if not mmd_path or not os.path.exists(mmd_path):\n",
        "        logger.warning(f\"MMD file not found or path is invalid: {mmd_path}. Skipping processing and chunking.\")\n",
        "        return None, None\n",
        "\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(mmd_path))\n",
        "    cleaned_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_cleaned.jsonl\")\n",
        "    garbage_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_garbage.jsonl\")\n",
        "\n",
        "    #always reprocess with new cleaning and chunking algos\n",
        "    #if os.path.exists(cleaned_jsonl_path) and os.path.exists(garbage_jsonl_path):\n",
        "    #    logger.info(f\"Output files {cleaned_jsonl_path} and {garbage_jsonl_path} already exist. Skipping processing and chunking for {mmd_path}.\")\n",
        "    #    return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    try:\n",
        "        with open(mmd_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        logger.debug(f\"Successfully read content from {mmd_path} (length: {len(content)}).\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading MMD file {mmd_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    chunks = chunk_text(content)\n",
        "    logger.info(f\"MMD content chunked into {len(chunks)} segments.\")\n",
        "\n",
        "    cleaned_count = 0\n",
        "    garbage_count = 0\n",
        "\n",
        "    try:\n",
        "        with open(cleaned_jsonl_path, 'w', encoding='utf-8') as cleaned_f, \\\n",
        "             open(garbage_jsonl_path, 'w', encoding='utf-8') as garbage_f:\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Processing chunk {i+1}/{len(chunks)} (length: {len(chunk)}).\")\n",
        "                cleaned_chunk = clean_text(chunk)\n",
        "                if is_garbage(cleaned_chunk):\n",
        "                    garbage_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    garbage_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as garbage.\")\n",
        "                else:\n",
        "                    cleaned_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    cleaned_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as cleaned text.\")\n",
        "\n",
        "        logger.info(f\"Finished processing and chunking {mmd_path}. Generated {cleaned_count} cleaned chunks and {garbage_count} garbage chunks.\")\n",
        "        return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during cleaning or writing chunk files for {mmd_path}: {e}\")\n",
        "        # Clean up potentially incomplete files\n",
        "        if os.path.exists(cleaned_jsonl_path):\n",
        "            os.remove(cleaned_jsonl_path)\n",
        "        if os.path.exists(garbage_jsonl_path):\n",
        "            os.remove(garbage_jsonl_path)\n",
        "        return None, None\n",
        "\n",
        "# Ensure sanitize_filename, clean_text, is_garbage are available\n",
        "# from .file_ops import sanitize_filename # Example if in a different file\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-9"
      },
      "source": [
        "### 3.5. Hugging Face Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406,
          "referenced_widgets": [
            "0baf8531fda442689b3b4b26747ec339",
            "467c81d6f87949219c6f14c5621d6b93",
            "c3d359f614d545068786c9c2c014c376",
            "0310c16e41ef437b865be5711786aa98",
            "2808ea9c676b4a598c1cb64d63dbe3f1",
            "9d03c35e1db14919ad694ff96fc27ad8",
            "ab631201308248fa86efa3f176c45bf9",
            "fc1c7d562d77444d8d5a481f5e15f79f",
            "bda48c2be7554f0fbc46eb4207bb3571",
            "f7259e227e4e462486a08df44f6eeaa2",
            "14700db230ad40ca9b1ca5d5f6d58941",
            "5a8a36ca7b00437caa7940ea8598f987",
            "6e9597e76dcc4818adc1cf7a429b83d1",
            "5f34c6c44b9a42008e771dc3b87c9b52",
            "08133308ed164f5eb0eb90ce0b9b084e",
            "a7197e199dba4d608bad4d5265b6a857",
            "81ec24ef01d64de78619e143d65671c0"
          ]
        },
        "id": "huggingface-cell",
        "outputId": "9b42cfee-ca92-45ae-f707-3b5e0afbb5ad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0baf8531fda442689b3b4b26747ec339",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import logging\n",
        "import os\n",
        "from huggingface_hub import HfApi, Repository,login # Import Repository for better practice if needed for cloning/managing\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "login()\n",
        "\n",
        "def upload_to_huggingface(file_path, repo_id, repo_type=\"dataset\"):\n",
        "    \"\"\"Upload a file to a Hugging Face repository.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"File not found for upload to Hugging Face: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "    logger.info(f\"Attempting to upload {file_path} to Hugging Face repo '{repo_id}' (type: {repo_type}).\")\n",
        "    api = HfApi()\n",
        "\n",
        "    # Log in to the Hugging Face Hub\n",
        "    # This will prompt you to enter your token or use a token from your environment/secrets\n",
        "    try:\n",
        "        # Use create_commit for potentially better handling of multiple files or larger uploads\n",
        "        # This example uses upload_file for simplicity as in the original code\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=os.path.basename(file_path),\n",
        "            repo_id=repo_id,\n",
        "            repo_type=repo_type,\n",
        "            # Optional: add commit_message, token if not using environment variable\n",
        "        )\n",
        "        logger.info(f\"Successfully uploaded {file_path} to {repo_id}\")\n",
        "        return True # Indicate success\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error uploading {file_path} to Hugging Face repo '{repo_id}': {e}\")\n",
        "        return False # Indicate failure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-10"
      },
      "source": [
        "## 4. Main Processing Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8f2ecb"
      },
      "source": [
        "## Scan and process local pdfs\n",
        "\n",
        "### Subtask:\n",
        "Modify the main loop to first search for and process any existing PDF files within the anticipated output directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f5ce23e",
        "outputId": "483d6bcd-5160-44d6-a17a-85d25501a125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,782 - INFO - --- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:--- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,785 - INFO - --- Processing Local PDF Files ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:--- Processing Local PDF Files ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,795 - INFO - Found 0 local PDF files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Found 0 local PDF files.\n",
            "Processing Local PDFs: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,799 - INFO - Finished processing local PDF files. Successfully uploaded cleaned data for 0 files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "INFO:__main__:Finished processing local PDF files. Successfully uploaded cleaned data for 0 files.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,800 - INFO - --- Finished Processing Local PDF Files ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:--- Finished Processing Local PDF Files ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,801 - INFO - Scanning for RAR files at https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Scanning for RAR files at https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:19,804 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/ (Depth: 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/ (Depth: 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:20,265 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/0.%20Info/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/0.%20Info/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:20,602 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/1.%20Prehistory/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/1.%20Prehistory/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:20,962 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:21,390 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:21,973 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Kingdoms/ (Depth: 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Kingdoms/ (Depth: 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:22,591 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/4.%20Early%20Modern/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/4.%20Early%20Modern/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:23,045 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/5.%20Ancient%20%26%20Classical%20Civilizations%20Series/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/5.%20Ancient%20%26%20Classical%20Civilizations%20Series/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:23,693 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/6.%20Middle%20Ages%20Series/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/6.%20Middle%20Ages%20Series/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:24,772 - INFO - Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/7.%20Early%20Modern%20Series/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Accessing URL: https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/7.%20Early%20Modern%20Series/ (Depth: 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:25,349 - INFO - Found 497 RAR files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Found 497 RAR files.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:25,358 - INFO - Saved RAR file list to cache: rar_list_cache.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Saved RAR file list to cache: rar_list_cache.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:25,360 - INFO - --- Processing Downloaded RAR Files ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:--- Processing Downloaded RAR Files ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:25,365 - INFO - --- Processing 1.%20Prehistory.rar ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:--- Processing 1.%20Prehistory.rar ---\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-10 11:28:25,393 - INFO - Attempting to download https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/1.%20Prehistory/1.%20Prehistory.rar to 1._20Prehistory.rar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:Attempting to download https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/1.%20Prehistory/1.%20Prehistory.rar to 1._20Prehistory.rar\n",
            "Overall RAR Processing:   0%|          | 0/497 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # Import ThreadPoolExecutor\n",
        "from urllib.parse import quote # Import quote here\n",
        "\n",
        "# Assuming helper functions are defined in other cells and available\n",
        "# from .file_ops import get_file_list, download_file, extract_rar, sanitize_filename\n",
        "# from .nougat_processing import process_pdf\n",
        "# from .chunking import process_and_chunk_mmd\n",
        "# from .huggingface_integration import upload_to_huggingface\n",
        "# from .config import BASE_URL, HUGGING_FACE_REPO # Assuming these are defined in config-cell\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define a cache file path\n",
        "RAR_LIST_CACHE = \"rar_list_cache.json\"\n",
        "\n",
        "def process_single_pdf_local(pdf_path, HUGGING_FACE_REPO):\n",
        "    \"\"\"Processes a single local PDF file: processes with Nougat, cleans, chunks, and uploads.\"\"\"\n",
        "    logger.info(f\"--- Processing local PDF: {pdf_path} ---\")\n",
        "\n",
        "    output_dir = os.path.dirname(pdf_path) # Use the PDF's directory as the output directory\n",
        "\n",
        "    # 1. Process PDF with Nougat\n",
        "    mmd_path = process_pdf(pdf_path, output_dir)\n",
        "\n",
        "    if mmd_path:\n",
        "        logger.info(f\"Nougat processing successful for {pdf_path}. MMD file: {mmd_path}\")\n",
        "        # 2. Clean and Chunk MMD file\n",
        "        cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, output_dir)\n",
        "\n",
        "        # 3. Upload to Hugging Face\n",
        "        if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "            logger.info(f\"Uploading cleaned data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "            if upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO):\n",
        "                return 1 # Indicate success\n",
        "            else:\n",
        "                logger.error(f\"Failed to upload cleaned data for {os.path.basename(pdf_path)}.\")\n",
        "                return 0 # Indicate failure\n",
        "        else:\n",
        "            logger.warning(f\"No cleaned data generated for {os.path.basename(pdf_path)}. Skipping upload.\")\n",
        "            return 0 # Indicate no cleaned data\n",
        "\n",
        "    else:\n",
        "        logger.error(f\"Nougat processing failed for {pdf_path}. Skipping cleaning, chunking, and upload.\")\n",
        "        return 0 # Indicate failure\n",
        "\n",
        "\n",
        "def process_single_rar(rar_file_url, HUGGING_FACE_REPO):\n",
        "    \"\"\"Processes a single RAR file: downloads, extracts, processes PDFs, and uploads.\"\"\"\n",
        "    rar_filename = rar_file_url.split('/')[-1]\n",
        "    sanitized_rar_filename = sanitize_filename(rar_filename)\n",
        "    rar_path = sanitized_rar_filename\n",
        "    extract_path = os.path.splitext(rar_path)[0]\n",
        "\n",
        "    logger.info(f\"--- Processing {rar_filename} ---\")\n",
        "\n",
        "    # 1. Download RAR file\n",
        "    # The download_file function now correctly returns True on success\n",
        "    if not download_file(rar_file_url, rar_path):\n",
        "        logger.error(f\"Failed to download RAR file: {rar_file_url}. Skipping.\")\n",
        "        return 0\n",
        "\n",
        "    # 2. Extract RAR file\n",
        "    if not extract_rar(rar_path, extract_path):\n",
        "        logger.error(f\"Failed to extract RAR file: {rar_path}. Cleaning up and skipping.\")\n",
        "        if os.path.exists(rar_path):\n",
        "            os.remove(rar_path)\n",
        "            logger.debug(f\"Removed failed RAR file: {rar_path}\")\n",
        "        return 0\n",
        "\n",
        "    # Clean up the downloaded RAR after successful extraction\n",
        "    if os.path.exists(rar_path):\n",
        "        os.remove(rar_path)\n",
        "        logger.debug(f\"Removed downloaded RAR file: {rar_path}\")\n",
        "\n",
        "\n",
        "    # 3. Find and Process PDF files within the extracted directory\n",
        "    pdf_files = [os.path.join(root, file) for root, _, files in os.walk(extract_path) for file in files if file.lower().endswith('.pdf')]\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files in extracted directory: {extract_path}\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.warning(f\"No PDF files found in {extract_path}. Cleaning up.\")\n",
        "        # Clean up the extracted directory\n",
        "        if os.path.exists(extract_path):\n",
        "            shutil.rmtree(extract_path)\n",
        "            logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "        return 0 # Indicate no PDFs processed\n",
        "\n",
        "    successful_uploads_count = 0\n",
        "    with tqdm(total=len(pdf_files), desc=f\"Processing PDFs in {sanitized_rar_filename}\", leave=False) as pbar_pdfs:\n",
        "        for pdf_path in pdf_files:\n",
        "            logger.info(f\"Processing PDF: {pdf_path}\")\n",
        "\n",
        "            # 4. Process PDF with Nougat\n",
        "            mmd_path = process_pdf(pdf_path, extract_path)\n",
        "\n",
        "            if mmd_path:\n",
        "                logger.info(f\"Nougat processing successful for {pdf_path}. MMD file: {mmd_path}\")\n",
        "                # 5. Clean and Chunk MMD file\n",
        "                cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, extract_path)\n",
        "\n",
        "                # 6. Upload to Hugging Face\n",
        "                if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "                    logger.info(f\"Uploading cleaned data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    if upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO):\n",
        "                        successful_uploads_count += 1\n",
        "                    else:\n",
        "                        logger.error(f\"Failed to upload cleaned data for {os.path.basename(pdf_path)}.\")\n",
        "                    # Optionally upload garbage data\n",
        "                    ##if garbage_jsonl and os.path.exists(garbage_jsonl):\n",
        "                    ##    logger.info(f\"Uploading garbage data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    ##    upload_to_huggingface(garbage_jsonl, HUGGING_FACE_REPO)\n",
        "                else:\n",
        "                    logger.warning(f\"No cleaned data generated for {os.path.basename(pdf_path)}. Skipping upload.\")\n",
        "            else:\n",
        "                logger.error(f\"Nougat processing failed for {pdf_path}. Skipping cleaning, chunking, and upload.\")\n",
        "\n",
        "            pbar_pdfs.update(1) # Update inner progress bar for each PDF\n",
        "\n",
        "    # 7. Clean up extracted directory after processing all PDFs in the RAR\n",
        "    logger.info(f\"Cleaning up extracted directory for {rar_filename}.\")\n",
        "    if os.path.exists(extract_path):\n",
        "        shutil.rmtree(extract_path)\n",
        "        logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "\n",
        "    return successful_uploads_count\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process the library.\"\"\"\n",
        "    logger.info(\"--- Starting Library Processing Pipeline ---\")\n",
        "\n",
        "    total_local_uploads = 0\n",
        "    logger.info(\"--- Processing Local PDF Files ---\")\n",
        "\n",
        "    local_pdf_files = []\n",
        "    for root, _, files in os.walk(\".\"): # Scan current directory and subdirectories\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                local_pdf_files.append(os.path.join(root, file))\n",
        "\n",
        "    logger.info(f\"Found {len(local_pdf_files)} local PDF files.\")\n",
        "\n",
        "    with tqdm(total=len(local_pdf_files), desc=\"Processing Local PDFs\") as pbar_local_pdfs:\n",
        "        for pdf_path in local_pdf_files:\n",
        "            total_local_uploads += process_single_pdf_local(pdf_path, HUGGING_FACE_REPO)\n",
        "            pbar_local_pdfs.update(1)\n",
        "\n",
        "    logger.info(f\"Finished processing local PDF files. Successfully uploaded cleaned data for {total_local_uploads} files.\")\n",
        "    logger.info(\"--- Finished Processing Local PDF Files ---\")\n",
        "\n",
        "\n",
        "    rar_files = []\n",
        "    if os.path.exists(RAR_LIST_CACHE):\n",
        "        logger.info(f\"Loading RAR file list from cache: {RAR_LIST_CACHE}\")\n",
        "        try:\n",
        "            with open(RAR_LIST_CACHE, 'r') as f:\n",
        "                rar_files = json.load(f)\n",
        "            logger.info(f\"Loaded {len(rar_files)} RAR files from cache.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading RAR file list from cache: {e}. Rescanning.\")\n",
        "            # If loading fails, proceed to rescan\n",
        "            rar_files = []\n",
        "\n",
        "    if not rar_files:\n",
        "        logger.info(f\"Scanning for RAR files at {BASE_URL}\")\n",
        "        try:\n",
        "            rar_files = get_file_list(BASE_URL)\n",
        "            logger.info(f\"Found {len(rar_files)} RAR files.\")\n",
        "            # Save the list to cache\n",
        "            try:\n",
        "                with open(RAR_LIST_CACHE, 'w') as f:\n",
        "                    json.dump(rar_files, f)\n",
        "                logger.info(f\"Saved RAR file list to cache: {RAR_LIST_CACHE}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving RAR file list to cache: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get RAR file list from {BASE_URL}: {e}\")\n",
        "            # Continue processing even if initial scan fails, as local processing is done\n",
        "            pass # Changed from return to pass\n",
        "\n",
        "    total_rar_uploads = 0\n",
        "\n",
        "    if rar_files:\n",
        "        logger.info(\"--- Processing Downloaded RAR Files ---\")\n",
        "        # Use ThreadPoolExecutor to process RAR files in parallel\n",
        "        # Adjust max_workers based on your runtime's capabilities and the task\n",
        "        max_workers = 1 # Example: process 1 RAR at a time to avoid resource issues\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit tasks to the executor\n",
        "            future_to_rar = {executor.submit(process_single_rar, rar_file_url, HUGGING_FACE_REPO): rar_file_url for rar_file_url in rar_files}\n",
        "\n",
        "            # Use tqdm to track overall progress\n",
        "            with tqdm(total=len(rar_files), desc=\"Overall RAR Processing\") as pbar_overall:\n",
        "                for future in as_completed(future_to_rar):\n",
        "                    rar_file_url = future_to_rar[future]\n",
        "                    try:\n",
        "                        successful_uploads_count = future.result()\n",
        "                        total_rar_uploads += successful_uploads_count\n",
        "                    except Exception as exc:\n",
        "                        logger.error(f'{rar_file_url} generated an exception: {exc}')\n",
        "\n",
        "                    pbar_overall.update(1) # Update outer progress bar for each completed RAR\n",
        "        logger.info(f\"Finished processing downloaded RAR files. Successfully uploaded cleaned data for {total_rar_uploads} PDF files.\")\n",
        "        logger.info(\"--- Finished Processing Downloaded RAR Files ---\")\n",
        "\n",
        "\n",
        "    logger.info(\"--- Library Processing Pipeline Finished ---\")\n",
        "    logger.info(f\"Total successfully uploaded cleaned data for {total_local_uploads + total_rar_uploads} PDF files to {HUGGING_FACE_REPO}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0567ee4d"
      },
      "source": [
        "**Reasoning**:\n",
        "Search online for Nougat's Python API documentation or examples to determine if it can be used without subprocess.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
