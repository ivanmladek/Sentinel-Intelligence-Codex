{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanmladek/Sentinel-Intelligence-Codex/blob/main/process_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Processing Pipeline"
      ],
      "metadata": {
        "id": "markdown-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Dependencies"
      ],
      "metadata": {
        "id": "markdown-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cybg_6vE2qM"
      },
      "outputs": [],
      "source": [
        "#@title Install System Dependencies\n",
        "!apt-get install -y poppler-utils tesseract-ocr libmagic-dev unrar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Python Libraries (Part 1)\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "MO_yilG-WRis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Python Libraries (Part 2)\n",
        "!pip install transformers==4.38.2 pyarrow==14.0.1 timm==0.5.4 requests==2.31.0 albumentations==1.0.0 git+https://github.com/facebookresearch/nougat\n",
        "!pip install textblob langdetect beautifulsoup4 huggingface_hub tqdm pandas"
      ],
      "metadata": {
        "id": "1sjFat5tLLfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Imports and Configuration"
      ],
      "metadata": {
        "id": "markdown-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import shutil\n",
        "import subprocess\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import HfApi\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = \"https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\"\n",
        "HUGGING_FACE_REPO = \"ivanmladek/Sentinel-Intelligence-Codex\"  # Replace with your Hugging Face repo\n",
        "GARBAGE_THRESHOLD = 0.8\n",
        "LENWORD = 50\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Download NLTK Data ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "config-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Helper Functions"
      ],
      "metadata": {
        "id": "markdown-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. File and Web Operations"
      ],
      "metadata": {
        "id": "markdown-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file_list(url):\n",
        "    \"\"\"Get a list of files from a URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return [link.get('href') for link in soup.find_all('a') if link.get('href').endswith('.rar')]\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    \"\"\"Download a file from a URL.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        logger.info(f\"{output_path} already exists. Skipping download.\")\n",
        "        return\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(output_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "def extract_rar(file_path, output_path):\n",
        "    \"\"\"Extract a RAR file.\"\"\"\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "    try:\n",
        "        subprocess.run(['unrar', 'x', file_path, output_path], check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error extracting {file_path}: {e.stderr}\")\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Sanitize a filename.\"\"\"\n",
        "    return re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)"
      ],
      "metadata": {
        "id": "file-ops-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. PDF Processing (Nougat)"
      ],
      "metadata": {
        "id": "markdown-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf(pdf_path, output_dir):\n",
        "    \"\"\"Process a single PDF file with Nougat.\"\"\"\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(pdf_path))\n",
        "    mmd_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}.mmd\")\n",
        "    if os.path.exists(mmd_path):\n",
        "        logger.info(f\"{mmd_path} already exists. Skipping Nougat processing.\")\n",
        "        return mmd_path\n",
        "\n",
        "    try:\n",
        "        subprocess.run(['nougat', pdf_path, '-o', output_dir, '--no-skipping', '--recompute'], check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error processing {pdf_path} with Nougat: {e.stderr}\")\n",
        "        return None\n",
        "    return mmd_path"
      ],
      "metadata": {
        "id": "nougat-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Text Cleaning and Quality Control"
      ],
      "metadata": {
        "id": "markdown-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean the extracted text.\"\"\"\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, lenword=LENWORD):\n",
        "    \"\"\"Check if the text is garbage.\"\"\"\n",
        "    if not text or len(text.split()) < 10:\n",
        "        return True\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            return True\n",
        "    except LangDetectException:\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "cleaning-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Text Chunking"
      ],
      "metadata": {
        "id": "markdown-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(content, max_size=8192):\n",
        "    \"\"\"Chunk the text into smaller segments.\"\"\"\n",
        "    segments = []\n",
        "    current_segment = \"\"\"\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith((\"#\", \"##\", \"###\")):\n",
        "            if current_segment:\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            current_segment = \"\"\"\n",
        "        else:\n",
        "            current_segment += line + \" \"\n",
        "\n",
        "    if current_segment:\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    return segments\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    \"\"\"Split a segment into smaller chunks.\"\"\"\n",
        "    sentences = sent_tokenize(segment)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 1 <= max_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_and_chunk_mmd(mmd_path, output_dir):\n",
        "    \"\"\"Process and chunk an MMD file.\"\"\"\n",
        "    if not mmd_path or not os.path.exists(mmd_path):\n",
        "        logger.warning(f\"MMD file not found: {mmd_path}. Skipping.\")\n",
        "        return None, None\n",
        "\n",
        "    cleaned_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(mmd_path))[0]}_cleaned.jsonl\")\n",
        "    garbage_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(mmd_path))[0]}_garbage.jsonl\")\n",
        "\n",
        "    with open(mmd_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    chunks = chunk_text(content)\n",
        "    with open(cleaned_jsonl_path, 'w') as cleaned_f, open(garbage_jsonl_path, 'w') as garbage_f:\n",
        "        for chunk in chunks:\n",
        "            cleaned_chunk = clean_text(chunk)\n",
        "            if is_garbage(cleaned_chunk):\n",
        "                garbage_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "            else:\n",
        "                cleaned_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "\n",
        "    return cleaned_jsonl_path, garbage_jsonl_path"
      ],
      "metadata": {
        "id": "chunking-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Hugging Face Integration"
      ],
      "metadata": {
        "id": "markdown-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_to_huggingface(file_path, repo_id):\n",
        "    \"\"\"Upload a file to a Hugging Face repository.\"\"\"\n",
        "    api = HfApi()\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=file_path,\n",
        "        path_in_repo=os.path.basename(file_path),\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"dataset\",\n",
        "    )\n",
        "    logger.info(f\"Uploaded {file_path} to {repo_id}\")"
      ],
      "metadata": {
        "id": "huggingface-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Main Processing Loop"
      ],
      "metadata": {
        "id": "markdown-10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to process the library.\"\"\"\n",
        "    rar_files = get_file_list(BASE_URL)\n",
        "\n",
        "    with tqdm(total=len(rar_files), desc=\"Processing RAR Files\") as pbar:\n",
        "        for rar_file in rar_files:\n",
        "            rar_url = BASE_URL + rar_file\n",
        "            rar_path = sanitize_filename(rar_file)\n",
        "            extract_path = os.path.splitext(rar_path)[0]\n",
        "\n",
        "            download_file(rar_url, rar_path)\n",
        "            extract_rar(rar_path, extract_path)\n",
        "\n",
        "            pdf_files = [os.path.join(root, file) for root, _, files in os.walk(extract_path) for file in files if file.endswith('.pdf')]\n",
        "\n",
        "            for pdf_path in pdf_files:\n",
        "                mmd_path = process_pdf(pdf_path, extract_path)\n",
        "                cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, extract_path)\n",
        "\n",
        "                if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "                    upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO)\n",
        "                if garbage_jsonl and os.path.exists(garbage_jsonl):\n",
        "                    upload_to_huggingface(garbage_jsonl, HUGGING_FACE_REPO)\n",
        "\n",
        "            os.remove(rar_path)\n",
        "            shutil.rmtree(extract_path)\n",
        "            pbar.update(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "main-loop-cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}