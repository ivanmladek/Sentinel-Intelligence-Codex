{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanmladek/Sentinel-Intelligence-Codex/blob/main/process_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-1"
      },
      "source": [
        "# Library Processing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYux-yCBECz3"
      },
      "source": [
        "The process of extracting, cleaning, and preparing the text from PDF files for the LLM is a multi-stage pipeline designed to ensure high-quality, structured data. This process is orchestrated by the process_refactor.ipynb notebook.\n",
        "\n",
        "1. Environment Setup and PDF Discovery\n",
        "\n",
        "Dependencies: The process begins by installing necessary Python libraries, including nougat-ocr for text extraction, nltk for natural language processing, and langdetect for language identification.\n",
        "PDF Discovery: The script recursively scans a specified directory (e.g., a Google Drive folder) to locate all PDF files.\n",
        "2. Text Extraction with Nougat\n",
        "\n",
        "Nougat OCR: For each PDF, the nougat command-line tool is used. Nougat is a state-of-the-art OCR tool specifically designed for academic and scientific documents, capable of recognizing and transcribing complex layouts, mathematical equations, and tables into a structured Markdown format (.mmd).\n",
        "Output: The raw extracted text is saved as a .mmd file, preserving the document's structure with Markdown headings.\n",
        "3. Text Cleaning and Garbage Detection\n",
        "\n",
        "This is a critical step to filter out irrelevant or low-quality text.\n",
        "\n",
        "Cleaning: A series of regular expressions and cleaning functions are applied to the raw text to:\n",
        "Remove extra newlines, spaces, and non-ASCII characters.\n",
        "Eliminate academic citations, references to tables/figures, and bracketed content.\n",
        "Sanitize garbled punctuation and symbols.\n",
        "Garbage Detection: Each segment of text is evaluated against a set of criteria to identify and discard \"garbage\" content. This includes:\n",
        "Language Detection: Text that is not identified as English is discarded.\n",
        "Heuristics: Checks for \"jammed\" words (long strings of characters without spaces), an unusually high proportion of single-letter words, and repetitive patterns.\n",
        "Quality Scoring: A text_quality_score is calculated based on the presence of common English words, proper part-of-speech patterns, and other linguistic features. Text falling below a certain threshold is flagged as garbage.\n",
        "4. Tokenization and Chunking\n",
        "\n",
        "Chunking Strategy: The cleaned .mmd content is chunked into smaller, manageable segments suitable for the LLM. The chunking logic is designed to respect the document's structure:\n",
        "The text is split by Markdown headings (#, ##, ###).\n",
        "These larger sections are then further divided into sentences using nltk.sent_tokenize.\n",
        "Size Constraints: The sentences are grouped into chunks with a maximum size (e.g., 8192 characters) to ensure they fit within the model's context window, while avoiding splitting sentences in the middle.\n",
        "Final Output: The cleaned, chunked text is saved to a .jsonl file, with each line containing a JSON object with a single \"text\" key, ready for training the LLM. Garbage text is saved to a separate file for review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-2"
      },
      "source": [
        "## 1. Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cybg_6vE2qM",
        "outputId": "03b01f8f-7198-4172-bf28-50a99d7d51f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "libmagic-dev is already the newest version (1:5.41-3ubuntu0.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Install System Dependencies\n",
        "!apt-get install -y poppler-utils tesseract-ocr libmagic-dev unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MO_yilG-WRis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae64ad85-24ce-488b-f627-76fb078357fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Python Libraries (Part 1)\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1sjFat5tLLfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d86ed47-9458-40b8-c08c-72c46cb6096a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/nougat\n",
            "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-t2tst78a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-t2tst78a\n",
            "  Resolved https://github.com/facebookresearch/nougat to commit 5a92920d342fb6acf05fc9b594ccb4053dbe8e7a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.11/dist-packages (4.38.2)\n",
            "Requirement already satisfied: pyarrow==14.0.1 in /usr/local/lib/python3.11/dist-packages (14.0.1)\n",
            "Requirement already satisfied: timm==0.5.4 in /usr/local/lib/python3.11/dist-packages (0.5.4)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.11/dist-packages (2.31.0)\n",
            "Requirement already satisfied: albumentations==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (0.21.0+cu124)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2025.6.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (1.15.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (0.25.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (4.11.0.86)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.10.18)\n",
            "Requirement already satisfied: datasets[vision] in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (2.14.4)\n",
            "Requirement already satisfied: lightning<2022,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (2.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.9.1)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.13.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (0.2.0)\n",
            "Requirement already satisfied: sconf>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (0.2.5)\n",
            "Requirement already satisfied: pypdf>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (5.7.0)\n",
            "Requirement already satisfied: pypdfium2 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (4.30.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.1.5)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (0.14.3)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (1.7.3)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (2.5.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (0.4)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.11/dist-packages (from sconf>=0.2.3->nougat-ocr==0.1.18) (0.18.14)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.11/dist-packages (from sconf>=0.2.3->nougat-ocr==0.1.18) (4.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.5.4) (1.3.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (1.5.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.20.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.5.4) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.18) (0.2.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.18) (1.17.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "#@title Install Python Libraries (Part 2)\n",
        "!pip install transformers==4.38.2 pyarrow==14.0.1 timm==0.5.4 requests==2.31.0 albumentations==1.0.0 git+https://github.com/facebookresearch/nougat\n",
        "!pip install textblob langdetect beautifulsoup4 huggingface_hub tqdm pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-3"
      },
      "source": [
        "## 2. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f9fd22-1314-4741-a91e-2c96b79b712e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import HfApi\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = \"https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\"\n",
        "HUGGING_FACE_REPO = \"ivanmladek/Sentinel-Intelligence-Codex\"  # Replace with your Hugging Face repo\n",
        "GARBAGE_THRESHOLD = 0.8\n",
        "LENWORD = 50\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.propagate = True # Ensure messages are propagated to the root logger\n",
        "\n",
        "# Explicitly set the logging level and add a handler to print to stdout\n",
        "logger.setLevel(logging.DEBUG)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "# Avoid adding duplicate handlers if the cell is run multiple times\n",
        "if not logger.handlers:\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# --- Download NLTK Data ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-4"
      },
      "source": [
        "## 3. Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-5"
      },
      "source": [
        "### 3.1. File and Web Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "file-ops-cell"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import logging\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_file_list(url, depth=0, max_depth=3):\n",
        "    \"\"\"Recursively get a list of files from a URL and its subdirectories up to a max depth, avoiding backlinks.\"\"\"\n",
        "    if depth > max_depth:\n",
        "        logger.debug(f\"Max depth ({max_depth}) reached at URL: {url}. Stopping recursion.\")\n",
        "        return []\n",
        "\n",
        "    rar_files = []\n",
        "    # Configure retries\n",
        "    retry_strategy = Retry(\n",
        "        total=3,  # Number of retries\n",
        "        backoff_factor=1, # Factor by which the delay increases\n",
        "        status_forcelist=[429, 500, 502, 503, 504] # HTTP status codes to retry on\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    http = requests.Session()\n",
        "    http.mount(\"http://\", adapter)\n",
        "    http.mount(\"https://\", adapter)\n",
        "\n",
        "    logger.info(f\"Accessing URL: {url} (Depth: {depth})\")\n",
        "    try:\n",
        "        response = http.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                # Handle relative and absolute links\n",
        "                full_url = requests.compat.urljoin(url, href)\n",
        "\n",
        "                # Ensure we only go deeper into subdirectories, avoiding backlinks\n",
        "                if full_url.startswith(url) and len(full_url) > len(url) and full_url.endswith('/'):\n",
        "                     logger.debug(f\"Found subdirectory: {full_url}. Recursing.\")\n",
        "                     rar_files.extend(get_file_list(full_url, depth + 1, max_depth))\n",
        "                elif full_url.endswith('.rar'):\n",
        "                    logger.debug(f\"Found RAR file: {full_url}\")\n",
        "                    rar_files.append(full_url)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error accessing URL {url}: {e}\")\n",
        "    logger.debug(f\"Finished processing URL: {url}. Found {len(rar_files)} RAR files in this branch.\")\n",
        "    return rar_files\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    \"\"\"Download a file from a URL.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        logger.info(f\"{output_path} already exists. Skipping download.\")\n",
        "        return True # Indicate success as file exists\n",
        "    logger.info(f\"Attempting to download {url} to {output_path}\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        logger.info(f\"Successfully downloaded {url} to {output_path}\")\n",
        "        return True # Indicate success\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error downloading file from {url}: {e}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "\n",
        "def extract_rar(file_path, output_path):\n",
        "    \"\"\"Extract a RAR file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"RAR file not found for extraction: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "        logger.debug(f\"Created output directory for extraction: {output_path}\")\n",
        "    logger.info(f\"Attempting to extract {file_path} to {output_path}\")\n",
        "    try:\n",
        "        # Added -o+ to overwrite without prompting\n",
        "        result = subprocess.run(['unrar', 'x', '-o+', file_path, output_path], check=True, capture_output=True, text=True)\n",
        "        logger.info(f\"Successfully extracted {file_path} to {output_path}\")\n",
        "        # Log stdout and stderr for debugging\n",
        "        if result.stdout:\n",
        "            logger.debug(f\"Unrar stdout for {file_path}:\\n{result.stdout}\")\n",
        "        if result.stderr:\n",
        "             logger.debug(f\"Unrar stderr for {file_path}:\\n{result.stderr}\")\n",
        "        return True # Indicate success\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error extracting {file_path}: {e.stderr}\")\n",
        "        return False # Indicate failure\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"Unrar command not found. Please ensure 'unrar' is installed.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during extraction of {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Sanitize a filename.\"\"\"\n",
        "    sanitized = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)\n",
        "    logger.debug(f\"Sanitized filename '{filename}' to '{sanitized}'\")\n",
        "    return sanitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-6"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### 3.2. PDF Processing (Nougat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nougat-cell"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import logging\n",
        "# Assuming sanitize_filename is defined in file-ops-cell and available\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def process_pdf(pdf_path, output_dir):\n",
        "    if not os.path.exists(pdf_path):\n",
        "        logger.error(f\"PDF file not found for processing: {pdf_path}\")\n",
        "        return None\n",
        "\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(pdf_path))\n",
        "    mmd_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}.mmd\")\n",
        "\n",
        "    if os.path.exists(mmd_path):\n",
        "        logger.info(f\"{mmd_path} already exists. Skipping Nougat processing for {pdf_path}.\")\n",
        "        return mmd_path\n",
        "\n",
        "    logger.info(f\"Attempting to process PDF: {pdf_path} with Nougat. Output to {output_dir}\")\n",
        "\n",
        "    try:\n",
        "        # Stream output live using Popen\n",
        "        process = subprocess.Popen(\n",
        "            ['nougat', pdf_path, '-o', output_dir, '--no-skipping', '--recompute'],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            bufsize=1,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        for line in process.stdout:\n",
        "            sys.stdout.write(line)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        process.stdout.close()\n",
        "        return_code = process.wait()\n",
        "\n",
        "        if return_code != 0:\n",
        "            logger.error(f\"Nougat process failed with exit code {return_code}\")\n",
        "            return None\n",
        "\n",
        "        logger.info(f\"Successfully processed {pdf_path} with Nougat.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during Nougat processing of {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    if not os.path.exists(mmd_path):\n",
        "        logger.error(f\"Nougat command finished but expected output {mmd_path} not found.\")\n",
        "        return None\n",
        "\n",
        "    return mmd_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-7"
      },
      "source": [
        "### 3.3. Text Cleaning and Quality Control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cleaning-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02ef5c7-7cc3-4626-8276-337dc5c282a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 15:01:36,670 - INFO - NLTK English words corpus loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:NLTK English words corpus loaded.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import logging\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown # Assuming these are downloaded in config-cell\n",
        "from nltk.tokenize import word_tokenize # Assuming this is downloaded in config-cell\n",
        "from textblob import TextBlob # Assuming TextBlob is installed\n",
        "\n",
        "# Assuming GARBAGE_THRESHOLD and LENWORD are defined in config-cell\n",
        "# from .config import GARBAGE_THRESHOLD, LENWORD # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the NLTK words corpus for garbage detection\n",
        "# Ensure this is done after nltk.download('words') in config-cell\n",
        "try:\n",
        "    ENGLISH_WORDS = set(words.words())\n",
        "    logger.info(\"NLTK English words corpus loaded.\")\n",
        "except LookupError:\n",
        "    logger.error(\"NLTK 'words' corpus not found. Please run nltk.download('words').\")\n",
        "    ENGLISH_WORDS = set() # Use an empty set to avoid errors\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean the extracted text.\"\"\"\n",
        "    logger.debug(f\"Cleaning text (first 100 chars): {text[:100]}...\")\n",
        "    initial_len = len(text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "    # Remove academic citations, references to tables/figures, and bracketed content.\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE)\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    # Sanitize garbled punctuation and symbols.\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text)\n",
        "    logger.debug(f\"Cleaned text (first 100 chars, original len {initial_len}): {text[:100]}...\")\n",
        "    return text\n",
        "\n",
        "def calculate_text_quality_score(text):\n",
        "    \"\"\"Calculate a quality score based on English words and sentence structure.\"\"\"\n",
        "    if not text:\n",
        "        return 0.0\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    if not words:\n",
        "        return 0.0\n",
        "\n",
        "    english_word_count = sum(1 for word in words if word.lower() in ENGLISH_WORDS)\n",
        "    english_word_ratio = english_word_count / len(words) if words else 0\n",
        "\n",
        "    # Simple heuristic for sentence structure (check for punctuation at end of sentences)\n",
        "    sentences = sent_tokenize(text)\n",
        "    well_formed_sentences = sum(1 for sent in sentences if sent.strip().endswith(('.', '!', '?')))\n",
        "    sentence_structure_score = well_formed_sentences / len(sentences) if sentences else 0\n",
        "\n",
        "    # Combine ratios - adjust weights as needed\n",
        "    quality_score = (english_word_ratio * 0.7) + (sentence_structure_score * 0.3)\n",
        "\n",
        "    logger.debug(f\"Text quality score calculated: {quality_score} for text (first 50 chars): {text[:50]}...\")\n",
        "    return quality_score\n",
        "\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, lenword=LENWORD):\n",
        "    \"\"\"Check if the text is garbage based on various heuristics.\"\"\"\n",
        "    logger.debug(f\"Checking if text is garbage (first 100 chars): {text[:100]}...\")\n",
        "\n",
        "    # Check for minimal length\n",
        "    if not text or len(text.split()) < 5: # Reduced minimum words\n",
        "        logger.debug(\"Identified as garbage: text too short or empty.\")\n",
        "        return True\n",
        "\n",
        "    # Language detection\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            logger.debug(\"Identified as garbage: language not English.\")\n",
        "            return True\n",
        "    except LangDetectException as e:\n",
        "        logger.debug(f\"Language detection failed for text (first 50 chars): {text[:50]}... Error: {e}. Assuming garbage.\")\n",
        "        return True # Assume garbage if language detection fails\n",
        "\n",
        "    # Check for jammed words (long strings without spaces)\n",
        "    words_list = text.split()\n",
        "    for word in words_list:\n",
        "        if len(word) > lenword and not '-' in word: # Allow hyphens in long words\n",
        "             logger.debug(f\"Identified as garbage: found jammed word '{word[:50]}...'\")\n",
        "             return True\n",
        "\n",
        "    # Check for unusually high proportion of single-letter words\n",
        "    single_letter_words = sum(1 for word in words_list if len(word) == 1)\n",
        "    if len(words_list) > 0 and single_letter_words / len(words_list) > 0.2: # More than 20% single letters\n",
        "        logger.debug(\"Identified as garbage: high proportion of single-letter words.\")\n",
        "        return True\n",
        "\n",
        "    # Check for repetitive patterns (simple heuristic)\n",
        "    if re.search(r'(.)\\1{4,}', text): # 5 or more of the same character in a row\n",
        "        logger.debug(\"Identified as garbage: found repetitive character pattern.\")\n",
        "        return True\n",
        "    if re.search(r'(\\w+\\s+)\\1{2,}', text): # A word repeated 3 or more times\n",
        "         logger.debug(\"Identified as garbage: found repetitive word pattern.\")\n",
        "         return True\n",
        "\n",
        "\n",
        "    # Quality scoring\n",
        "    quality_score = calculate_text_quality_score(text)\n",
        "    if quality_score < threshold:\n",
        "        logger.debug(f\"Identified as garbage: quality score {quality_score} below threshold {threshold}.\")\n",
        "        return True\n",
        "\n",
        "    logger.debug(\"Text passed garbage checks.\")\n",
        "    return False\n",
        "\n",
        "# Ensure GARBAGE_THRESHOLD and LENWORD are available if not in this cell\n",
        "# GARBAGE_THRESHOLD = 0.8 # Example default\n",
        "# LENWORD = 50 # Example default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-8"
      },
      "source": [
        "### 3.4. Text Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "chunking-cell"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize # Assuming this is downloaded in config-cell\n",
        "\n",
        "# Assuming clean_text and is_garbage are defined in cleaning-cell and available\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def chunk_text(content, max_size=8192):\n",
        "    \"\"\"Chunk the text into smaller segments, respecting markdown headings.\"\"\"\n",
        "    logger.debug(f\"Starting chunking process with max_size={max_size}.\")\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    lines = content.split('\\n')\n",
        "    logger.debug(f\"Splitting content into {len(lines)} lines.\")\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        # Check for markdown headings\n",
        "        if line.strip().startswith((\"# \", \"## \", \"### \")):\n",
        "            logger.debug(f\"Found markdown heading at line {i}: {line.strip()}\")\n",
        "            # If the current segment is not empty, process it before starting a new one\n",
        "            if current_segment:\n",
        "                logger.debug(f\"Processing previous segment before heading (length: {len(current_segment)}).\")\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            # Start a new segment with the heading line\n",
        "            current_segment = line + \"\\n\" # Keep the heading line in the new segment\n",
        "            logger.debug(\"Starting new segment after heading.\")\n",
        "        else:\n",
        "            # Add non-heading lines to the current segment\n",
        "            current_segment += line + \"\\n\"\n",
        "\n",
        "    # Process any remaining content in the last segment\n",
        "    if current_segment:\n",
        "        logger.debug(f\"Processing final segment (length: {len(current_segment)}).\")\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    logger.info(f\"Chunking complete. Produced {len(segments)} initial segments based on headings.\")\n",
        "    return segments\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    \"\"\"Split a segment (potentially from a heading section) into smaller chunks by sentences.\"\"\"\n",
        "    logger.debug(f\"Splitting segment by sentences (length: {len(segment)}).\")\n",
        "    sentences = sent_tokenize(segment)\n",
        "    logger.debug(f\"Segment split into {len(sentences)} sentences.\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Add a space before adding the new sentence if the current chunk is not empty\n",
        "        sentence_to_add = sentence + \" \" if current_chunk else sentence\n",
        "        # Check if adding the current sentence exceeds the max size\n",
        "        if len(current_chunk) + len(sentence_to_add) <= max_size:\n",
        "            current_chunk += sentence_to_add\n",
        "            logger.debug(f\"Added sentence {i+1}/{len(sentences)} to current chunk (current size: {len(current_chunk)}).\")\n",
        "        else:\n",
        "            # If adding the sentence exceeds max size, add the current chunk to chunks list\n",
        "            if current_chunk: # Add the chunk only if it's not empty\n",
        "                chunks.append(current_chunk.strip())\n",
        "                logger.debug(f\"Chunk completed (size: {len(current_chunk)}). Starting new chunk with sentence {i+1}.\")\n",
        "            # Start a new chunk with the current sentence\n",
        "            current_chunk = sentence + \" \" # Start new chunk with the current sentence\n",
        "\n",
        "    # Add the last current chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "        logger.debug(f\"Added final chunk (size: {len(current_chunk)}).\")\n",
        "\n",
        "    logger.debug(f\"Segment split into {len(chunks)} smaller chunks.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def process_and_chunk_mmd(mmd_path, output_dir):\n",
        "    \"\"\"Process, clean, chunk, and categorize text from an MMD file.\"\"\"\n",
        "    logger.info(f\"Starting processing and chunking for MMD file: {mmd_path}\")\n",
        "\n",
        "    if not mmd_path or not os.path.exists(mmd_path):\n",
        "        logger.warning(f\"MMD file not found or path is invalid: {mmd_path}. Skipping processing and chunking.\")\n",
        "        return None, None\n",
        "\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(mmd_path))\n",
        "    cleaned_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_cleaned.jsonl\")\n",
        "    garbage_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_garbage.jsonl\")\n",
        "\n",
        "    if os.path.exists(cleaned_jsonl_path) and os.path.exists(garbage_jsonl_path):\n",
        "        logger.info(f\"Output files {cleaned_jsonl_path} and {garbage_jsonl_path} already exist. Skipping processing and chunking for {mmd_path}.\")\n",
        "        return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    try:\n",
        "        with open(mmd_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        logger.debug(f\"Successfully read content from {mmd_path} (length: {len(content)}).\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading MMD file {mmd_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    chunks = chunk_text(content)\n",
        "    logger.info(f\"MMD content chunked into {len(chunks)} segments.\")\n",
        "\n",
        "    cleaned_count = 0\n",
        "    garbage_count = 0\n",
        "\n",
        "    try:\n",
        "        with open(cleaned_jsonl_path, 'w', encoding='utf-8') as cleaned_f, \\\n",
        "             open(garbage_jsonl_path, 'w', encoding='utf-8') as garbage_f:\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Processing chunk {i+1}/{len(chunks)} (length: {len(chunk)}).\")\n",
        "                cleaned_chunk = clean_text(chunk)\n",
        "                if is_garbage(cleaned_chunk):\n",
        "                    garbage_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    garbage_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as garbage.\")\n",
        "                else:\n",
        "                    cleaned_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    cleaned_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as cleaned text.\")\n",
        "\n",
        "        logger.info(f\"Finished processing and chunking {mmd_path}. Generated {cleaned_count} cleaned chunks and {garbage_count} garbage chunks.\")\n",
        "        return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during cleaning or writing chunk files for {mmd_path}: {e}\")\n",
        "        # Clean up potentially incomplete files\n",
        "        if os.path.exists(cleaned_jsonl_path):\n",
        "            os.remove(cleaned_jsonl_path)\n",
        "        if os.path.exists(garbage_jsonl_path):\n",
        "            os.remove(garbage_jsonl_path)\n",
        "        return None, None\n",
        "\n",
        "# Ensure sanitize_filename, clean_text, is_garbage are available\n",
        "# from .file_ops import sanitize_filename # Example if in a different file\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-9"
      },
      "source": [
        "### 3.5. Hugging Face Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "huggingface-cell"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "from huggingface_hub import HfApi, Repository # Import Repository for better practice if needed for cloning/managing\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def upload_to_huggingface(file_path, repo_id, repo_type=\"dataset\"):\n",
        "    \"\"\"Upload a file to a Hugging Face repository.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"File not found for upload to Hugging Face: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "    logger.info(f\"Attempting to upload {file_path} to Hugging Face repo '{repo_id}' (type: {repo_type}).\")\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        # Use create_commit for potentially better handling of multiple files or larger uploads\n",
        "        # This example uses upload_file for simplicity as in the original code\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=os.path.basename(file_path),\n",
        "            repo_id=repo_id,\n",
        "            repo_type=repo_type,\n",
        "            # Optional: add commit_message, token if not using environment variable\n",
        "        )\n",
        "        logger.info(f\"Successfully uploaded {file_path} to {repo_id}\")\n",
        "        return True # Indicate success\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error uploading {file_path} to Hugging Face repo '{repo_id}': {e}\")\n",
        "        return False # Indicate failure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "markdown-10"
      },
      "source": [
        "## 4. Main Processing Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da8f2ecb"
      },
      "source": [
        "## Scan and process local pdfs\n",
        "\n",
        "### Subtask:\n",
        "Modify the main loop to first search for and process any existing PDF files within the anticipated output directory structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f5ce23e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0caeab8d-9dca-45fd-d5c6-826dd259c574"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # Import ThreadPoolExecutor\n",
        "# Assuming helper functions are defined in other cells and available\n",
        "# from .file_ops import get_file_list, download_file, extract_rar, sanitize_filename\n",
        "# from .nougat_processing import process_pdf\n",
        "# from .chunking import process_and_chunk_mmd\n",
        "# from .huggingface_integration import upload_to_huggingface\n",
        "# from .config import BASE_URL, HUGGING_FACE_REPO # Assuming these are defined in config-cell\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define a cache file path\n",
        "RAR_LIST_CACHE = \"rar_list_cache.json\"\n",
        "\n",
        "def process_single_pdf_local(pdf_path, HUGGING_FACE_REPO):\n",
        "    \"\"\"Processes a single local PDF file: processes with Nougat, cleans, chunks, and uploads.\"\"\"\n",
        "    logger.info(f\"--- Processing local PDF: {pdf_path} ---\")\n",
        "\n",
        "    output_dir = os.path.dirname(pdf_path) # Use the PDF's directory as the output directory\n",
        "\n",
        "    # 1. Process PDF with Nougat\n",
        "    mmd_path = process_pdf(pdf_path, output_dir)\n",
        "\n",
        "    if mmd_path:\n",
        "        logger.info(f\"Nougat processing successful for {pdf_path}. MMD file: {mmd_path}\")\n",
        "        # 2. Clean and Chunk MMD file\n",
        "        cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, output_dir)\n",
        "\n",
        "        # 3. Upload to Hugging Face\n",
        "        if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "            logger.info(f\"Uploading cleaned data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "            if upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO):\n",
        "                return 1 # Indicate success\n",
        "            else:\n",
        "                logger.error(f\"Failed to upload cleaned data for {os.path.basename(pdf_path)}.\")\n",
        "                return 0 # Indicate failure\n",
        "        else:\n",
        "            logger.warning(f\"No cleaned data generated for {os.path.basename(pdf_path)}. Skipping upload.\")\n",
        "            return 0 # Indicate no cleaned data\n",
        "\n",
        "    else:\n",
        "        logger.error(f\"Nougat processing failed for {pdf_path}. Skipping cleaning, chunking, and upload.\")\n",
        "        return 0 # Indicate failure\n",
        "\n",
        "\n",
        "def process_single_rar(rar_file_url, HUGGING_FACE_REPO):\n",
        "    \"\"\"Processes a single RAR file: downloads, extracts, processes PDFs, and uploads.\"\"\"\n",
        "    rar_filename = rar_file_url.split('/')[-1]\n",
        "    sanitized_rar_filename = sanitize_filename(rar_filename)\n",
        "    rar_path = sanitized_rar_filename\n",
        "    extract_path = os.path.splitext(rar_path)[0]\n",
        "\n",
        "    logger.info(f\"--- Processing {rar_filename} ---\")\n",
        "\n",
        "    # 1. Download RAR file\n",
        "    # The download_file function now correctly returns True on success\n",
        "    if not download_file(rar_file_url, rar_path):\n",
        "        logger.error(f\"Failed to download RAR file: {rar_file_url}. Skipping.\")\n",
        "        return 0\n",
        "\n",
        "    # 2. Extract RAR file\n",
        "    if not extract_rar(rar_path, extract_path):\n",
        "        logger.error(f\"Failed to extract RAR file: {rar_path}. Cleaning up and skipping.\")\n",
        "        if os.path.exists(rar_path):\n",
        "            os.remove(rar_path)\n",
        "            logger.debug(f\"Removed failed RAR file: {rar_path}\")\n",
        "        return 0\n",
        "\n",
        "    # Clean up the downloaded RAR after successful extraction\n",
        "    if os.path.exists(rar_path):\n",
        "        os.remove(rar_path)\n",
        "        logger.debug(f\"Removed downloaded RAR file: {rar_path}\")\n",
        "\n",
        "\n",
        "    # 3. Find and Process PDF files within the extracted directory\n",
        "    pdf_files = [os.path.join(root, file) for root, _, files in os.walk(extract_path) for file in files if file.lower().endswith('.pdf')]\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files in extracted directory: {extract_path}\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.warning(f\"No PDF files found in {extract_path}. Cleaning up.\")\n",
        "        # Clean up the extracted directory\n",
        "        if os.path.exists(extract_path):\n",
        "            shutil.rmtree(extract_path)\n",
        "            logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "        return 0 # Indicate no PDFs processed\n",
        "\n",
        "    successful_uploads_count = 0\n",
        "    with tqdm(total=len(pdf_files), desc=f\"Processing PDFs in {sanitized_rar_filename}\", leave=False) as pbar_pdfs:\n",
        "        for pdf_path in pdf_files:\n",
        "            logger.info(f\"Processing PDF: {pdf_path}\")\n",
        "\n",
        "            # 4. Process PDF with Nougat\n",
        "            mmd_path = process_pdf(pdf_path, extract_path)\n",
        "\n",
        "            if mmd_path:\n",
        "                logger.info(f\"Nougat processing successful for {pdf_path}. MMD file: {mmd_path}\")\n",
        "                # 5. Clean and Chunk MMD file\n",
        "                cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, extract_path)\n",
        "\n",
        "                # 6. Upload to Hugging Face\n",
        "                if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "                    logger.info(f\"Uploading cleaned data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    if upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO):\n",
        "                        successful_uploads_count += 1\n",
        "                    else:\n",
        "                        logger.error(f\"Failed to upload cleaned data for {os.path.basename(pdf_path)}.\")\n",
        "                    # Optionally upload garbage data\n",
        "                    ##if garbage_jsonl and os.path.exists(garbage_jsonl):\n",
        "                    ##    logger.info(f\"Uploading garbage data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    ##    upload_to_huggingface(garbage_jsonl, HUGGING_FACE_REPO)\n",
        "                else:\n",
        "                    logger.warning(f\"No cleaned data generated for {os.path.basename(pdf_path)}. Skipping upload.\")\n",
        "            else:\n",
        "                logger.error(f\"Nougat processing failed for {pdf_path}. Skipping cleaning, chunking, and upload.\")\n",
        "\n",
        "            pbar_pdfs.update(1) # Update inner progress bar for each PDF\n",
        "\n",
        "    # 7. Clean up extracted directory after processing all PDFs in the RAR\n",
        "    logger.info(f\"Cleaning up extracted directory for {rar_filename}.\")\n",
        "    if os.path.exists(extract_path):\n",
        "        shutil.rmtree(extract_path)\n",
        "        logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "\n",
        "    return successful_uploads_count\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process the library.\"\"\"\n",
        "    logger.info(\"--- Starting Library Processing Pipeline ---\")\n",
        "\n",
        "    total_local_uploads = 0\n",
        "    logger.info(\"--- Processing Local PDF Files ---\")\n",
        "\n",
        "    local_pdf_files = []\n",
        "    for root, _, files in os.walk(\".\"): # Scan current directory and subdirectories\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.pdf'):\n",
        "                local_pdf_files.append(os.path.join(root, file))\n",
        "\n",
        "    logger.info(f\"Found {len(local_pdf_files)} local PDF files.\")\n",
        "\n",
        "    with tqdm(total=len(local_pdf_files), desc=\"Processing Local PDFs\") as pbar_local_pdfs:\n",
        "        for pdf_path in local_pdf_files:\n",
        "            total_local_uploads += process_single_pdf_local(pdf_path, HUGGING_FACE_REPO)\n",
        "            pbar_local_pdfs.update(1)\n",
        "\n",
        "    logger.info(f\"Finished processing local PDF files. Successfully uploaded cleaned data for {total_local_uploads} files.\")\n",
        "    logger.info(\"--- Finished Processing Local PDF Files ---\")\n",
        "\n",
        "\n",
        "    rar_files = []\n",
        "    if os.path.exists(RAR_LIST_CACHE):\n",
        "        logger.info(f\"Loading RAR file list from cache: {RAR_LIST_CACHE}\")\n",
        "        try:\n",
        "            with open(RAR_LIST_CACHE, 'r') as f:\n",
        "                rar_files = json.load(f)\n",
        "            logger.info(f\"Loaded {len(rar_files)} RAR files from cache.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading RAR file list from cache: {e}. Rescanning.\")\n",
        "            # If loading fails, proceed to rescan\n",
        "            rar_files = []\n",
        "\n",
        "    if not rar_files:\n",
        "        logger.info(f\"Scanning for RAR files at {BASE_URL}\")\n",
        "        try:\n",
        "            rar_files = get_file_list(BASE_URL)\n",
        "            logger.info(f\"Found {len(rar_files)} RAR files.\")\n",
        "            # Save the list to cache\n",
        "            try:\n",
        "                with open(RAR_LIST_CACHE, 'w') as f:\n",
        "                    json.dump(rar_files, f)\n",
        "                logger.info(f\"Saved RAR file list to cache: {RAR_LIST_CACHE}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving RAR file list to cache: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get RAR file list from {BASE_URL}: {e}\")\n",
        "            # Continue processing even if initial scan fails, as local processing is done\n",
        "            pass # Changed from return to pass\n",
        "\n",
        "    total_rar_uploads = 0\n",
        "\n",
        "    if rar_files:\n",
        "        logger.info(\"--- Processing Downloaded RAR Files ---\")\n",
        "        # Use ThreadPoolExecutor to process RAR files in parallel\n",
        "        # Adjust max_workers based on your runtime's capabilities and the task\n",
        "        max_workers = 1 # Example: process 1 RAR at a time to avoid resource issues\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit tasks to the executor\n",
        "            future_to_rar = {executor.submit(process_single_rar, rar_file_url, HUGGING_FACE_REPO): rar_file_url for rar_file_url in rar_files}\n",
        "\n",
        "            # Use tqdm to track overall progress\n",
        "            with tqdm(total=len(rar_files), desc=\"Overall RAR Processing\") as pbar_overall:\n",
        "                for future in as_completed(future_to_rar):\n",
        "                    rar_file_url = future_to_rar[future]\n",
        "                    try:\n",
        "                        successful_uploads_count = future.result()\n",
        "                        total_rar_uploads += successful_uploads_count\n",
        "                    except Exception as exc:\n",
        "                        logger.error(f'{rar_file_url} generated an exception: {exc}')\n",
        "\n",
        "                    pbar_overall.update(1) # Update outer progress bar for each completed RAR\n",
        "        logger.info(f\"Finished processing downloaded RAR files. Successfully uploaded cleaned data for {total_rar_uploads} PDF files.\")\n",
        "        logger.info(\"--- Finished Processing Downloaded RAR Files ---\")\n",
        "\n",
        "\n",
        "    logger.info(\"--- Library Processing Pipeline Finished ---\")\n",
        "    logger.info(f\"Total successfully uploaded cleaned data for {total_local_uploads + total_rar_uploads} PDF files to {HUGGING_FACE_REPO}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 15:07:14,355 - INFO - --- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 15:07:14,356 - INFO - --- Processing Local PDF Files ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Local PDF Files ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 15:07:14,360 - INFO - Found 329 local PDF files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Found 329 local PDF files.\n",
            "Processing Local PDFs:   0%|          | 0/329 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 15:07:14,363 - INFO - --- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing local PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 15:07:14,364 - DEBUG - Sanitized filename 'Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf' to 'Ingrid_Fuglestvedt_-_Rock_Art_and_the_Wild_Mind._Visual_Imagery_in_Mesolithic_Northern_Europe__Retail_.pdf'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:__main__:Sanitized filename 'Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf' to 'Ingrid_Fuglestvedt_-_Rock_Art_and_the_Wild_Mind._Visual_Imagery_in_Mesolithic_Northern_Europe__Retail_.pdf'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-02 15:07:14,365 - INFO - Attempting to process PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/Mesolithic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Attempting to process PDF: ./1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf with Nougat. Output to ./1._20Prehistory/1. Prehistory/Mesolithic\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/115 [00:00<?, ?it/s]INFO:root:Processing file 1._20Prehistory/1. Prehistory/Mesolithic/Ingrid Fuglestvedt - Rock Art and the Wild Mind. Visual Imagery in Mesolithic Northern Europe [Retail].pdf with 458 pages\n",
            "\n",
            "  1%|          | 1/115 [00:08<15:47,  8.31s/it]\n",
            "  2%|         | 2/115 [00:55<58:22, 30.99s/it]\n",
            "  3%|         | 3/115 [01:03<38:44, 20.75s/it]\n",
            "  3%|         | 4/115 [01:12<29:52, 16.15s/it]\n",
            "  4%|         | 5/115 [01:59<49:42, 27.11s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0567ee4d"
      },
      "source": [
        "**Reasoning**:\n",
        "Search online for Nougat's Python API documentation or examples to determine if it can be used without subprocess.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}