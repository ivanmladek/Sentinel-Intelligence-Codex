{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanmladek/Sentinel-Intelligence-Codex/blob/main/process_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Processing Pipeline"
      ],
      "metadata": {
        "id": "markdown-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of extracting, cleaning, and preparing the text from PDF files for the LLM is a multi-stage pipeline designed to ensure high-quality, structured data. This process is orchestrated by the process_refactor.ipynb notebook.\n",
        "\n",
        "1. Environment Setup and PDF Discovery\n",
        "\n",
        "Dependencies: The process begins by installing necessary Python libraries, including nougat-ocr for text extraction, nltk for natural language processing, and langdetect for language identification.\n",
        "PDF Discovery: The script recursively scans a specified directory (e.g., a Google Drive folder) to locate all PDF files.\n",
        "2. Text Extraction with Nougat\n",
        "\n",
        "Nougat OCR: For each PDF, the nougat command-line tool is used. Nougat is a state-of-the-art OCR tool specifically designed for academic and scientific documents, capable of recognizing and transcribing complex layouts, mathematical equations, and tables into a structured Markdown format (.mmd).\n",
        "Output: The raw extracted text is saved as a .mmd file, preserving the document's structure with Markdown headings.\n",
        "3. Text Cleaning and Garbage Detection\n",
        "\n",
        "This is a critical step to filter out irrelevant or low-quality text.\n",
        "\n",
        "Cleaning: A series of regular expressions and cleaning functions are applied to the raw text to:\n",
        "Remove extra newlines, spaces, and non-ASCII characters.\n",
        "Eliminate academic citations, references to tables/figures, and bracketed content.\n",
        "Sanitize garbled punctuation and symbols.\n",
        "Garbage Detection: Each segment of text is evaluated against a set of criteria to identify and discard \"garbage\" content. This includes:\n",
        "Language Detection: Text that is not identified as English is discarded.\n",
        "Heuristics: Checks for \"jammed\" words (long strings of characters without spaces), an unusually high proportion of single-letter words, and repetitive patterns.\n",
        "Quality Scoring: A text_quality_score is calculated based on the presence of common English words, proper part-of-speech patterns, and other linguistic features. Text falling below a certain threshold is flagged as garbage.\n",
        "4. Tokenization and Chunking\n",
        "\n",
        "Chunking Strategy: The cleaned .mmd content is chunked into smaller, manageable segments suitable for the LLM. The chunking logic is designed to respect the document's structure:\n",
        "The text is split by Markdown headings (#, ##, ###).\n",
        "These larger sections are then further divided into sentences using nltk.sent_tokenize.\n",
        "Size Constraints: The sentences are grouped into chunks with a maximum size (e.g., 8192 characters) to ensure they fit within the model's context window, while avoiding splitting sentences in the middle.\n",
        "Final Output: The cleaned, chunked text is saved to a .jsonl file, with each line containing a JSON object with a single \"text\" key, ready for training the LLM. Garbage text is saved to a separate file for review."
      ],
      "metadata": {
        "id": "qYux-yCBECz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Dependencies"
      ],
      "metadata": {
        "id": "markdown-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cybg_6vE2qM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9079523d-875d-4cb6-801d-2215c31eb82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "libmagic-dev is already the newest version (1:5.41-3ubuntu0.1).\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "unrar is already the newest version (1:6.1.5-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#@title Install System Dependencies\n",
        "!apt-get install -y poppler-utils tesseract-ocr libmagic-dev unrar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Python Libraries (Part 1)\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "MO_yilG-WRis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc308088-aada-4b63-be49-e28beb640808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Python Libraries (Part 2)\n",
        "!pip install transformers==4.38.2 pyarrow==14.0.1 timm==0.5.4 requests==2.31.0 albumentations==1.0.0 git+https://github.com/facebookresearch/nougat\n",
        "!pip install textblob langdetect beautifulsoup4 huggingface_hub tqdm pandas"
      ],
      "metadata": {
        "id": "1sjFat5tLLfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966b718e-2153-4cf9-f71e-9aacb7baa90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/nougat\n",
            "  Cloning https://github.com/facebookresearch/nougat to /tmp/pip-req-build-z4g1vguv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/nougat /tmp/pip-req-build-z4g1vguv\n",
            "  Resolved https://github.com/facebookresearch/nougat to commit 5a92920d342fb6acf05fc9b594ccb4053dbe8e7a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.38.2\n",
            "  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting timm==0.5.4\n",
            "  Downloading timm-0.5.4-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting requests==2.31.0\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting albumentations==1.0.0\n",
            "  Downloading albumentations-1.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2024.11.6)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4) (0.21.0+cu124)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0) (2025.6.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (1.15.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (0.25.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.0.0) (4.11.0.86)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.10.18)\n",
            "Requirement already satisfied: datasets[vision] in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (2.14.4)\n",
            "Collecting lightning<2022,>=2.0.0 (from nougat-ocr==0.1.18)\n",
            "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (3.9.1)\n",
            "Collecting rapidfuzz (from nougat-ocr==0.1.18)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.18) (0.2.0)\n",
            "Collecting sconf>=0.2.3 (from nougat-ocr==0.1.18)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pypdf>=3.1.0 (from nougat-ocr==0.1.18)\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pypdfium2 (from nougat-ocr==0.1.18)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.1.5)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr==0.1.18)\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.0.0) (0.4)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting munch (from sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.5.4)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.5.4) (1.3.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.18) (3.11.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.18) (1.5.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.18) (1.20.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.18) (75.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.5.4) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.18) (2025.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.18)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.18) (1.17.0)\n",
            "Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-14.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-1.0.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nougat-ocr\n",
            "  Building wheel for nougat-ocr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nougat-ocr: filename=nougat_ocr-0.1.18-py3-none-any.whl size=81724 sha256=f84425a35f456e24fd0ed33fdd205bf53943ccd16eff4e013504f22eabae9a6a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-galcl48s/wheels/a8/b1/ca/3509cca72deee3fab5cf02371a58b5d57b82b1c99656402e8c\n",
            "Successfully built nougat-ocr\n",
            "Installing collected packages: ruamel.yaml.clib, requests, rapidfuzz, pypdfium2, pypdf, pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, lightning-utilities, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, sconf, nvidia-cusolver-cu12, albumentations, transformers, torchmetrics, timm, pytorch-lightning, lightning, nougat-ocr\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.8\n",
            "    Uninstalling albumentations-2.0.8:\n",
            "      Successfully uninstalled albumentations-2.0.8\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.15\n",
            "    Uninstalling timm-1.0.15:\n",
            "      Successfully uninstalled timm-1.0.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "bigframes 2.8.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed albumentations-1.0.0 lightning-2.5.2 lightning-utilities-0.14.3 munch-4.0.0 nougat-ocr-0.1.18 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyarrow-14.0.1 pypdf-5.7.0 pypdfium2-4.30.1 pytorch-lightning-2.5.2 rapidfuzz-3.13.0 requests-2.31.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 sconf-0.2.5 timm-0.5.4 tokenizers-0.15.2 torchmetrics-1.7.3 transformers-4.38.2\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=f04d9647e3d6dd90fe038ea22827f516e87eb009954b9ed9c9544864a017dc18\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Imports and Configuration"
      ],
      "metadata": {
        "id": "markdown-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import logging\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from huggingface_hub import HfApi\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_URL = \"https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/\"\n",
        "HUGGING_FACE_REPO = \"ivanmladek/Sentinel-Intelligence-Codex\"  # Replace with your Hugging Face repo\n",
        "GARBAGE_THRESHOLD = 0.8\n",
        "LENWORD = 50\n",
        "\n",
        "# --- Logging Setup ---\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.propagate = True # Ensure messages are propagated to the root logger\n",
        "\n",
        "# Explicitly set the logging level and add a handler to print to stdout\n",
        "logger.setLevel(logging.INFO)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "# Avoid adding duplicate handlers if the cell is run multiple times\n",
        "if not logger.handlers:\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# --- Download NLTK Data ---\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "config-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9814d02-1fd1-4971-83da-110c08df8967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Helper Functions"
      ],
      "metadata": {
        "id": "markdown-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. File and Web Operations"
      ],
      "metadata": {
        "id": "markdown-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import logging\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def get_file_list(url, depth=0, max_depth=3):\n",
        "    \"\"\"Recursively get a list of files from a URL and its subdirectories up to a max depth.\"\"\"\n",
        "    if depth > max_depth:\n",
        "        logger.debug(f\"Max depth ({max_depth}) reached at URL: {url}. Stopping recursion.\")\n",
        "        return []\n",
        "\n",
        "    rar_files = []\n",
        "    # Configure retries\n",
        "    retry_strategy = Retry(\n",
        "        total=3,  # Number of retries\n",
        "        backoff_factor=1, # Factor by which the delay increases\n",
        "        status_forcelist=[429, 500, 502, 503, 504] # HTTP status codes to retry on\n",
        "    )\n",
        "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "    http = requests.Session()\n",
        "    http.mount(\"http://\", adapter)\n",
        "    http.mount(\"https://\", adapter)\n",
        "\n",
        "    logger.info(f\"Accessing URL: {url} (Depth: {depth})\")\n",
        "    try:\n",
        "        response = http.get(url)\n",
        "        response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                # Handle relative and absolute links\n",
        "                full_url = requests.compat.urljoin(url, href)\n",
        "                if full_url.endswith('.rar'):\n",
        "                    logger.debug(f\"Found RAR file: {full_url}\")\n",
        "                    rar_files.append(full_url)\n",
        "                elif full_url.endswith('/'):\n",
        "                    # Recursively call for subdirectories, avoiding infinite loops\n",
        "                    if url != full_url: # Avoid processing the same directory again\n",
        "                         logger.debug(f\"Found subdirectory: {full_url}. Recursing.\")\n",
        "                         rar_files.extend(get_file_list(full_url, depth + 1, max_depth))\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error accessing URL {url}: {e}\")\n",
        "    logger.debug(f\"Finished processing URL: {url}. Found {len(rar_files)} RAR files in this branch.\")\n",
        "    return rar_files\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    \"\"\"Download a file from a URL.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        logger.info(f\"{output_path} already exists. Skipping download.\")\n",
        "        return True # Indicate success as file exists\n",
        "    logger.info(f\"Attempting to download {url} to {output_path}\")\n",
        "    try:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        with open(output_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        logger.info(f\"Successfully downloaded {url} to {output_path}\")\n",
        "        return True # Indicate success\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logger.error(f\"Error downloading file from {url}: {e}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "\n",
        "def extract_rar(file_path, output_path):\n",
        "    \"\"\"Extract a RAR file.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"RAR file not found for extraction: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "    if not os.path.exists(output_path):\n",
        "        os.makedirs(output_path)\n",
        "        logger.debug(f\"Created output directory for extraction: {output_path}\")\n",
        "    logger.info(f\"Attempting to extract {file_path} to {output_path}\")\n",
        "    try:\n",
        "        # Added -o+ to overwrite without prompting\n",
        "        result = subprocess.run(['unrar', 'x', '-o+', file_path, output_path], check=True, capture_output=True, text=True)\n",
        "        logger.info(f\"Successfully extracted {file_path} to {output_path}\")\n",
        "        # Log stdout and stderr for debugging\n",
        "        if result.stdout:\n",
        "            logger.debug(f\"Unrar stdout for {file_path}:\\n{result.stdout}\")\n",
        "        if result.stderr:\n",
        "             logger.debug(f\"Unrar stderr for {file_path}:\\n{result.stderr}\")\n",
        "        return True # Indicate success\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error extracting {file_path}: {e.stderr}\")\n",
        "        return False # Indicate failure\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"Unrar command not found. Please ensure 'unrar' is installed.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during extraction of {file_path}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Sanitize a filename.\"\"\"\n",
        "    sanitized = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)\n",
        "    logger.debug(f\"Sanitized filename '{filename}' to '{sanitized}'\")\n",
        "    return sanitized"
      ],
      "metadata": {
        "id": "file-ops-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. PDF Processing (Nougat)"
      ],
      "metadata": {
        "id": "markdown-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import logging\n",
        "# Assuming sanitize_filename is defined in file-ops-cell and available\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def process_pdf(pdf_path, output_dir):\n",
        "    \"\"\"Process a single PDF file with Nougat.\"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        logger.error(f\"PDF file not found for processing: {pdf_path}\")\n",
        "        return None\n",
        "\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(pdf_path))\n",
        "    mmd_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}.mmd\")\n",
        "\n",
        "    if os.path.exists(mmd_path):\n",
        "        logger.info(f\"{mmd_path} already exists. Skipping Nougat processing for {pdf_path}.\")\n",
        "        return mmd_path\n",
        "\n",
        "    logger.info(f\"Attempting to process PDF: {pdf_path} with Nougat. Output to {output_dir}\")\n",
        "    try:\n",
        "        # Using --no-skipping and --recompute for thorough processing\n",
        "        result = subprocess.run(\n",
        "            ['nougat', pdf_path, '-o', output_dir, '--no-skipping', '--recompute'],\n",
        "            check=True,\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        logger.info(f\"Successfully processed {pdf_path} with Nougat.\")\n",
        "        logger.debug(f\"Nougat stdout for {pdf_path}:\\n{result.stdout}\")\n",
        "        if result.stderr:\n",
        "            logger.debug(f\"Nougat stderr for {pdf_path}:\\n{result.stderr}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error processing {pdf_path} with Nougat: {e.stderr}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred during Nougat processing of {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Check if the expected output file was actually created\n",
        "    if not os.path.exists(mmd_path):\n",
        "         logger.error(f\"Nougat command finished without error, but expected output file {mmd_path} was not created.\")\n",
        "         return None\n",
        "\n",
        "    return mmd_path\n",
        "\n",
        "# Ensure sanitize_filename is available if it's not in this cell\n",
        "# from .file_ops import sanitize_filename # Example if in a different file"
      ],
      "metadata": {
        "id": "nougat-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Text Cleaning and Quality Control"
      ],
      "metadata": {
        "id": "markdown-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import logging\n",
        "from langdetect import detect, LangDetectException\n",
        "from nltk.corpus import words, brown # Assuming these are downloaded in config-cell\n",
        "from nltk.tokenize import word_tokenize # Assuming this is downloaded in config-cell\n",
        "from textblob import TextBlob # Assuming TextBlob is installed\n",
        "\n",
        "# Assuming GARBAGE_THRESHOLD and LENWORD are defined in config-cell\n",
        "# from .config import GARBAGE_THRESHOLD, LENWORD # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Load the NLTK words corpus for garbage detection\n",
        "# Ensure this is done after nltk.download('words') in config-cell\n",
        "try:\n",
        "    ENGLISH_WORDS = set(words.words())\n",
        "    logger.info(\"NLTK English words corpus loaded.\")\n",
        "except LookupError:\n",
        "    logger.error(\"NLTK 'words' corpus not found. Please run nltk.download('words').\")\n",
        "    ENGLISH_WORDS = set() # Use an empty set to avoid errors\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean the extracted text.\"\"\"\n",
        "    logger.debug(f\"Cleaning text (first 100 chars): {text[:100]}...\")\n",
        "    initial_len = len(text)\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = text.strip()\n",
        "    # Remove academic citations, references to tables/figures, and bracketed content.\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE)\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    # Sanitize garbled punctuation and symbols.\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text)\n",
        "    logger.debug(f\"Cleaned text (first 100 chars, original len {initial_len}): {text[:100]}...\")\n",
        "    return text\n",
        "\n",
        "def calculate_text_quality_score(text):\n",
        "    \"\"\"Calculate a quality score based on English words and sentence structure.\"\"\"\n",
        "    if not text:\n",
        "        return 0.0\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    if not words:\n",
        "        return 0.0\n",
        "\n",
        "    english_word_count = sum(1 for word in words if word.lower() in ENGLISH_WORDS)\n",
        "    english_word_ratio = english_word_count / len(words) if words else 0\n",
        "\n",
        "    # Simple heuristic for sentence structure (check for punctuation at end of sentences)\n",
        "    sentences = sent_tokenize(text)\n",
        "    well_formed_sentences = sum(1 for sent in sentences if sent.strip().endswith(('.', '!', '?')))\n",
        "    sentence_structure_score = well_formed_sentences / len(sentences) if sentences else 0\n",
        "\n",
        "    # Combine ratios - adjust weights as needed\n",
        "    quality_score = (english_word_ratio * 0.7) + (sentence_structure_score * 0.3)\n",
        "\n",
        "    logger.debug(f\"Text quality score calculated: {quality_score} for text (first 50 chars): {text[:50]}...\")\n",
        "    return quality_score\n",
        "\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, lenword=LENWORD):\n",
        "    \"\"\"Check if the text is garbage based on various heuristics.\"\"\"\n",
        "    logger.debug(f\"Checking if text is garbage (first 100 chars): {text[:100]}...\")\n",
        "\n",
        "    # Check for minimal length\n",
        "    if not text or len(text.split()) < 5: # Reduced minimum words\n",
        "        logger.debug(\"Identified as garbage: text too short or empty.\")\n",
        "        return True\n",
        "\n",
        "    # Language detection\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            logger.debug(\"Identified as garbage: language not English.\")\n",
        "            return True\n",
        "    except LangDetectException as e:\n",
        "        logger.debug(f\"Language detection failed for text (first 50 chars): {text[:50]}... Error: {e}. Assuming garbage.\")\n",
        "        return True # Assume garbage if language detection fails\n",
        "\n",
        "    # Check for jammed words (long strings without spaces)\n",
        "    words_list = text.split()\n",
        "    for word in words_list:\n",
        "        if len(word) > lenword and not '-' in word: # Allow hyphens in long words\n",
        "             logger.debug(f\"Identified as garbage: found jammed word '{word[:50]}...'\")\n",
        "             return True\n",
        "\n",
        "    # Check for unusually high proportion of single-letter words\n",
        "    single_letter_words = sum(1 for word in words_list if len(word) == 1)\n",
        "    if len(words_list) > 0 and single_letter_words / len(words_list) > 0.2: # More than 20% single letters\n",
        "        logger.debug(\"Identified as garbage: high proportion of single-letter words.\")\n",
        "        return True\n",
        "\n",
        "    # Check for repetitive patterns (simple heuristic)\n",
        "    if re.search(r'(.)\\1{4,}', text): # 5 or more of the same character in a row\n",
        "        logger.debug(\"Identified as garbage: found repetitive character pattern.\")\n",
        "        return True\n",
        "    if re.search(r'(\\w+\\s+)\\1{2,}', text): # A word repeated 3 or more times\n",
        "         logger.debug(\"Identified as garbage: found repetitive word pattern.\")\n",
        "         return True\n",
        "\n",
        "\n",
        "    # Quality scoring\n",
        "    quality_score = calculate_text_quality_score(text)\n",
        "    if quality_score < threshold:\n",
        "        logger.debug(f\"Identified as garbage: quality score {quality_score} below threshold {threshold}.\")\n",
        "        return True\n",
        "\n",
        "    logger.debug(\"Text passed garbage checks.\")\n",
        "    return False\n",
        "\n",
        "# Ensure GARBAGE_THRESHOLD and LENWORD are available if not in this cell\n",
        "# GARBAGE_THRESHOLD = 0.8 # Example default\n",
        "# LENWORD = 50 # Example default"
      ],
      "metadata": {
        "id": "cleaning-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4. Text Chunking"
      ],
      "metadata": {
        "id": "markdown-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize # Assuming this is downloaded in config-cell\n",
        "\n",
        "# Assuming clean_text and is_garbage are defined in cleaning-cell and available\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def chunk_text(content, max_size=8192):\n",
        "    \"\"\"Chunk the text into smaller segments, respecting markdown headings.\"\"\"\n",
        "    logger.debug(f\"Starting chunking process with max_size={max_size}.\")\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    lines = content.split('\\n')\n",
        "    logger.debug(f\"Splitting content into {len(lines)} lines.\")\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        # Check for markdown headings\n",
        "        if line.strip().startswith((\"# \", \"## \", \"### \")):\n",
        "            logger.debug(f\"Found markdown heading at line {i}: {line.strip()}\")\n",
        "            # If the current segment is not empty, process it before starting a new one\n",
        "            if current_segment:\n",
        "                logger.debug(f\"Processing previous segment before heading (length: {len(current_segment)}).\")\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            # Start a new segment with the heading line\n",
        "            current_segment = line + \"\\n\" # Keep the heading line in the new segment\n",
        "            logger.debug(\"Starting new segment after heading.\")\n",
        "        else:\n",
        "            # Add non-heading lines to the current segment\n",
        "            current_segment += line + \"\\n\"\n",
        "\n",
        "    # Process any remaining content in the last segment\n",
        "    if current_segment:\n",
        "        logger.debug(f\"Processing final segment (length: {len(current_segment)}).\")\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    logger.info(f\"Chunking complete. Produced {len(segments)} initial segments based on headings.\")\n",
        "    return segments\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    \"\"\"Split a segment (potentially from a heading section) into smaller chunks by sentences.\"\"\"\n",
        "    logger.debug(f\"Splitting segment by sentences (length: {len(segment)}).\")\n",
        "    sentences = sent_tokenize(segment)\n",
        "    logger.debug(f\"Segment split into {len(sentences)} sentences.\")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Add a space before adding the new sentence if the current chunk is not empty\n",
        "        sentence_to_add = sentence + \" \" if current_chunk else sentence\n",
        "        # Check if adding the current sentence exceeds the max size\n",
        "        if len(current_chunk) + len(sentence_to_add) <= max_size:\n",
        "            current_chunk += sentence_to_add\n",
        "            logger.debug(f\"Added sentence {i+1}/{len(sentences)} to current chunk (current size: {len(current_chunk)}).\")\n",
        "        else:\n",
        "            # If adding the sentence exceeds max size, add the current chunk to chunks list\n",
        "            if current_chunk: # Add the chunk only if it's not empty\n",
        "                chunks.append(current_chunk.strip())\n",
        "                logger.debug(f\"Chunk completed (size: {len(current_chunk)}). Starting new chunk with sentence {i+1}.\")\n",
        "            # Start a new chunk with the current sentence\n",
        "            current_chunk = sentence + \" \" # Start new chunk with the current sentence\n",
        "\n",
        "    # Add the last current chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "        logger.debug(f\"Added final chunk (size: {len(current_chunk)}).\")\n",
        "\n",
        "    logger.debug(f\"Segment split into {len(chunks)} smaller chunks.\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def process_and_chunk_mmd(mmd_path, output_dir):\n",
        "    \"\"\"Process, clean, chunk, and categorize text from an MMD file.\"\"\"\n",
        "    logger.info(f\"Starting processing and chunking for MMD file: {mmd_path}\")\n",
        "\n",
        "    if not mmd_path or not os.path.exists(mmd_path):\n",
        "        logger.warning(f\"MMD file not found or path is invalid: {mmd_path}. Skipping processing and chunking.\")\n",
        "        return None, None\n",
        "\n",
        "    sanitized_filename = sanitize_filename(os.path.basename(mmd_path))\n",
        "    cleaned_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_cleaned.jsonl\")\n",
        "    garbage_jsonl_path = os.path.join(output_dir, f\"{os.path.splitext(sanitized_filename)[0]}_garbage.jsonl\")\n",
        "\n",
        "    if os.path.exists(cleaned_jsonl_path) and os.path.exists(garbage_jsonl_path):\n",
        "        logger.info(f\"Output files {cleaned_jsonl_path} and {garbage_jsonl_path} already exist. Skipping processing and chunking for {mmd_path}.\")\n",
        "        return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    try:\n",
        "        with open(mmd_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        logger.debug(f\"Successfully read content from {mmd_path} (length: {len(content)}).\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading MMD file {mmd_path}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    chunks = chunk_text(content)\n",
        "    logger.info(f\"MMD content chunked into {len(chunks)} segments.\")\n",
        "\n",
        "    cleaned_count = 0\n",
        "    garbage_count = 0\n",
        "\n",
        "    try:\n",
        "        with open(cleaned_jsonl_path, 'w', encoding='utf-8') as cleaned_f, \\\n",
        "             open(garbage_jsonl_path, 'w', encoding='utf-8') as garbage_f:\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                logger.debug(f\"Processing chunk {i+1}/{len(chunks)} (length: {len(chunk)}).\")\n",
        "                cleaned_chunk = clean_text(chunk)\n",
        "                if is_garbage(cleaned_chunk):\n",
        "                    garbage_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    garbage_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as garbage.\")\n",
        "                else:\n",
        "                    cleaned_f.write(json.dumps({\"text\": cleaned_chunk}) + '\\n')\n",
        "                    cleaned_count += 1\n",
        "                    logger.debug(f\"Chunk {i+1} identified as cleaned text.\")\n",
        "\n",
        "        logger.info(f\"Finished processing and chunking {mmd_path}. Generated {cleaned_count} cleaned chunks and {garbage_count} garbage chunks.\")\n",
        "        return cleaned_jsonl_path, garbage_jsonl_path\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during cleaning or writing chunk files for {mmd_path}: {e}\")\n",
        "        # Clean up potentially incomplete files\n",
        "        if os.path.exists(cleaned_jsonl_path):\n",
        "            os.remove(cleaned_jsonl_path)\n",
        "        if os.path.exists(garbage_jsonl_path):\n",
        "            os.remove(garbage_jsonl_path)\n",
        "        return None, None\n",
        "\n",
        "# Ensure sanitize_filename, clean_text, is_garbage are available\n",
        "# from .file_ops import sanitize_filename # Example if in a different file\n",
        "# from .cleaning import clean_text, is_garbage # Example if in a different file"
      ],
      "metadata": {
        "id": "chunking-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5. Hugging Face Integration"
      ],
      "metadata": {
        "id": "markdown-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "from huggingface_hub import HfApi, Repository # Import Repository for better practice if needed for cloning/managing\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def upload_to_huggingface(file_path, repo_id, repo_type=\"dataset\"):\n",
        "    \"\"\"Upload a file to a Hugging Face repository.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        logger.error(f\"File not found for upload to Hugging Face: {file_path}\")\n",
        "        return False # Indicate failure\n",
        "\n",
        "    logger.info(f\"Attempting to upload {file_path} to Hugging Face repo '{repo_id}' (type: {repo_type}).\")\n",
        "    api = HfApi()\n",
        "    try:\n",
        "        # Use create_commit for potentially better handling of multiple files or larger uploads\n",
        "        # This example uses upload_file for simplicity as in the original code\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=os.path.basename(file_path),\n",
        "            repo_id=repo_id,\n",
        "            repo_type=repo_type,\n",
        "            # Optional: add commit_message, token if not using environment variable\n",
        "        )\n",
        "        logger.info(f\"Successfully uploaded {file_path} to {repo_id}\")\n",
        "        return True # Indicate success\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error uploading {file_path} to Hugging Face repo '{repo_id}': {e}\")\n",
        "        return False # Indicate failure"
      ],
      "metadata": {
        "id": "huggingface-cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Main Processing Loop"
      ],
      "metadata": {
        "id": "markdown-10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # Import ThreadPoolExecutor\n",
        "# Assuming helper functions are defined in other cells and available\n",
        "# from .file_ops import get_file_list, download_file, extract_rar, sanitize_filename\n",
        "# from .nougat_processing import process_pdf\n",
        "# from .chunking import process_and_chunk_mmd\n",
        "# from .huggingface_integration import upload_to_huggingface\n",
        "# from .config import BASE_URL, HUGGING_FACE_REPO # Assuming these are defined in config-cell\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define a cache file path\n",
        "RAR_LIST_CACHE = \"rar_list_cache.json\"\n",
        "\n",
        "def process_single_rar(rar_file_url, HUGGING_FACE_REPO):\n",
        "    \"\"\"Processes a single RAR file: downloads, extracts, processes PDFs, and uploads.\"\"\"\n",
        "    rar_filename = rar_file_url.split('/')[-1]\n",
        "    sanitized_rar_filename = sanitize_filename(rar_filename)\n",
        "    rar_path = sanitized_rar_filename\n",
        "    extract_path = os.path.splitext(rar_path)[0]\n",
        "\n",
        "    logger.info(f\"--- Processing {rar_filename} ---\")\n",
        "\n",
        "    # 1. Download RAR file\n",
        "    # The download_file function now correctly returns True on success\n",
        "    download_file(rar_file_url, rar_path)\n",
        "\n",
        "    # 2. Extract RAR file\n",
        "    extract_rar(rar_path, extract_path)\n",
        "    # Clean up the downloaded RAR even if extraction failed\n",
        "    if os.path.exists(rar_path):\n",
        "        os.remove(rar_path)\n",
        "        logger.debug(f\"Removed partially downloaded/failed RAR file: {rar_path}\")\n",
        "\n",
        "    # 3. Find and Process PDF files within the extracted directory\n",
        "    pdf_files = [os.path.join(root, file) for root, _, files in os.walk(extract_path) for file in files if file.lower().endswith('.pdf')]\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files in extracted directory: {extract_path}\")\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.warning(f\"No PDF files found in {extract_path}. Cleaning up.\")\n",
        "        # Clean up the downloaded RAR and extracted directory\n",
        "        if os.path.exists(rar_path):\n",
        "            os.remove(rar_path)\n",
        "            logger.debug(f\"Removed downloaded RAR file: {rar_path}\")\n",
        "        if os.path.exists(extract_path):\n",
        "            shutil.rmtree(extract_path)\n",
        "            logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "        return 0 # Indicate no PDFs processed\n",
        "\n",
        "    successful_uploads_count = 0\n",
        "    with tqdm(total=len(pdf_files), desc=f\"Processing PDFs in {sanitized_rar_filename}\", leave=False) as pbar_pdfs:\n",
        "        for pdf_path in pdf_files:\n",
        "            logger.info(f\"Processing PDF: {pdf_path}\")\n",
        "\n",
        "            # 4. Process PDF with Nougat\n",
        "            mmd_path = process_pdf(pdf_path, extract_path)\n",
        "\n",
        "            if mmd_path:\n",
        "                logger.info(f\"Nougat processing successful for {pdf_path}. MMD file: {mmd_path}\")\n",
        "                # 5. Clean and Chunk MMD file\n",
        "                cleaned_jsonl, garbage_jsonl = process_and_chunk_mmd(mmd_path, extract_path)\n",
        "\n",
        "                # 6. Upload to Hugging Face\n",
        "                if cleaned_jsonl and os.path.exists(cleaned_jsonl):\n",
        "                    logger.info(f\"Uploading cleaned data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    if upload_to_huggingface(cleaned_jsonl, HUGGING_FACE_REPO):\n",
        "                        successful_uploads_count += 1\n",
        "                    else:\n",
        "                        logger.error(f\"Failed to upload cleaned data for {os.path.basename(pdf_path)}.\")\n",
        "                    # Optionally upload garbage data\n",
        "                    ##if garbage_jsonl and os.path.exists(garbage_jsonl):\n",
        "                    ##    logger.info(f\"Uploading garbage data for {os.path.basename(pdf_path)} to Hugging Face.\")\n",
        "                    ##    upload_to_huggingface(garbage_jsonl, HUGGING_FACE_REPO)\n",
        "                else:\n",
        "                    logger.warning(f\"No cleaned data generated for {os.path.basename(pdf_path)}. Skipping upload.\")\n",
        "            else:\n",
        "                logger.error(f\"Nougat processing failed for {pdf_path}. Skipping cleaning, chunking, and upload.\")\n",
        "\n",
        "            pbar_pdfs.update(1) # Update inner progress bar for each PDF\n",
        "\n",
        "    # 7. Clean up downloaded RAR and extracted directory after processing all PDFs in the RAR\n",
        "    logger.info(f\"Cleaning up downloaded RAR and extracted directory for {rar_filename}.\")\n",
        "    if os.path.exists(rar_path):\n",
        "        os.remove(rar_path)\n",
        "        logger.debug(f\"Removed downloaded RAR file: {rar_path}\")\n",
        "    if os.path.exists(extract_path):\n",
        "        shutil.rmtree(extract_path)\n",
        "        logger.debug(f\"Removed extracted directory: {extract_path}\")\n",
        "\n",
        "    return successful_uploads_count\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process the library.\"\"\"\n",
        "    logger.info(\"--- Starting Library Processing Pipeline ---\")\n",
        "\n",
        "    rar_files = []\n",
        "    if os.path.exists(RAR_LIST_CACHE):\n",
        "        logger.info(f\"Loading RAR file list from cache: {RAR_LIST_CACHE}\")\n",
        "        try:\n",
        "            with open(RAR_LIST_CACHE, 'r') as f:\n",
        "                rar_files = json.load(f)\n",
        "            logger.info(f\"Loaded {len(rar_files)} RAR files from cache.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading RAR file list from cache: {e}. Rescanning.\")\n",
        "            # If loading fails, proceed to rescan\n",
        "            rar_files = []\n",
        "\n",
        "    if not rar_files:\n",
        "        logger.info(f\"Scanning for RAR files at {BASE_URL}\")\n",
        "        try:\n",
        "            rar_files = get_file_list(BASE_URL)\n",
        "            logger.info(f\"Found {len(rar_files)} RAR files.\")\n",
        "            # Save the list to cache\n",
        "            try:\n",
        "                with open(RAR_LIST_CACHE, 'w') as f:\n",
        "                    json.dump(rar_files, f)\n",
        "                logger.info(f\"Saved RAR file list to cache: {RAR_LIST_CACHE}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error saving RAR file list to cache: {e}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to get RAR file list from {BASE_URL}: {e}\")\n",
        "            return # Exit if initial scan fails\n",
        "\n",
        "    total_successful_uploads = 0\n",
        "\n",
        "    # Use ThreadPoolExecutor to process RAR files in parallel\n",
        "    # Adjust max_workers based on your runtime's capabilities and the task\n",
        "    max_workers = 4 # Example: process 4 RARs at a time\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit tasks to the executor\n",
        "        future_to_rar = {executor.submit(process_single_rar, rar_file_url, HUGGING_FACE_REPO): rar_file_url for rar_file_url in rar_files}\n",
        "\n",
        "        # Use tqdm to track overall progress\n",
        "        with tqdm(total=len(rar_files), desc=\"Overall RAR Processing\") as pbar_overall:\n",
        "            for future in as_completed(future_to_rar):\n",
        "                rar_file_url = future_to_rar[future]\n",
        "                try:\n",
        "                    successful_uploads_count = future.result()\n",
        "                    total_successful_uploads += successful_uploads_count\n",
        "                except Exception as exc:\n",
        "                    logger.error(f'{rar_file_url} generated an exception: {exc}')\n",
        "\n",
        "                pbar_overall.update(1) # Update outer progress bar for each completed RAR\n",
        "\n",
        "    logger.info(\"--- Library Processing Pipeline Finished ---\")\n",
        "    logger.info(f\"Successfully uploaded cleaned data for {total_successful_uploads} PDF files to {HUGGING_FACE_REPO}.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "main-loop-cell",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6066b1-c6d2-4fa9-a897-062c851b7fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,116 - INFO - --- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Starting Library Processing Pipeline ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,133 - INFO - Loading RAR file list from cache: rar_list_cache.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Loading RAR file list from cache: rar_list_cache.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,156 - INFO - Loaded 4824 RAR files from cache.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Loaded 4824 RAR files from cache.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,168 - INFO - --- Processing 1.%20Prehistory.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing 1.%20Prehistory.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,186 - INFO - --- Processing Alexander%20the%20Great.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Alexander%20the%20Great.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,190 - INFO - --- Processing Ancient%20Africa.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Ancient%20Africa.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,201 - INFO - --- Processing Ancient%20Britain.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Ancient%20Britain.rar ---\n",
            "Overall RAR Processing:   0%|          | 0/4824 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:38:57,937 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/The%20Etruscans.rar to The_20Etruscans.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/The%20Etruscans.rar to The_20Etruscans.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:39:17,626 - INFO - Extracted The_20Etruscans.rar to The_20Etruscans\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Extracted The_20Etruscans.rar to The_20Etruscans\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:39:17,630 - ERROR - Skipping PDF processing for The%20Etruscans.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for The%20Etruscans.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:39:17,760 - INFO - --- Processing World%20Literature%20%26%20Myths.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing World%20Literature%20%26%20Myths.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:39:17,788 - INFO - World_20Literature_20_26_20Myths.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:World_20Literature_20_26_20Myths.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:39:39,679 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Ancient%20Africa.rar to Ancient_20Africa.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Ancient%20Africa.rar to Ancient_20Africa.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:40:01,647 - INFO - Extracted Ancient_20Africa.rar to Ancient_20Africa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Extracted Ancient_20Africa.rar to Ancient_20Africa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:40:01,764 - INFO - Found 14 PDF files in extracted directory: Ancient_20Africa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Found 14 PDF files in extracted directory: Ancient_20Africa\n",
            "\n",
            "Processing PDFs in Ancient_20Africa.rar:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:40:01,830 - INFO - Processing PDF: Ancient_20Africa/Ancient Africa/Anna Leone - The End of the Pagan City. Religion, Economy, and Urbanism in Late Antique North Africa.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: Ancient_20Africa/Ancient Africa/Anna Leone - The End of the Pagan City. Religion, Economy, and Urbanism in Late Antique North Africa.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:40:16,030 - ERROR - Error extracting World_20Literature_20_26_20Myths.rar: \n",
            "Unexpected end of archive\n",
            "World Literature & Myths/Jennifer R. March - Dictionary of Classical Mythology [Retail].pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting World_20Literature_20_26_20Myths.rar: \n",
            "Unexpected end of archive\n",
            "World Literature & Myths/Jennifer R. March - Dictionary of Classical Mythology [Retail].pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:40:16,032 - ERROR - Skipping PDF processing for World%20Literature%20%26%20Myths.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for World%20Literature%20%26%20Myths.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:40:16,035 - INFO - --- Processing Church%20and%20Theology%20in%20Middle%20Ages.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Church%20and%20Theology%20in%20Middle%20Ages.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:40:16,043 - INFO - Church_20and_20Theology_20in_20Middle_20Ages.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Church_20and_20Theology_20in_20Middle_20Ages.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:19,667 - ERROR - Error extracting Church_20and_20Theology_20in_20Middle_20Ages.rar: \n",
            "Unexpected end of archive\n",
            "Church and Theology in Middle Ages/F. Donald Logan - A History of the Church in the Middle Ages (2nd Edition) [Retail] (2).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Church_20and_20Theology_20in_20Middle_20Ages.rar: \n",
            "Unexpected end of archive\n",
            "Church and Theology in Middle Ages/F. Donald Logan - A History of the Church in the Middle Ages (2nd Edition) [Retail] (2).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:19,671 - ERROR - Skipping PDF processing for Church%20and%20Theology%20in%20Middle%20Ages.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Church%20and%20Theology%20in%20Middle%20Ages.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:19,674 - INFO - --- Processing Crusades.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Crusades.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:19,676 - INFO - Crusades.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Crusades.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:29,475 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Roman%20Empire%20%26%20History.rar: ('Connection broken: IncompleteRead(1719676591 bytes read, 18009109827 more expected)', IncompleteRead(1719676591 bytes read, 18009109827 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Roman%20Empire%20%26%20History.rar: ('Connection broken: IncompleteRead(1719676591 bytes read, 18009109827 more expected)', IncompleteRead(1719676591 bytes read, 18009109827 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:33,418 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/1.%20Prehistory/1.%20Prehistory.rar: ('Connection broken: IncompleteRead(981507759 bytes read, 19728188992 more expected)', IncompleteRead(981507759 bytes read, 19728188992 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/1.%20Prehistory/1.%20Prehistory.rar: ('Connection broken: IncompleteRead(981507759 bytes read, 19728188992 more expected)', IncompleteRead(981507759 bytes read, 19728188992 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:33,503 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Ancient%20Britain.rar: ('Connection broken: IncompleteRead(939564721 bytes read, 1686823080 more expected)', IncompleteRead(939564721 bytes read, 1686823080 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Ancient%20Britain.rar: ('Connection broken: IncompleteRead(939564721 bytes read, 1686823080 more expected)', IncompleteRead(939564721 bytes read, 1686823080 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:33,916 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/World%20Literature%20%26%20Myths.rar: ('Connection broken: IncompleteRead(1568767665 bytes read, 1559578686 more expected)', IncompleteRead(1568767665 bytes read, 1559578686 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/World%20Literature%20%26%20Myths.rar: ('Connection broken: IncompleteRead(1568767665 bytes read, 1559578686 more expected)', IncompleteRead(1568767665 bytes read, 1559578686 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:33,939 - ERROR - Skipping extraction and processing for World%20Literature%20%26%20Myths.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for World%20Literature%20%26%20Myths.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:33,962 - INFO - --- Processing Hanseatic%20League%20%26%20East%20India%20Company.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Hanseatic%20League%20%26%20East%20India%20Company.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:33,994 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Alexander%20the%20Great.rar: ('Connection broken: IncompleteRead(981507761 bytes read, 166399684 more expected)', IncompleteRead(981507761 bytes read, 166399684 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Alexander%20the%20Great.rar: ('Connection broken: IncompleteRead(981507761 bytes read, 166399684 more expected)', IncompleteRead(981507761 bytes read, 166399684 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:34,730 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Church%20and%20Theology%20in%20Middle%20Ages.rar: ('Connection broken: IncompleteRead(1275109041 bytes read, 2188715028 more expected)', IncompleteRead(1275109041 bytes read, 2188715028 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Church%20and%20Theology%20in%20Middle%20Ages.rar: ('Connection broken: IncompleteRead(1275109041 bytes read, 2188715028 more expected)', IncompleteRead(1275109041 bytes read, 2188715028 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:34,752 - ERROR - Skipping extraction and processing for Church%20and%20Theology%20in%20Middle%20Ages.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Church%20and%20Theology%20in%20Middle%20Ages.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:34,764 - INFO - --- Processing History%20of%20Ships.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing History%20of%20Ships.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:38,430 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Ancient%20Greece.rar: ('Connection broken: IncompleteRead(3888021167 bytes read, 13324521886 more expected)', IncompleteRead(3888021167 bytes read, 13324521886 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Ancient%20Greece.rar: ('Connection broken: IncompleteRead(3888021167 bytes read, 13324521886 more expected)', IncompleteRead(3888021167 bytes read, 13324521886 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:38,437 - ERROR - Skipping extraction and processing for Ancient%20Greece.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Ancient%20Greece.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:38,445 - INFO - --- Processing Knights%20%26%20Chivalry.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Knights%20%26%20Chivalry.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:58,229 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/History%20of%20Ships.rar: ('Connection broken: IncompleteRead(376497 bytes read, 3054386406 more expected)', IncompleteRead(376497 bytes read, 3054386406 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/History%20of%20Ships.rar: ('Connection broken: IncompleteRead(376497 bytes read, 3054386406 more expected)', IncompleteRead(376497 bytes read, 3054386406 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:58,253 - ERROR - Skipping extraction and processing for History%20of%20Ships.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for History%20of%20Ships.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:41:58,330 - INFO - --- Processing Medieval%20Architecture.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20Architecture.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:43,166 - ERROR - Error extracting 1._20Prehistory.rar: \n",
            "Unexpected end of archive\n",
            "1. Prehistory/Archaeology/Archaeological Studies/Brian M. Fagan, Nadia Durrani - In the Beginning. An Introduction to Archaeology (14th Edition) (Retail).epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting 1._20Prehistory.rar: \n",
            "Unexpected end of archive\n",
            "1. Prehistory/Archaeology/Archaeological Studies/Brian M. Fagan, Nadia Durrani - In the Beginning. An Introduction to Archaeology (14th Edition) (Retail).epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:43,400 - INFO - Found 31 PDF files in extracted directory: 1._20Prehistory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Found 31 PDF files in extracted directory: 1._20Prehistory\n",
            "\n",
            "\n",
            "Processing PDFs in 1._20Prehistory.rar:   0%|          | 0/31 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:43,427 - INFO - Processing PDF: 1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: 1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:43,537 - ERROR - Error extracting Ancient_20Britain.rar: \n",
            "Unexpected end of archive\n",
            "Ancient Britain/John Sheehan, Donnchadh Ó Corráin - The Viking Age. Ireland and the West (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Ancient_20Britain.rar: \n",
            "Unexpected end of archive\n",
            "Ancient Britain/John Sheehan, Donnchadh Ó Corráin - The Viking Age. Ireland and the West (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:43,755 - INFO - Found 38 PDF files in extracted directory: Ancient_20Britain\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Found 38 PDF files in extracted directory: Ancient_20Britain\n",
            "\n",
            "\n",
            "\n",
            "Processing PDFs in Ancient_20Britain.rar:   0%|          | 0/38 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:43,773 - INFO - Processing PDF: Ancient_20Britain/Ancient Britain/Francesca Kaminski-Jones, Rhys Kaminski-Jones - Celts, Romans, Britons. Classical and Celtic Influence in the Construction of British Identities (Classical Presences) (Retail).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: Ancient_20Britain/Ancient Britain/Francesca Kaminski-Jones, Rhys Kaminski-Jones - Celts, Romans, Britons. Classical and Celtic Influence in the Construction of British Identities (Classical Presences) (Retail).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:53,576 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Religion%2C%20History%20of%20Religion.rar: ('Connection broken: IncompleteRead(1896038065 bytes read, 2019845764 more expected)', IncompleteRead(1896038065 bytes read, 2019845764 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Religion%2C%20History%20of%20Religion.rar: ('Connection broken: IncompleteRead(1896038065 bytes read, 2019845764 more expected)', IncompleteRead(1896038065 bytes read, 2019845764 more expected))\n",
            "Overall RAR Processing:   0%|          | 0/4824 [03:56<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,036 - ERROR - Error extracting Alexander_20the_20Great.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ERROR:__main__:Error extracting Alexander_20the_20Great.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,047 - ERROR - Error extracting Religion_2C_20History_20of_20Religion.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Religion_2C_20History_20of_20Religion.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,053 - ERROR - Skipping PDF processing for Religion%2C%20History%20of%20Religion.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Religion%2C%20History%20of%20Religion.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,175 - ERROR - Error extracting Roman_20Empire_20_26_20History.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Roman_20Empire_20_26_20History.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,187 - ERROR - Skipping PDF processing for Roman%20Empire%20%26%20History.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Roman%20Empire%20%26%20History.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,316 - INFO - --- Processing Hanseatic%20League%20%26%20East%20India%20Company.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Hanseatic%20League%20%26%20East%20India%20Company.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,323 - INFO - Hanseatic_20League_20_26_20East_20India_20Company.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Hanseatic_20League_20_26_20East_20India_20Company.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,337 - ERROR - Error extracting Crusades.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Crusades.rar: \n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,343 - ERROR - Skipping PDF processing for Crusades.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Crusades.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,349 - INFO - --- Processing History%20of%20Ships.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing History%20of%20Ships.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,361 - INFO - History_20of_20Ships.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:History_20of_20Ships.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,362 - INFO - Found 44 PDF files in extracted directory: Alexander_20the_20Great\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Found 44 PDF files in extracted directory: Alexander_20the_20Great\n",
            "Processing PDFs in Alexander_20the_20Great.rar:   0%|          | 0/44 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,400 - INFO - Processing PDF: Alexander_20the_20Great/Alexander the Great/Bob Bennett, Mike Roberts - The Wars of Alexander's Successors, 323 – 281 BC. Volume 2 Battles and Tactics (Retail).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: Alexander_20the_20Great/Alexander the Great/Bob Bennett, Mike Roberts - The Wars of Alexander's Successors, 323 – 281 BC. Volume 2 Battles and Tactics (Retail).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,433 - ERROR - Error extracting History_20of_20Ships.rar: \n",
            "Unexpected end of archive\n",
            "History of Ships/A. J. Hoving - Nicolaes Witsen and Shipbuilding in the Dutch Golden Age.pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting History_20of_20Ships.rar: \n",
            "Unexpected end of archive\n",
            "History of Ships/A. J. Hoving - Nicolaes Witsen and Shipbuilding in the Dutch Golden Age.pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,443 - ERROR - Skipping PDF processing for History%20of%20Ships.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for History%20of%20Ships.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,453 - INFO - --- Processing Knights%20%26%20Chivalry.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Knights%20%26%20Chivalry.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,466 - INFO - Knights_20_26_20Chivalry.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Knights_20_26_20Chivalry.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,529 - INFO - --- Processing Medieval%20Architecture.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20Architecture.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:54,537 - INFO - Medieval_20Architecture.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Medieval_20Architecture.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:55,319 - ERROR - Error processing 1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf with Nougat: Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 5, in <module>\n",
            "    from predict import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/predict.py\", line 15, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2253, in <module>\n",
            "    from torch import masked as masked\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/__init__.py\", line 1, in <module>\n",
            "    from torch.masked._ops import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/_ops.py\", line 10, in <module>\n",
            "    from torch.masked.maskedtensor.core import is_masked_tensor, MaskedTensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/__init__.py\", line 4, in <module>\n",
            "    from .binary import _apply_native_binary, _is_native_binary\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 179, in <module>\n",
            "    NATIVE_BINARY_MAP = {\n",
            "                        ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 180, in <dictcomp>\n",
            "    getattr(torch.ops.aten, name): _torch_binary(name) for name in BINARY_NAMES\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1243, in __getattr__\n",
            "    opoverloadpacket = OpOverloadPacket(\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1024, in __init__\n",
            "    self._has_torchbind_op_overload = any(\n",
            "                                      ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1025, in <genexpr>\n",
            "    _has_script_object_arg(schema) for schema in self._schemas.values()\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1010, in _has_script_object_arg\n",
            "    return any(isinstance(arg.type, torch.ClassType) for arg in schema.arguments)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1010, in <genexpr>\n",
            "    return any(isinstance(arg.type, torch.ClassType) for arg in schema.arguments)\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error processing 1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf with Nougat: Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 5, in <module>\n",
            "    from predict import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/predict.py\", line 15, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2253, in <module>\n",
            "    from torch import masked as masked\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/__init__.py\", line 1, in <module>\n",
            "    from torch.masked._ops import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/_ops.py\", line 10, in <module>\n",
            "    from torch.masked.maskedtensor.core import is_masked_tensor, MaskedTensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/__init__.py\", line 4, in <module>\n",
            "    from .binary import _apply_native_binary, _is_native_binary\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 179, in <module>\n",
            "    NATIVE_BINARY_MAP = {\n",
            "                        ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 180, in <dictcomp>\n",
            "    getattr(torch.ops.aten, name): _torch_binary(name) for name in BINARY_NAMES\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1243, in __getattr__\n",
            "    opoverloadpacket = OpOverloadPacket(\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1024, in __init__\n",
            "    self._has_torchbind_op_overload = any(\n",
            "                                      ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1025, in <genexpr>\n",
            "    _has_script_object_arg(schema) for schema in self._schemas.values()\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1010, in _has_script_object_arg\n",
            "    return any(isinstance(arg.type, torch.ClassType) for arg in schema.arguments)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1010, in <genexpr>\n",
            "    return any(isinstance(arg.type, torch.ClassType) for arg in schema.arguments)\n",
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:55,321 - ERROR - Nougat processing failed for 1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf. Skipping cleaning, chunking, and upload.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Nougat processing failed for 1._20Prehistory/1. Prehistory/America/Michelle Hayward, Lesley-Gail Atkinson, Michael A. Cinquino - Rock Art of the Caribbean (Caribbean Archaeology and Ethnohistory) [Retail].pdf. Skipping cleaning, chunking, and upload.\n",
            "\n",
            "\n",
            "Processing PDFs in 1._20Prehistory.rar:   3%|▎         | 1/31 [00:11<05:57, 11.91s/it]\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:55,343 - INFO - Processing PDF: 1._20Prehistory/1. Prehistory/America/Bradley J. Vierra - The Late Archaic across the Borderlands From Foraging to Farming.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: 1._20Prehistory/1. Prehistory/America/Bradley J. Vierra - The Late Archaic across the Borderlands From Foraging to Farming.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:55,643 - ERROR - Error processing Ancient_20Britain/Ancient Britain/Francesca Kaminski-Jones, Rhys Kaminski-Jones - Celts, Romans, Britons. Classical and Celtic Influence in the Construction of British Identities (Classical Presences) (Retail).pdf with Nougat: Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 5, in <module>\n",
            "    from predict import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/predict.py\", line 15, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2253, in <module>\n",
            "    from torch import masked as masked\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/__init__.py\", line 1, in <module>\n",
            "    from torch.masked._ops import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/_ops.py\", line 10, in <module>\n",
            "    from torch.masked.maskedtensor.core import is_masked_tensor, MaskedTensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/__init__.py\", line 4, in <module>\n",
            "    from .binary import _apply_native_binary, _is_native_binary\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 179, in <module>\n",
            "    NATIVE_BINARY_MAP = {\n",
            "                        ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 180, in <dictcomp>\n",
            "    getattr(torch.ops.aten, name): _torch_binary(name) for name in BINARY_NAMES\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1243, in __getattr__\n",
            "    opoverloadpacket = OpOverloadPacket(\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1024, in __init__\n",
            "    self._has_torchbind_op_overload = any(\n",
            "                                      ^^^^\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error processing Ancient_20Britain/Ancient Britain/Francesca Kaminski-Jones, Rhys Kaminski-Jones - Celts, Romans, Britons. Classical and Celtic Influence in the Construction of British Identities (Classical Presences) (Retail).pdf with Nougat: Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 5, in <module>\n",
            "    from predict import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/predict.py\", line 15, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2253, in <module>\n",
            "    from torch import masked as masked\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/__init__.py\", line 1, in <module>\n",
            "    from torch.masked._ops import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/_ops.py\", line 10, in <module>\n",
            "    from torch.masked.maskedtensor.core import is_masked_tensor, MaskedTensor\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/__init__.py\", line 4, in <module>\n",
            "    from .binary import _apply_native_binary, _is_native_binary\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 179, in <module>\n",
            "    NATIVE_BINARY_MAP = {\n",
            "                        ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/masked/maskedtensor/binary.py\", line 180, in <dictcomp>\n",
            "    getattr(torch.ops.aten, name): _torch_binary(name) for name in BINARY_NAMES\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1243, in __getattr__\n",
            "    opoverloadpacket = OpOverloadPacket(\n",
            "                       ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1024, in __init__\n",
            "    self._has_torchbind_op_overload = any(\n",
            "                                      ^^^^\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:55,645 - ERROR - Nougat processing failed for Ancient_20Britain/Ancient Britain/Francesca Kaminski-Jones, Rhys Kaminski-Jones - Celts, Romans, Britons. Classical and Celtic Influence in the Construction of British Identities (Classical Presences) (Retail).pdf. Skipping cleaning, chunking, and upload.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Nougat processing failed for Ancient_20Britain/Ancient Britain/Francesca Kaminski-Jones, Rhys Kaminski-Jones - Celts, Romans, Britons. Classical and Celtic Influence in the Construction of British Identities (Classical Presences) (Retail).pdf. Skipping cleaning, chunking, and upload.\n",
            "\n",
            "\n",
            "\n",
            "Processing PDFs in Ancient_20Britain.rar:   3%|▎         | 1/38 [00:11<07:19, 11.88s/it]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:42:55,677 - INFO - Processing PDF: Ancient_20Britain/Ancient Britain/Heather O'Donoghue - English Poetry and Old Norse Myth. A History [Retail].pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: Ancient_20Britain/Ancient Britain/Heather O'Donoghue - English Poetry and Old Norse Myth. A History [Retail].pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:02,973 - ERROR - Error extracting Knights_20_26_20Chivalry.rar: \n",
            "Unexpected end of archive\n",
            "Knights & Chivalry/Maurice Keen - Chivalry.pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Knights_20_26_20Chivalry.rar: \n",
            "Unexpected end of archive\n",
            "Knights & Chivalry/Maurice Keen - Chivalry.pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:02,979 - ERROR - Skipping PDF processing for Knights%20%26%20Chivalry.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Knights%20%26%20Chivalry.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:02,983 - INFO - --- Processing Medieval%20Literature.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20Literature.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:03,828 - ERROR - Error extracting Medieval_20Architecture.rar: \n",
            "Unexpected end of archive\n",
            "Medieval Architecture/Arleen Pabón-Charneco - Architecture History, Theory and Preservation Prehistory to the Middle Ages (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Medieval_20Architecture.rar: \n",
            "Unexpected end of archive\n",
            "Medieval Architecture/Arleen Pabón-Charneco - Architecture History, Theory and Preservation Prehistory to the Middle Ages (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:03,829 - ERROR - Skipping PDF processing for Medieval%20Architecture.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Medieval%20Architecture.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:03,831 - INFO - --- Processing Medieval%20People.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20People.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:06,285 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Miscellaneous.rar: ('Connection broken: IncompleteRead(1887477424 bytes read, 7791888393 more expected)', IncompleteRead(1887477424 bytes read, 7791888393 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/2.%20Ancient%20e%20Classical/Miscellaneous.rar: ('Connection broken: IncompleteRead(1887477424 bytes read, 7791888393 more expected)', IncompleteRead(1887477424 bytes read, 7791888393 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:06,452 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Crusades.rar: ('Connection broken: IncompleteRead(1333800625 bytes read, 2290090166 more expected)', IncompleteRead(1333800625 bytes read, 2290090166 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Crusades.rar: ('Connection broken: IncompleteRead(1333800625 bytes read, 2290090166 more expected)', IncompleteRead(1333800625 bytes read, 2290090166 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:06,466 - ERROR - Skipping extraction and processing for Crusades.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Crusades.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:06,496 - INFO - --- Processing Medieval%20Literature.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20Literature.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:09,379 - ERROR - Error processing Ancient_20Africa/Ancient Africa/Anna Leone - The End of the Pagan City. Religion, Economy, and Urbanism in Late Antique North Africa.pdf with Nougat: WARNING:root:No GPU found. Conversion on CPU is very slow.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/342 [00:00<?, ?it/s]\n",
            "  0%|          | 0/342 [01:47<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/predict.py\", line 167, in main\n",
            "    model_output = model.inference(\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nougat/model.py\", line 592, in inference\n",
            "    decoder_output = self.decoder.model.generate(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 1544, in generate\n",
            "    return self.greedy_search(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2404, in greedy_search\n",
            "    outputs = self(\n",
            "              ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\", line 2082, in forward\n",
            "    logits = self.lm_head(outputs[0])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error processing Ancient_20Africa/Ancient Africa/Anna Leone - The End of the Pagan City. Religion, Economy, and Urbanism in Late Antique North Africa.pdf with Nougat: WARNING:root:No GPU found. Conversion on CPU is very slow.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/342 [00:00<?, ?it/s]\n",
            "  0%|          | 0/342 [01:47<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/nougat\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/predict.py\", line 167, in main\n",
            "    model_output = model.inference(\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nougat/model.py\", line 592, in inference\n",
            "    decoder_output = self.decoder.model.generate(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 1544, in generate\n",
            "    return self.greedy_search(\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2404, in greedy_search\n",
            "    outputs = self(\n",
            "              ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\", line 2082, in forward\n",
            "    logits = self.lm_head(outputs[0])\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "-> Cannot close object, library is destroyed. This may cause a memory leak!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:09,396 - ERROR - Nougat processing failed for Ancient_20Africa/Ancient Africa/Anna Leone - The End of the Pagan City. Religion, Economy, and Urbanism in Late Antique North Africa.pdf. Skipping cleaning, chunking, and upload.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Nougat processing failed for Ancient_20Africa/Ancient Africa/Anna Leone - The End of the Pagan City. Religion, Economy, and Urbanism in Late Antique North Africa.pdf. Skipping cleaning, chunking, and upload.\n",
            "\n",
            "Processing PDFs in Ancient_20Africa.rar:   7%|▋         | 1/14 [03:07<40:38, 187.57s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:09,429 - INFO - Processing PDF: Ancient_20Africa/Ancient Africa/Peter L. Shinnie - Ancient Nubia [Retail].pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: Ancient_20Africa/Ancient Africa/Peter L. Shinnie - Ancient Nubia [Retail].pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:11,678 - ERROR - Error extracting Hanseatic_20League_20_26_20East_20India_20Company.rar: \n",
            "Unexpected end of archive\n",
            "Hanseatic League & East India Company/John Man - Marco Polo. The Journey that Changed the World [Retail].epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Hanseatic_20League_20_26_20East_20India_20Company.rar: \n",
            "Unexpected end of archive\n",
            "Hanseatic League & East India Company/John Man - Marco Polo. The Journey that Changed the World [Retail].epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:11,681 - ERROR - Skipping PDF processing for Hanseatic%20League%20%26%20East%20India%20Company.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Hanseatic%20League%20%26%20East%20India%20Company.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:43:11,689 - INFO - --- Processing Medieval%20Society%20and%20Everyday%20Life.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20Society%20and%20Everyday%20Life.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,480 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Hanseatic%20League%20%26%20East%20India%20Company.rar to Hanseatic_20League_20_26_20East_20India_20Company.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Hanseatic%20League%20%26%20East%20India%20Company.rar to Hanseatic_20League_20_26_20East_20India_20Company.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,502 - ERROR - Skipping extraction and processing for Hanseatic%20League%20%26%20East%20India%20Company.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Hanseatic%20League%20%26%20East%20India%20Company.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,530 - INFO - --- Processing Medieval%20People.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20People.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,570 - INFO - Medieval_20People.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Medieval_20People.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,587 - ERROR - Skipping extraction and processing for Medieval%20People.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Medieval%20People.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,598 - INFO - --- Processing Medieval%20Society%20and%20Everyday%20Life.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Medieval%20Society%20and%20Everyday%20Life.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,623 - INFO - Medieval_20Society_20and_20Everyday_20Life.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Medieval_20Society_20and_20Everyday_20Life.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,638 - ERROR - Skipping extraction and processing for Medieval%20Society%20and%20Everyday%20Life.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Medieval%20Society%20and%20Everyday%20Life.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,646 - INFO - --- Processing Miscellaneous.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Miscellaneous.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,686 - INFO - Miscellaneous.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Miscellaneous.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,692 - ERROR - Skipping extraction and processing for Miscellaneous.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Miscellaneous.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:04,705 - INFO - --- Processing Renaissance%20and%20Enlightenment.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Renaissance%20and%20Enlightenment.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:05,411 - ERROR - Error extracting Miscellaneous.rar: \n",
            "Unexpected end of archive\n",
            "Miscellaneous/DK - Civilization. A History of the World in 1000 Objects (2nd Edition) (UK Edition) [Retail].pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Miscellaneous.rar: \n",
            "Unexpected end of archive\n",
            "Miscellaneous/DK - Civilization. A History of the World in 1000 Objects (2nd Edition) (UK Edition) [Retail].pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:05,419 - ERROR - Skipping PDF processing for Miscellaneous.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Miscellaneous.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:05,837 - INFO - --- Processing Miscellaneous.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Miscellaneous.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:19,586 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20People.rar to Medieval_20People.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20People.rar to Medieval_20People.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:19,666 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Knights%20%26%20Chivalry.rar to Knights_20_26_20Chivalry.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Knights%20%26%20Chivalry.rar to Knights_20_26_20Chivalry.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:19,698 - ERROR - Skipping extraction and processing for Knights%20%26%20Chivalry.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Knights%20%26%20Chivalry.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:19,723 - INFO - --- Processing The%20Age%20of%20Discovery.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20Age%20of%20Discovery.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:38,524 - INFO - Extracted Medieval_20People.rar to Medieval_20People\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Extracted Medieval_20People.rar to Medieval_20People\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:38,551 - ERROR - Skipping PDF processing for Medieval%20People.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Medieval%20People.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:38,716 - INFO - --- Processing Renaissance%20and%20Enlightenment.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Renaissance%20and%20Enlightenment.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:38,754 - INFO - Renaissance_20and_20Enlightenment.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Renaissance_20and_20Enlightenment.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:59,455 - ERROR - Error extracting Renaissance_20and_20Enlightenment.rar: \n",
            "Unexpected end of archive\n",
            "Renaissance and Enlightenment/Giuseppe Marcocci - The Globe on Paper. Writing Histories of the World in Renaissance Europe and the Americas (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Renaissance_20and_20Enlightenment.rar: \n",
            "Unexpected end of archive\n",
            "Renaissance and Enlightenment/Giuseppe Marcocci - The Globe on Paper. Writing Histories of the World in Renaissance Europe and the Americas (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:59,462 - ERROR - Skipping PDF processing for Renaissance%20and%20Enlightenment.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Renaissance%20and%20Enlightenment.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:59,469 - INFO - --- Processing The%20Age%20of%20Discovery.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20Age%20of%20Discovery.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:44:59,472 - INFO - The_20Age_20of_20Discovery.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:The_20Age_20of_20Discovery.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:03,897 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Age%20of%20Discovery.rar: ('Connection broken: IncompleteRead(184590002 bytes read, 552106546 more expected)', IncompleteRead(184590002 bytes read, 552106546 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Age%20of%20Discovery.rar: ('Connection broken: IncompleteRead(184590002 bytes read, 552106546 more expected)', IncompleteRead(184590002 bytes read, 552106546 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:03,901 - ERROR - Skipping extraction and processing for The%20Age%20of%20Discovery.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for The%20Age%20of%20Discovery.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:03,904 - INFO - --- Processing The%20Early%20Middle%20Ages%20%28400-800%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20Early%20Middle%20Ages%20%28400-800%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:03,999 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Miscellaneous.rar: ('Connection broken: IncompleteRead(310419121 bytes read, 2616774376 more expected)', IncompleteRead(310419121 bytes read, 2616774376 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Miscellaneous.rar: ('Connection broken: IncompleteRead(310419121 bytes read, 2616774376 more expected)', IncompleteRead(310419121 bytes read, 2616774376 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:04,111 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Architecture.rar: ('Connection broken: IncompleteRead(796958385 bytes read, 2405511168 more expected)', IncompleteRead(796958385 bytes read, 2405511168 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Architecture.rar: ('Connection broken: IncompleteRead(796958385 bytes read, 2405511168 more expected)', IncompleteRead(796958385 bytes read, 2405511168 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:04,116 - ERROR - Skipping extraction and processing for Medieval%20Architecture.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Medieval%20Architecture.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:04,118 - INFO - --- Processing The%20High%20Middle%20Ages%20%281000-1300%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20High%20Middle%20Ages%20%281000-1300%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:05,874 - ERROR - Error extracting The_20Age_20of_20Discovery.rar: \n",
            "Unexpected end of archive\n",
            "The Age of Discovery/Jerry H. Bentley, Renate Bridenthal, Kären Wigen - Seascapes. Maritime Histories, Littoral Cultures, and Transoceanic Exchanges (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting The_20Age_20of_20Discovery.rar: \n",
            "Unexpected end of archive\n",
            "The Age of Discovery/Jerry H. Bentley, Renate Bridenthal, Kären Wigen - Seascapes. Maritime Histories, Littoral Cultures, and Transoceanic Exchanges (Retail).pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:05,881 - ERROR - Skipping PDF processing for The%20Age%20of%20Discovery.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for The%20Age%20of%20Discovery.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:05,912 - INFO - --- Processing The%20Early%20Middle%20Ages%20%28400-800%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20Early%20Middle%20Ages%20%28400-800%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:10,199 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Literature.rar: ('Connection broken: IncompleteRead(679517872 bytes read, 5278550621 more expected)', IncompleteRead(679517872 bytes read, 5278550621 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Literature.rar: ('Connection broken: IncompleteRead(679517872 bytes read, 5278550621 more expected)', IncompleteRead(679517872 bytes read, 5278550621 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:10,382 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Renaissance%20and%20Enlightenment.rar: ('Connection broken: IncompleteRead(293813937 bytes read, 869303860 more expected)', IncompleteRead(293813937 bytes read, 869303860 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Renaissance%20and%20Enlightenment.rar: ('Connection broken: IncompleteRead(293813937 bytes read, 869303860 more expected)', IncompleteRead(293813937 bytes read, 869303860 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:10,393 - ERROR - Skipping extraction and processing for Renaissance%20and%20Enlightenment.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Renaissance%20and%20Enlightenment.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:10,404 - INFO - --- Processing The%20Late%20Middle%20Ages%20%281300-1600%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20Late%20Middle%20Ages%20%281300-1600%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:27,204 - ERROR - Error extracting Miscellaneous.rar: \n",
            "Unexpected end of archive\n",
            "Miscellaneous/Christopher de Hamel - Meetings with Remarkable Manuscripts.epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Miscellaneous.rar: \n",
            "Unexpected end of archive\n",
            "Miscellaneous/Christopher de Hamel - Meetings with Remarkable Manuscripts.epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:27,214 - ERROR - Skipping PDF processing for Miscellaneous.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Miscellaneous.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:27,295 - INFO - --- Processing The%20High%20Middle%20Ages%20%281000-1300%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20High%20Middle%20Ages%20%281000-1300%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:27,298 - INFO - The_20High_20Middle_20Ages_20_281000-1300_29.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:The_20High_20Middle_20Ages_20_281000-1300_29.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:48,437 - ERROR - Error extracting The_20High_20Middle_20Ages_20_281000-1300_29.rar: \n",
            "Unexpected end of archive\n",
            "The High Middle Ages (1000-1300)/James Westfall Thompson, Edgar Nathaniel Johnson - An Introduction to Medieval Europe 300-1500.pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting The_20High_20Middle_20Ages_20_281000-1300_29.rar: \n",
            "Unexpected end of archive\n",
            "The High Middle Ages (1000-1300)/James Westfall Thompson, Edgar Nathaniel Johnson - An Introduction to Medieval Europe 300-1500.pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:48,447 - ERROR - Skipping PDF processing for The%20High%20Middle%20Ages%20%281000-1300%29.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for The%20High%20Middle%20Ages%20%281000-1300%29.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:48,456 - INFO - --- Processing The%20Late%20Middle%20Ages%20%281300-1600%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing The%20Late%20Middle%20Ages%20%281300-1600%29.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:45:48,459 - INFO - The_20Late_20Middle_20Ages_20_281300-1600_29.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:The_20Late_20Middle_20Ages_20_281300-1600_29.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:46:08,261 - ERROR - Error extracting The_20Late_20Middle_20Ages_20_281300-1600_29.rar: \n",
            "Unexpected end of archive\n",
            "The Late Middle Ages (1300-1600)/Barbara W. Tuchman - A Distant Mirror The Calamitous 14th Century (Updated Edition) [Retail].epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting The_20Late_20Middle_20Ages_20_281300-1600_29.rar: \n",
            "Unexpected end of archive\n",
            "The Late Middle Ages (1300-1600)/Barbara W. Tuchman - A Distant Mirror The Calamitous 14th Century (Updated Edition) [Retail].epub - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:46:08,264 - ERROR - Skipping PDF processing for The%20Late%20Middle%20Ages%20%281300-1600%29.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for The%20Late%20Middle%20Ages%20%281300-1600%29.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:46:08,265 - INFO - --- Processing Weapons%20%26%20Warfare.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Weapons%20%26%20Warfare.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:46:12,439 - ERROR - Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Society%20and%20Everyday%20Life.rar: ('Connection broken: IncompleteRead(616464049 bytes read, 2870199682 more expected)', IncompleteRead(616464049 bytes read, 2870199682 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error downloading file from https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/Medieval%20Society%20and%20Everyday%20Life.rar: ('Connection broken: IncompleteRead(616464049 bytes read, 2870199682 more expected)', IncompleteRead(616464049 bytes read, 2870199682 more expected))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:46:48,992 - ERROR - Error processing 1._20Prehistory/1. Prehistory/America/Bradley J. Vierra - The Late Archaic across the Borderlands From Foraging to Farming.pdf with Nougat: WARNING:root:No GPU found. Conversion on CPU is very slow.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/345 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error processing 1._20Prehistory/1. Prehistory/America/Bradley J. Vierra - The Late Archaic across the Borderlands From Foraging to Farming.pdf with Nougat: WARNING:root:No GPU found. Conversion on CPU is very slow.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\n",
            "  0%|          | 0/345 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:46:49,023 - ERROR - Nougat processing failed for 1._20Prehistory/1. Prehistory/America/Bradley J. Vierra - The Late Archaic across the Borderlands From Foraging to Farming.pdf. Skipping cleaning, chunking, and upload.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Nougat processing failed for 1._20Prehistory/1. Prehistory/America/Bradley J. Vierra - The Late Archaic across the Borderlands From Foraging to Farming.pdf. Skipping cleaning, chunking, and upload.\n",
            "\n",
            "\n",
            "Processing PDFs in 1._20Prehistory.rar:   6%|▋         | 2/31 [04:05<1:08:48, 142.37s/it]\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:46:49,061 - INFO - Processing PDF: 1._20Prehistory/1. Prehistory/America/R. G. Matson, Gary Coupland - The Prehistory of the Northwest Coast (Retail).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing PDF: 1._20Prehistory/1. Prehistory/America/R. G. Matson, Gary Coupland - The Prehistory of the Northwest Coast (Retail).pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:32,012 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Early%20Middle%20Ages%20%28400-800%29.rar to The_20Early_20Middle_20Ages_20_28400-800_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Early%20Middle%20Ages%20%28400-800%29.rar to The_20Early_20Middle_20Ages_20_28400-800_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:32,077 - ERROR - Skipping extraction and processing for The%20Early%20Middle%20Ages%20%28400-800%29.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for The%20Early%20Middle%20Ages%20%28400-800%29.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:32,114 - INFO - --- Processing Weapons%20%26%20Warfare.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Weapons%20%26%20Warfare.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:32,151 - INFO - Weapons_20_26_20Warfare.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Weapons_20_26_20Warfare.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:32,186 - ERROR - Skipping extraction and processing for Weapons%20%26%20Warfare.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for Weapons%20%26%20Warfare.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:32,215 - INFO - --- Processing 4.%20Early%20Modern.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing 4.%20Early%20Modern.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:32,285 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Early%20Middle%20Ages%20%28400-800%29.rar to The_20Early_20Middle_20Ages_20_28400-800_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Early%20Middle%20Ages%20%28400-800%29.rar to The_20Early_20Middle_20Ages_20_28400-800_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:37,332 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Late%20Middle%20Ages%20%281300-1600%29.rar to The_20Late_20Middle_20Ages_20_281300-1600_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20Late%20Middle%20Ages%20%281300-1600%29.rar to The_20Late_20Middle_20Ages_20_281300-1600_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:37,342 - ERROR - Skipping extraction and processing for The%20Late%20Middle%20Ages%20%281300-1600%29.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for The%20Late%20Middle%20Ages%20%281300-1600%29.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:37,348 - INFO - --- Processing 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:56,900 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/5.%20Ancient%20%26%20Classical%20Civilizations%20Series/24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar to 24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/5.%20Ancient%20%26%20Classical%20Civilizations%20Series/24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar to 24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:56,913 - ERROR - Skipping extraction and processing for 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:47:56,940 - INFO - --- Processing A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:33,157 - ERROR - Error extracting Medieval_20Society_20and_20Everyday_20Life.rar: \n",
            "Unexpected end of archive\n",
            "Medieval Society and Everyday Life/Arts and Crafts/Mariah Proctor-Tiffany - Medieval Art in Motion. The Inventory and Gift Giving of Queen Clemence de Hongrie [Retail].pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error extracting Medieval_20Society_20and_20Everyday_20Life.rar: \n",
            "Unexpected end of archive\n",
            "Medieval Society and Everyday Life/Arts and Crafts/Mariah Proctor-Tiffany - Medieval Art in Motion. The Inventory and Gift Giving of Queen Clemence de Hongrie [Retail].pdf - checksum error\n",
            "Unexpected end of archive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:33,164 - ERROR - Skipping PDF processing for Medieval%20Society%20and%20Everyday%20Life.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for Medieval%20Society%20and%20Everyday%20Life.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:33,207 - INFO - --- Processing 4.%20Early%20Modern.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing 4.%20Early%20Modern.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:33,214 - INFO - 4._20Early_20Modern.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:4._20Early_20Modern.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:44,907 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/5.%20Ancient%20%26%20Classical%20Civilizations%20Series/A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar to A_20Week_20in_20the_20Life_20_287_20Books_29_20_5BComplete_5D.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/5.%20Ancient%20%26%20Classical%20Civilizations%20Series/A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar to A_20Week_20in_20the_20Life_20_287_20Books_29_20_5BComplete_5D.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:44,935 - ERROR - Skipping extraction and processing for A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:44,952 - INFO - --- Processing Agora%20Picture%20Books%20%2827%20Books%29%20%5BComplete%5D%20%E2%80%A0.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Agora%20Picture%20Books%20%2827%20Books%29%20%5BComplete%5D%20%E2%80%A0.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:47,573 - INFO - Extracted The_20Early_20Middle_20Ages_20_28400-800_29.rar to The_20Early_20Middle_20Ages_20_28400-800_29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Extracted The_20Early_20Middle_20Ages_20_28400-800_29.rar to The_20Early_20Middle_20Ages_20_28400-800_29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:47,577 - ERROR - Skipping PDF processing for The%20Early%20Middle%20Ages%20%28400-800%29.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for The%20Early%20Middle%20Ages%20%28400-800%29.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:47,696 - INFO - --- Processing 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:47,710 - INFO - 24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:47,966 - INFO - Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20High%20Middle%20Ages%20%281000-1300%29.rar to The_20High_20Middle_20Ages_20_281000-1300_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Downloaded https://the-eye.eu/public/Books/Bibliotheca%20Alexandrina/3.%20Middle%20Ages/The%20High%20Middle%20Ages%20%281000-1300%29.rar to The_20High_20Middle_20Ages_20_281000-1300_29.rar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:48,007 - ERROR - Skipping extraction and processing for The%20High%20Middle%20Ages%20%281000-1300%29.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping extraction and processing for The%20High%20Middle%20Ages%20%281000-1300%29.rar due to download failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:48:48,013 - INFO - --- Processing Ancient%20Civilizations%20%28ABDO%20Publishing%29%20%288%20Books%29%20%5BComplete%5D%20%E2%80%A0.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing Ancient%20Civilizations%20%28ABDO%20Publishing%29%20%288%20Books%29%20%5BComplete%5D%20%E2%80%A0.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:49:01,818 - INFO - Extracted 24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D.rar to 24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Extracted 24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D.rar to 24_20Hours_20in_20Ancient_20History_20_284_20Books_29_20_5BComplete_5D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:49:01,827 - ERROR - Skipping PDF processing for 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Skipping PDF processing for 24%20Hours%20in%20Ancient%20History%20%284%20Books%29%20%5BComplete%5D.rar due to extraction failure.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:49:01,854 - INFO - --- Processing A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:--- Processing A%20Week%20in%20the%20Life%20%287%20Books%29%20%5BComplete%5D.rar ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-01 13:49:01,869 - INFO - A_20Week_20in_20the_20Life_20_287_20Books_29_20_5BComplete_5D.rar already exists. Skipping download.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:A_20Week_20in_20the_20Life_20_287_20Books_29_20_5BComplete_5D.rar already exists. Skipping download.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}