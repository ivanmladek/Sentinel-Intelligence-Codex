{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanmladek/Sentinel-Intelligence-Codex/blob/main/process_refactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cybg_6vE2qM",
        "outputId": "4484c321-7d4c-4fe6-b54b-494b301d70a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nougat-ocr==0.1.17\n",
            "  Downloading nougat_ocr-0.1.17-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.17) (4.52.4)\n",
            "Collecting timm==0.5.4 (from nougat-ocr==0.1.17)\n",
            "  Downloading timm-0.5.4-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.17) (3.10.18)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.17) (4.11.0.86)\n",
            "Requirement already satisfied: datasets[vision] in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.17) (2.14.4)\n",
            "Collecting lightning<2022,>=2.0.0 (from nougat-ocr==0.1.17)\n",
            "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.17) (3.9.1)\n",
            "Collecting python-Levenshtein (from nougat-ocr==0.1.17)\n",
            "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.17) (0.2.0)\n",
            "Collecting sconf>=0.2.3 (from nougat-ocr==0.1.17)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: albumentations>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from nougat-ocr==0.1.17) (2.0.8)\n",
            "Collecting pypdf>=3.1.0 (from nougat-ocr==0.1.17)\n",
            "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting pypdfium2 (from nougat-ocr==0.1.17)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4->nougat-ocr==0.1.17) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==0.5.4->nougat-ocr==0.1.17) (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (1.15.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (2.11.7)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations>=1.0.0->nougat-ocr==0.1.17) (0.0.24)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations>=1.0.0->nougat-ocr==0.1.17) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations>=1.0.0->nougat-ocr==0.1.17) (6.4.9)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (2025.3.2)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (24.2)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (4.14.0)\n",
            "Collecting pytorch-lightning (from lightning<2022,>=2.0.0->nougat-ocr==0.1.17)\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr==0.1.17)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting munch (from sconf>=0.2.3->nougat-ocr==0.1.17)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (0.33.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->nougat-ocr==0.1.17) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (3.11.15)\n",
            "Requirement already satisfied: Pillow>=6.2.1 in /usr/local/lib/python3.11/dist-packages (from datasets[vision]->nougat-ocr==0.1.17) (11.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.17) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->nougat-ocr==0.1.17) (1.5.1)\n",
            "Collecting Levenshtein==0.27.1 (from python-Levenshtein->nougat-ocr==0.1.17)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein->nougat-ocr==0.1.17)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets[vision]->nougat-ocr==0.1.17) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=4.25.1->nougat-ocr==0.1.17) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2022,>=2.0.0->nougat-ocr==0.1.17) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.0.0->nougat-ocr==0.1.17) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.0.0->nougat-ocr==0.1.17) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations>=1.0.0->nougat-ocr==0.1.17) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.25.1->nougat-ocr==0.1.17) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets[vision]->nougat-ocr==0.1.17) (2025.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr==0.1.17)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets[vision]->nougat-ocr==0.1.17) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4->timm==0.5.4->nougat-ocr==0.1.17) (3.0.2)\n",
            "Downloading nougat_ocr-0.1.17-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.7/664.8 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:12\u001b[0m"
          ]
        }
      ],
      "source": [
        "#@title Setup and Authentication\n",
        "#!apt-get install -y poppler-utils tesseract-ocr libmagic-dev\n",
        "\n",
        "# Downgrade numpy to a version compatible with other libraries\n",
        "#!pip install numpy<2\n",
        "#https://github.com/facebookresearch/nougat/issues/245\n",
        "!pip install nougat-ocr==0.1.17\n",
        "!pip install albucore==0.0.16 albumentations==1.0.0 pip install transformers==4.38.2\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install   textblob langdetect\n",
        "#!pip install nltk>3.8.1\n",
        "import os\n",
        "import logging\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "from google.colab import drive\n",
        "import re\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "def find_pdfs(directory):\n",
        "    pdf_files = []\n",
        "    for root, dirs, files in os.walk(directory, followlinks=True):\n",
        "        for file in files:\n",
        "            if file.endswith('.pdf'):\n",
        "                pdf_files.append(os.path.join(root, file))\n",
        "    return pdf_files\n",
        "\n",
        "pdf_directory = '/content/drive/MyDrive/world_tracker/Biology'  # Replace with your directory path\n",
        "pdf_files = find_pdfs(pdf_directory)\n",
        "for p in pdf_files:\n",
        "    print(p)"
      ],
      "metadata": {
        "id": "PyLuNnwd4nWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_XNyT3EEnom"
      },
      "outputs": [],
      "source": [
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "import fcntl  # Import the fcntl module for file locking\n",
        "def sanitize_filename(filename):\n",
        "    # Remove spaces and other potentially problematic characters\n",
        "    sanitized = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "\n",
        "def upload_to_drive(csv_path, drive_directory):\n",
        "    try:\n",
        "        #drive.mount('/content/drive', force_remount=True)\n",
        "        destination_path = os.path.join(drive_directory, os.path.basename(csv_path))\n",
        "        shutil.copy(csv_path, destination_path)\n",
        "        print(f\"Uploaded {csv_path} to {destination_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error uploading to Google Drive: {e}\")\n",
        "\n",
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import words, brown\n",
        "from transformers import pipeline\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('brown')\n",
        "GARBAGE_THRESHOLD = 0.8\n",
        "\n",
        "# Load a pre-trained language model for fluency scoring\n",
        "language_model = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "from textblob import TextBlob\n",
        "\n",
        "def text_quality_score(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Check if tokens are common English words\n",
        "    english_words = set(words.words())\n",
        "    common_word_ratio = sum(1 for word in tokens if word.lower() in english_words) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Part-of-Speech tagging\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    pos_score = sum(1 for word, tag in pos_tags if tag in ['NN', 'VB', 'JJ', 'RB']) / len(pos_tags) if pos_tags else 0\n",
        "\n",
        "    # Language Model fluency score (simplified)\n",
        "    lm_score = len(text.split()) / 10  # Simplified scoring\n",
        "\n",
        "    # Spell check\n",
        "    blob = TextBlob(text)\n",
        "    misspelled_words = [word for word, pos in blob.tags if pos != 'NNP' and pos != 'NNPS' and word.lower() not in english_words and not word.isalpha()]\n",
        "    misspelled_ratio = len(misspelled_words) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Check for too many numbers\n",
        "    number_ratio = sum(1 for token in tokens if token.isdigit()) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Check for words with spaces between letters\n",
        "    spaced_word_ratio = sum(1 for token in tokens if re.search(r'(\\w) (\\w)', token)) / len(tokens) if tokens else 0\n",
        "\n",
        "    # Combine scores\n",
        "    combined_score = (common_word_ratio + pos_score + lm_score) / 3\n",
        "\n",
        "    # Include misspelled ratio, number ratio, and spaced word ratio in the score\n",
        "    combined_score *= (1 - misspelled_ratio)\n",
        "    combined_score *= (1 - number_ratio)\n",
        "    combined_score *= (1 - spaced_word_ratio)\n",
        "\n",
        "    return combined_score\n",
        "\n",
        "def is_garbage(text, threshold=GARBAGE_THRESHOLD, LENWORD=50):\n",
        "    # Calculate the quality score\n",
        "    score = text_quality_score(text) #this check is now ooutsourced to the nougat OCR\n",
        "\n",
        "    # Determine if the text is garbage based on the threshold\n",
        "    if score < threshold:\n",
        "        return True\n",
        "\n",
        "    # Check for jammed words\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    for word in words:\n",
        "        if len(word) > LENWORD and '_' not in word:  # heuristic to detect jammed words\n",
        "            return True\n",
        "\n",
        "    # Check for a high proportion of single-letter words\n",
        "    single_letter_words = [word for word in words if len(word) == 1]\n",
        "    if len(single_letter_words) > 0.1 * len(words):  # threshold for single-letter words\n",
        "        return True\n",
        "\n",
        "    #TODO check for repetition\n",
        "      # Check for repetitive patterns\n",
        "     # Check for repetitive loops\n",
        "    loops = re.findall(r'(.+?)\\1{2,}', text) #finds almost all loops but some sneak in\n",
        "    #loops = re.findall(r'(.+?)\\1', text) #finds almost no loops\n",
        "    if loops:\n",
        "        return True\n",
        "\n",
        "    # Check if the text is predominantly in English\n",
        "    try:\n",
        "        if detect(text) != 'en':\n",
        "            return True\n",
        "    except LangDetectException:\n",
        "        # If language detection fails, consider the text as garbage\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "import re\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra newlines and spaces\n",
        "    text = re.sub(r'\\n+', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    # Remove leading and trailing spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove any text between square brackets\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
        "\n",
        "    # Remove academic references, bracketed references, and references to tables and figures\n",
        "    text = re.sub(r'\\(\\d+\\)', '', text, flags=re.IGNORECASE)  # Remove (82)\n",
        "    text = re.sub(r'\\[[A-Za-z0-9]+\\]', '', text, flags=re.IGNORECASE)  # Remove [RD47] and similar\n",
        "    text = re.sub(r'\\([\\w\\s]+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)  # Remove (Krizhevsky et al., 2014)\n",
        "    text = re.sub(r'\\(\\w+\\s+and\\s+\\w+\\s+\\d{4}\\)', '', text, flags=re.IGNORECASE)  # Remove (Menendez and Woodworth 2010)\n",
        "    text = re.sub(r'\\(see\\s+equations\\s+\\(\\d+\\)\\s+and\\s+\\(\\d+\\)\\)', '', text, flags=re.IGNORECASE)  # Remove (see equations (5) and (10))\n",
        "    text = re.sub(r'\\(\\w+\\s+et\\s+al\\., \\d{4};\\s*\\w+\\s+et\\s+al\\., \\d{4}\\)', '', text, flags=re.IGNORECASE)  # Remove (Yao et al., 2007; Lin et al., 2016)\n",
        "    text = re.sub(r'Table\\s+\\d+', '', text, flags=re.IGNORECASE)  # Remove Table 7\n",
        "    text = re.sub(r'\\[FIGURE:[^]]+\\]', '', text, flags=re.IGNORECASE)  # Remove [FIGURE:S4.F1]\n",
        "    text = re.sub(r'\\[\\d+(,\\s*\\d+)*\\]', '', text, flags=re.IGNORECASE)  # Remove [23, 32, 36, 37, 38, 39]\n",
        "    text = re.sub(r'\\[.*arxiv.*\\]', '', text, flags=re.IGNORECASE)  # Remove any text containing \"arxiv\" between brackets\n",
        "\n",
        "    # Remove any remaining non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove any sequences of punctuation or symbols that are likely to be garbled text\n",
        "    text = re.sub(r'[\\.,;:!?]{2,}', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "def download_pdf(file_id, output_name):\n",
        "    try:\n",
        "        # Assuming file_id is the source path and output_name is the destination path\n",
        "        if not os.path.exists(output_name):\n",
        "            shutil.copy(file_id, output_name)\n",
        "            print(f\"Copied {file_id} to {output_name}\")\n",
        "        else:\n",
        "            print(f\"{output_name} already exists. Skipping download.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error copying {file_id} to {output_name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gapbfXGUhvnY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import os\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "#import PyPDF2\n",
        "\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "import fcntl\n",
        "\n",
        "\n",
        "def chunk_text(content, max_size=8192):\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    lines = content.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith((\"#\", \"##\", \"###\")):\n",
        "            if current_segment:\n",
        "                segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "            current_segment = \"\"\n",
        "        else:\n",
        "            current_segment += line + \" \"\n",
        "\n",
        "    if current_segment:\n",
        "        segments.extend(split_segment(current_segment.strip(), max_size))\n",
        "\n",
        "    return segments\n",
        "\n",
        "import nltk\n",
        "\n",
        "def split_segment(segment, max_size):\n",
        "    sentences = nltk.sent_tokenize(segment)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) + 1 <= max_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def clean_text_from_mmd(file_path, garbage_threshold=0.8):\n",
        "    raw_text_path = file_path  # Assuming the file is the raw text file\n",
        "    garbage_text_path = f\"{os.path.splitext(file_path)[0]}_garbage_text.jsonl\"\n",
        "    cleaned_text_path = f\"{os.path.splitext(file_path)[0]}_cleaned_text.jsonl\"\n",
        "\n",
        "    if not os.path.exists(raw_text_path):\n",
        "        print(f\"Raw text for {file_path} does not exist. Skipping cleaning.\")\n",
        "        return\n",
        "\n",
        "    # Delete existing JSONL files\n",
        "    if os.path.exists(garbage_text_path):\n",
        "        os.remove(garbage_text_path)\n",
        "    if os.path.exists(cleaned_text_path):\n",
        "        os.remove(cleaned_text_path)\n",
        "\n",
        "    print(f\"Starting text cleaning from {file_path}\")\n",
        "    garbage_count = 0\n",
        "    total_segments = 0\n",
        "\n",
        "    with open(raw_text_path, \"r\", encoding='utf-8') as raw_f, \\\n",
        "         open(cleaned_text_path, \"a\", encoding='utf-8') as cleaned_f, \\\n",
        "         open(garbage_text_path, \"a\", encoding='utf-8') as garbage_f:\n",
        "        progress_bar = tqdm(desc=f\"Cleaning {file_path}\", unit=\"segment\", leave=True)\n",
        "\n",
        "        # Read the entire file content\n",
        "        content = raw_f.read()\n",
        "\n",
        "        # Call a separate function for chunking the text\n",
        "        segments = chunk_text(content)\n",
        "\n",
        "        for segment in segments:\n",
        "            #print(segment)\n",
        "            try:\n",
        "                cleaned, is_garb = process_segment(segment, garbage_threshold)\n",
        "                if is_garb:\n",
        "                    fcntl.flock(garbage_f, fcntl.LOCK_EX)\n",
        "                    garbage_f.write(json.dumps({\"text\": cleaned}) + '\\n')\n",
        "                    fcntl.flock(garbage_f, fcntl.LOCK_UN)\n",
        "                    garbage_count += 1\n",
        "                else:\n",
        "                    fcntl.flock(cleaned_f, fcntl.LOCK_EX)\n",
        "                    cleaned_f.write(json.dumps({\"text\": cleaned}) + '\\n')\n",
        "                    fcntl.flock(cleaned_f, fcntl.LOCK_UN)\n",
        "                total_segments += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing segment: {e}\")\n",
        "            progress_bar.update(1)\n",
        "        progress_bar.close()\n",
        "\n",
        "    garbage_percentage = (garbage_count / total_segments) * 100 if total_segments else 0\n",
        "    print(f\"Completed text cleaning from {file_path}. Garbage segments: {garbage_percentage:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I829qzLMhpY1"
      },
      "outputs": [],
      "source": [
        "def extract_page_text(page):\n",
        "    return page.extract_text()\n",
        "\n",
        "def process_segment(line, garbage_threshold):\n",
        "    segment = line#json.loads(line)[\"text\"]\n",
        "    cleaned = clean_text(segment)\n",
        "    is_garb = is_garbage(cleaned, threshold=garbage_threshold)\n",
        "    return cleaned, is_garb\n",
        "\n",
        "\n",
        "def check_file_in_drive(file_path, drive_directory):\n",
        "    \"\"\"Check if a file exists in the specified Google Drive directory.\"\"\"\n",
        "    # Construct the full path in the mounted drive\n",
        "    full_path = os.path.join(drive_directory, os.path.basename(file_path))\n",
        "\n",
        "    # Check if the file exists\n",
        "    return os.path.exists(full_path)\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def process_pdf(file_id, drive_directory):\n",
        "    try:\n",
        "        logger.info(f\"Starting processing for {file_id}\")\n",
        "\n",
        "        base_filename = os.path.basename(file_id)\n",
        "        sanitized_base_filename = sanitize_filename(base_filename)\n",
        "        sanitized_output_name = f\"{sanitized_base_filename}.pdf\"\n",
        "        logger.info(f\"Sanitized output filename: {sanitized_output_name}\")\n",
        "\n",
        "        if not os.path.exists(sanitized_output_name):\n",
        "            download_pdf(file_id, sanitized_output_name)\n",
        "            logger.info(f\"Downloaded {sanitized_output_name}\")\n",
        "        else:\n",
        "            logger.info(f\"{sanitized_output_name} already exists. Skipping download.\")\n",
        "\n",
        "        raw_text_path = f\"{os.path.splitext(sanitized_output_name)[0]}.mmd\"\n",
        "\n",
        "        # Check if the files already exist in the drive directory\n",
        "        if check_file_in_drive(raw_text_path, drive_directory):\n",
        "            logger.info(f\"{raw_text_path} already exists in {drive_directory}. Skipping processing.Going to cleaning\")\n",
        "            clean_text_from_mmd(drive_directory+\"/\"+raw_text_path, GARBAGE_THRESHOLD)\n",
        "            return\n",
        "\n",
        "        # Check for the presence of raw_text_path (MML file)\n",
        "        if not os.path.exists(raw_text_path):\n",
        "            # Use nougat for text extraction if MML file is not present\n",
        "            nougat_output_path = f\"{os.path.splitext(sanitized_output_name)[0]}.mmd\"\n",
        "            nougat_command = f\"nougat {sanitized_output_name} -o {nougat_output_path} --no-skipping -m 0.1.0-base --recompute\"\n",
        "            !nougat {sanitized_output_name} -o ./ --no-skipping --recompute\n",
        "            #result = subprocess.run(nougat_command, shell=True, check=True, capture_output=True, text=True)\n",
        "            #logger.info(f\"Nougat output: {result.stdout}\")\n",
        "\n",
        "            # Check for the presence of MML output from nougat\n",
        "            if not os.path.exists(raw_text_path):\n",
        "                logger.error(f\"MML output not found for {sanitized_output_name}\")\n",
        "                return\n",
        "\n",
        "        # Upload results to drive\n",
        "        if os.path.exists(raw_text_path):\n",
        "            upload_to_drive(raw_text_path, drive_directory)\n",
        "            logger.info(f\"Uploaded {raw_text_path} to {drive_directory}\")\n",
        "\n",
        "        os.remove(sanitized_output_name)  # Clean up the downloaded PDF file\n",
        "        logger.info(f\"Removed {sanitized_output_name}\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Error processing {file_id}: {e}\")\n",
        "        logger.error(f\"Nougat stderr: {e.stderr}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing {file_id}: {e}\")\n",
        "\n",
        "\n",
        "def process_pdfs(pdf_files, drive_directory):\n",
        "    total_pdfs = len(pdf_files)\n",
        "    progress_bar = tqdm(total=total_pdfs, desc=\"Processing PDFs\", unit=\"PDF\", leave=True)\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"Copying {pdf_file} to working directory\")\n",
        "        # Assuming you have a function to copy the file to the working directory\n",
        "        # copy_to_working_directory(pdf_file)\n",
        "        process_pdf(pdf_file, drive_directory)\n",
        "        progress_bar.update(1)\n",
        "    progress_bar.close()\n",
        "    print(\"Completed processing all PDFs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN58P5wtll1H"
      },
      "outputs": [],
      "source": [
        "\n",
        "!nougat {sanitized_output_name} -o ./ --no-skipping --recompute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxQL7-rNEzNX"
      },
      "outputs": [],
      "source": [
        "\n",
        "drive_directory = '/content/drive/MyDrive/'\n",
        "\n",
        "process_pdfs(pdf_files, drive_directory)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}